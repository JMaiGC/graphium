{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Positional Encodings\n",
    "\n",
    "One of the main advantage of this library is the ability to easily incorporate novel positional encodings on the node, edge and graph level. The positional encodings are computed and feed into respective encoders and then the hidden embeddings from all pe encoders are pooled (according to if they are node, edge, or graph level) and then feed into the GNN layers as features. The designs allow any combination of positional encodings to be used by modifying the configuration file. For more details on the data processing part, please visit the [design page of the doc](https://valence-discovery.github.io/goli/design.html).\n",
    "\n",
    "Here is the workflow for computing and processing positional encoding in the library:\n",
    "1. modify the arguments in the configuration file\n",
    "\n",
    "2. compute the raw positional encoding from the graph in [`goli/features/positional_encoding.py`](https://valence-discovery.github.io/goli/api/goli.features.html#goli.features.positional_encoding) (from the [`graph positional encoder`](https://valence-discovery.github.io/goli/api/goli.features.html#goli.features.positional_encoding.graph_positional_encoder))\n",
    "\n",
    "3. feed the raw positional encoding into the respective (specialized) encoders in [`goli/nn/encoders`](https://valence-discovery.github.io/goli/api/goli.nn/encoders.html). For example, a simple [`MLP positional encoder`](https://valence-discovery.github.io/goli/api/goli.nn/encoders.html#goli.nn.encoders.mlp_encoder) can be found. \n",
    "\n",
    "4. Output the hidden embeddings of pe from the encoders in their respective output keys: `feat`(node feature), `edge_feat`(edge feature), `graph_feat`(graph feature) and potentially other keys if needed such as `nodepair_feat` \n",
    "\n",
    "5. pool the hidden embeddings with same keys together: for example, all output with `feat` key will be pooled together\n",
    "\n",
    "6. Construct the [`PyG Batch`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.Batch.html#torch_geometric.data.Batch), batch of graphs, each contain the output keys seen above, ready for use in the [GNN layers](https://valence-discovery.github.io/goli/api/goli.nn/pyg_layers.html) \n",
    "\n",
    "Since this library is built using PyG, we recommend looking at their [Docs](https://pytorch-geometric.readthedocs.io/en/latest/) and [Tutorials](https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html) for more info. \n",
    "\n",
    "We start by editing the configuration file first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of content\n",
    "1. [edit the config file](#Edit-the-Configuration-File)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit the Configuration File\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pos_encoding_as_features:\n",
    "    pos_types:\n",
    "      la_pos: &pos_enc  #use same name as pe_encoder\n",
    "        pos_type: laplacian_eigvec_eigval #laplacian_eigvec\n",
    "        num_pos: 3\n",
    "        normalization: \"none\"\n",
    "        disconnected_comp: True\n",
    "      rw_pos: #use same name as pe_encoder\n",
    "        pos_type: rwse\n",
    "        ksteps: 16\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import dgl\n",
    "from copy import deepcopy\n",
    "\n",
    "from goli.nn.dgl_layers import BaseDGLLayer\n",
    "from goli.nn.base_layers import FCLayer\n",
    "from goli.utils.decorators import classproperty\n",
    "\n",
    "\n",
    "_ = torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-defining test variables\n",
    "\n",
    "We define below a small batched graph on which we can test the created layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=7, num_edges=14,\n",
      "      ndata_schemes={'h': Scheme(shape=(5,), dtype=torch.float64)}\n",
      "      edata_schemes={'e': Scheme(shape=(13,), dtype=torch.float64)})\n"
     ]
    }
   ],
   "source": [
    "in_dim = 5          # Input node-feature dimensions\n",
    "out_dim = 11        # Desired output node-feature dimensions\n",
    "in_dim_edges = 13   # Input edge-feature dimensions\n",
    "\n",
    "# Let's create 2 simple graphs. Here the tensors represent the connectivity between nodes\n",
    "g1 = dgl.graph((torch.tensor([0, 1, 2]), torch.tensor([1, 2, 3])))\n",
    "g2 = dgl.graph((torch.tensor([0, 0, 0, 1]), torch.tensor([0, 1, 2, 0])))\n",
    "\n",
    "# We add some node features to the graphs\n",
    "g1.ndata[\"h\"] = torch.rand(g1.num_nodes(), in_dim, dtype=float)\n",
    "g2.ndata[\"h\"] = torch.rand(g2.num_nodes(), in_dim, dtype=float)\n",
    "\n",
    "# We also add some edge features to the graphs\n",
    "g1.edata[\"e\"] = torch.rand(g1.num_edges(), in_dim_edges, dtype=float)\n",
    "g2.edata[\"e\"] = torch.rand(g2.num_edges(), in_dim_edges, dtype=float)\n",
    "\n",
    "# Finally we batch the graphs in a way compatible with the DGL library\n",
    "bg = dgl.batch([g1, g2])\n",
    "bg = dgl.add_self_loop(bg)\n",
    "\n",
    "# The batched graph will show as a single graph with 7 nodes\n",
    "print(bg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a simple layer\n",
    "\n",
    "Here, we will show how to create a GNN layer that does a mean aggregation on the neighbouring features.\n",
    "\n",
    "First, for the layer to be fully compatible with the flexible architecture provided by `FeedForwardDGL`, it needs to inherit from the class `BaseDGLLayer`. This base-layer has multiple virtual methods that must be implemented in any class that inherits from it.\n",
    "\n",
    "The virtual methods are below\n",
    "\n",
    "- `layer_supports_edges`: We want to return `False` since our layer doesn't support edges\n",
    "- `layer_inputs_edges`: We want to return `False` since our layer doesn't input edges\n",
    "- `layer_outputs_edges`: We want to return `False` since our layer doesn't output edges\n",
    "- `layer_outdim_factor`: We want to return `1` since the output dimension does not depend on internal parameters.\n",
    "\n",
    "The example is given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMeanLayer(BaseDGLLayer):\n",
    "    def __init__(self, in_dim, out_dim, activation, dropout, normalization):\n",
    "        # Initialize the parent class\n",
    "        super().__init__(   in_dim=in_dim, out_dim=out_dim, activation=activation,\n",
    "                            dropout=dropout, normalization=normalization)\n",
    "\n",
    "        # Create the layer with learned parameters\n",
    "        self.layer = FCLayer(in_dim=in_dim, out_dim=out_dim)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        # We first apply the mean aggregation\n",
    "        g.ndata[\"h\"] = h\n",
    "        g.update_all(message_func=dgl.function.copy_u(\"h\", \"m\"), \n",
    "                    reduce_func=dgl.function.mean(\"m\", \"h\"))\n",
    "\n",
    "        # Then we apply the FCLayer, and the non-linearities\n",
    "        h = g.ndata[\"h\"]\n",
    "        h = self.layer(h)\n",
    "        h = self.apply_norm_activation_dropout(h)\n",
    "        return h\n",
    "\n",
    "    # Finally, we define all the virtual properties according to how\n",
    "    # the class works\n",
    "    @classproperty\n",
    "    def layer_supports_edges(cls):\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def layer_inputs_edges(self):\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def layer_outputs_edges(self):\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def out_dim_factor(self):\n",
    "        return 1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to test the `SimpleMeanLayer` on some DGL graphs. Note that in this example, we **ignore** the edge features since they are not supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 5])\n",
      "torch.Size([7, 11])\n"
     ]
    }
   ],
   "source": [
    "graph = deepcopy(bg)\n",
    "h_in = graph.ndata[\"h\"]\n",
    "layer = SimpleMeanLayer(\n",
    "            in_dim=in_dim, out_dim=out_dim, \n",
    "            activation=\"relu\", dropout=.3, normalization=\"batch_norm\").to(float)\n",
    "h_out = layer(graph, h_in)\n",
    "\n",
    "print(h_in.shape)\n",
    "print(h_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a complex layer with edges\n",
    "\n",
    "Here, we will show how to create a GNN layer that does a mean aggregation on the neighbouring features, concatenated to the edge features with their neighbours. In that case, only the node features will change, and the network will not update the edge features.\n",
    "\n",
    "The virtual methods will have different outputs\n",
    "\n",
    "- `layer_supports_edges`: We want to return `True` since our layer does support edges\n",
    "- `layer_inputs_edges`: We want to return `True` since our layer does input edges\n",
    "- `layer_outputs_edges`: We want to return `False` since our layer will not output new edges\n",
    "- `layer_outdim_factor`: We want to return `1` since the output dimension does not depend on internal parameters.\n",
    "\n",
    "The example is given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexMeanLayer(BaseDGLLayer):\n",
    "    def __init__(self, in_dim, out_dim, in_dim_edges, activation, dropout, normalization):\n",
    "        # Initialize the parent class\n",
    "        super().__init__(   in_dim=in_dim, out_dim=out_dim, activation=activation,\n",
    "                            dropout=dropout, normalization=normalization)\n",
    "\n",
    "        # Create the layer with learned parameters. Note the addition\n",
    "        self.layer = FCLayer(in_dim=in_dim + in_dim_edges, out_dim=out_dim)\n",
    "\n",
    "    def cat_nodes_edges(self, edges):\n",
    "        # Create a message \"m\" by concatenating \"h\" and \"e\" for each pair of nodes\n",
    "        nodes_edges = torch.cat([edges.src[\"h\"], edges.data[\"e\"]], dim=-1)\n",
    "        return {\"m\": nodes_edges}\n",
    "\n",
    "    def get_edges_messages(self, edges): # Simply return the messages on the edges\n",
    "        return {\"m\": edges.data[\"m\"]}\n",
    "\n",
    "    def forward(self, g, h, e):\n",
    "\n",
    "        # We first concatenate both the node and edge features on the edges\n",
    "        g.ndata[\"h\"] = h\n",
    "        g.edata[\"e\"] = e\n",
    "        g.apply_edges(self.cat_nodes_edges)\n",
    "\n",
    "        # Then we apply the mean aggregation to generate a message \"m\"\n",
    "        g.update_all(message_func=self.get_edges_messages, \n",
    "                    reduce_func=dgl.function.mean(\"m\", \"h\"))\n",
    "\n",
    "        # Finally we apply the FCLayer, and the non-linearities\n",
    "        h = g.ndata[\"h\"]\n",
    "        h = self.layer(h)\n",
    "        h = self.apply_norm_activation_dropout(h)\n",
    "        return h\n",
    "\n",
    "    # Finally, we define all the virtual properties according to how\n",
    "    # the class works\n",
    "    @classproperty\n",
    "    def layer_supports_edges(cls):\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def layer_inputs_edges(self):\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def layer_outputs_edges(self):\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def out_dim_factor(self):\n",
    "        return 1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to test the `ComplexMeanLayer` on some DGL graphs. Note that in this example, we **use** the edge features since they are mandatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 5])\n",
      "torch.Size([7, 11])\n"
     ]
    }
   ],
   "source": [
    "graph = deepcopy(bg)\n",
    "h_in = graph.ndata[\"h\"]\n",
    "e_in = graph.edata[\"e\"]\n",
    "layer = ComplexMeanLayer(\n",
    "            in_dim=in_dim, out_dim=out_dim, in_dim_edges=in_dim_edges,\n",
    "            activation=\"relu\", dropout=.3, normalization=\"batch_norm\").to(float)\n",
    "h_out = layer(graph, h_in, e_in)\n",
    "\n",
    "print(h_in.shape)\n",
    "print(h_out.shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4a99d018a205fcbcc0480c84566beaebcb91b08d0414b39a842df533e2a1d25"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
