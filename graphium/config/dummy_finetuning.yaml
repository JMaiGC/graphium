# Here, we are finetuning a FullGraphMultitaskNetwork
# trained on ToyMix. We finetune from the zinc task-head
# (graph-level) on the TDC dataset lipophilicity_astraceneca

# Here are the changes to the architecture:
#
# Change zinc task-head:
#   depth:        2 -> 2 - 1 + 2 = 3
#   out_dim:      3 -> 8
#
# Add finetuning head
#   model_type:   FeedForwardNN
#   out_dim:      1
#   hidden_dims:  8
#   depth:        2


###################################################
########### How to combine information  ###########
###################################################


###########################
### FINETUNING-SPECIFIC ###
###########################

finetuning:
  # New task
  task: lipophilicity_astrazeneca
  level: graph

  # Pretrained model
  pretrained_model: dummy-pretrained-model-cpu
  finetuning_module: task_heads                
  task_head_from_pretrained: zinc # none       # could be more general      

  # Changes to finetuning_module                                                  
  drop_depth: 1 
  new_out_dim: 8
  added_depth: 2

  # Training
  unfreeze_pretrained_depth: 0
  epoch_unfreeze_all: none

  # # Finetuning network
  # model_type: FullGraphFinetuningNetwork

  # Optional finetuning head appended to model after finetuning_module
  finetuning_head:
    task: lipophilicity_astrazeneca
    previous_module: task_heads
    incoming_level: graph
    model_type: mlp
    in_dim: 8
    out_dim: 1
    hidden_dims: 8
    depth: 2
    last_layer_is_readout: true

constants:
  seed: 42
  max_epochs: 100

accelerator:
  float32_matmul_precision: medium
  type: cpu

predictor:
  metrics_on_progress_bar:
    lipophilicity_astrazeneca: ["mae"]
  loss_fun:
    lipophilicity_astrazeneca: mae

metrics:
  lipophilicity_astrazeneca:
    - name: mae
      metric: mae
      target_nan_mask: null
      multitask_handling: flatten
      threshold_kwargs: null
    - name: spearman
      metric: spearmanr
      threshold_kwargs: null
      target_nan_mask: null
      multitask_handling: mean-per-label
    - name: pearson
      metric: pearsonr
      threshold_kwargs: null
      target_nan_mask: null
      multitask_handling: mean-per-label
    - name: r2_score
      metric: r2
      target_nan_mask: null
      multitask_handling: mean-per-label
      threshold_kwargs: null


####################
### ARCHITECTURE ###
####################

architecture:

# ### FINETUNING ###

#   # Finetuning network
#   model_type: FullGraphFinetuningNetwork

#   # Optional finetuning head appended to model after finetuning_module
#   finetuning_head:
#     task: lipophilicity_astrazeneca
#     previous_module: task_heads
#     incoming_level: graph
#     model_type: mlp
#     in_dim: 8
#     out_dim: 1
#     hidden_dims: 8
#     depth: 2
#     last_layer_is_readout: true

### PRETRAINING ###

  mup_base_path: null
  pre_nn:
    out_dim: 64
    hidden_dims: 256
    depth: 2
    activation: relu
    last_activation: none
    dropout: &dropout 0.18
    normalization: &normalization layer_norm
    last_normalization: *normalization
    residual_type: none

  pre_nn_edges: null

  pe_encoders:
    out_dim: 32
    pool: "sum" #"mean" "max"
    last_norm: None #"batch_norm", "layer_norm"
    encoders: #la_pos |  rw_pos
      la_pos:  # Set as null to avoid a pre-nn network
        encoder_type: "laplacian_pe"
        input_keys: ["laplacian_eigvec", "laplacian_eigval"]
        output_keys: ["feat"]
        hidden_dim: 64
        out_dim: 32
        model_type: 'DeepSet' #'Transformer' or 'DeepSet'
        num_layers: 2
        num_layers_post: 1 # Num. layers to apply after pooling
        dropout: 0.1
        first_normalization: "none" #"batch_norm" or "layer_norm"
      rw_pos:
        encoder_type: "mlp"
        input_keys: ["rw_return_probs"]
        output_keys: ["feat"]
        hidden_dim: 64
        out_dim: 32
        num_layers: 2
        dropout: 0.1
        normalization: "layer_norm" #"batch_norm" or "layer_norm"
        first_normalization: "layer_norm" #"batch_norm" or "layer_norm"

  gnn:
    in_dim: 64 # or otherwise the correct value
    out_dim: &gnn_dim 96
    hidden_dims: *gnn_dim
    depth: 4
    activation: gelu
    last_activation: none
    dropout: 0.1
    normalization: "layer_norm"
    last_normalization: *normalization
    residual_type: simple
    virtual_node: 'none'
    layer_type: 'pyg:gcn' #pyg:gine #'pyg:gps' # pyg:gated-gcn, pyg:gine,pyg:gps
    layer_kwargs: null # Parameters for the model itself. You could define dropout_attn: 0.1

  graph_output_nn:
    graph:
      pooling: [sum]
      out_dim: *gnn_dim
      hidden_dims: *gnn_dim
      depth: 1
      activation: relu
      last_activation: none
      dropout: *dropout
      normalization: *normalization
      last_normalization: "none"
      residual_type: none

  task_heads:
    qm9:
      task_level: graph
      out_dim: 19
      hidden_dims: 128
      depth: 2
      activation: relu
      last_activation: none
      dropout: *dropout
      normalization: *normalization
      last_normalization: "none"
      residual_type: none
    tox21:
      task_level: graph
      out_dim: 12
      hidden_dims: 64
      depth: 2
      activation: relu
      last_activation: none
      dropout: *dropout
      normalization: *normalization
      last_normalization: "none"
      residual_type: none
    zinc:
      task_level: graph
      out_dim: 3
      hidden_dims: 32
      depth: 2
      activation: relu
      last_activation: none
      dropout: *dropout
      normalization: *normalization
      last_normalization: "none"
      residual_type: none
      last_layer_is_readout: false  # this needs to be also set through finetuning config

  
##################
### DATAMODULE ###
##################

datamodule:

### FINETUNING ###

  module_type: "ADMETBenchmarkDataModule"
  args:
    # TDC specific
    tdc_benchmark_names: [lipophilicity_astrazeneca]
    tdc_train_val_seed: ${constants.seed}
    
    batch_size_training: 200
    batch_size_inference: 200
    featurization_n_jobs: 0
    num_workers: 0

    prepare_dict_or_graph: pyg:graph
    featurization_progress: True
    featurization_backend: "loky"
    processed_graph_data_path: "../datacache/neurips2023-small/"
    persistent_workers: False
    
### PRETRAINING ###

    featurization:
      atom_property_list_onehot: [atomic-number, group, period, total-valence]
      atom_property_list_float: [degree, formal-charge, radical-electron, aromatic, in-ring]
      edge_property_list: [bond-type-onehot, stereo, in-ring]
      add_self_loop: False
      explicit_H: False # if H is included
      use_bonds_weights: False
      pos_encoding_as_features:
        pos_types:
          lap_eigvec:
            pos_level: node
            pos_type: laplacian_eigvec
            num_pos: 8
            normalization: "none" # normalization already applied on the eigen vectors
            disconnected_comp: True # if eigen values/vector for disconnected graph are included
          lap_eigval:
            pos_level: node
            pos_type: laplacian_eigval
            num_pos: 8
            normalization: "none" # normalization already applied on the eigen vectors
            disconnected_comp: True # if eigen values/vector for disconnected graph are included
          rw_pos: # use same name as pe_encoder
            pos_level: node
            pos_type: rw_return_probs
            ksteps: 16



# constants:
#   name: neurips2023_small_data_gcn
#   entity: "finetuning-gnn"
#   seed: 42
#   max_epochs: 100
#   data_dir: expts/data/neurips2023/small-dataset
#   raise_train_error: true

# predictor:
#   random_seed: ${constants.seed}
#   optim_kwargs: {}
#   torch_scheduler_kwargs:
#     max_num_epochs: &max_epochs 100
#   scheduler_kwargs:
#   target_nan_mask: null
#   multitask_handling: flatten # flatten, mean-per-label

# trainer:
#   seed: ${constants.seed}
#   logger:
#     save_dir: logs/neurips2023-small/
#     name: ${constants.name}
#     project: ${constants.name}
#   # model_checkpoint:
#   #   dirpath: saved_models/pretrained_models/
#   #   filename: dummy-pretrained-model-{epoch}
#   #   save_on_train_epoch_end: true
#   trainer:
#     precision: 16
#     max_epochs: *max_epochs
#     min_epochs: 1
#     check_val_every_n_epoch: 20
#     accumulate_grad_batches: 1
#     max_epochs: *max_epochs




