# Testing the gcn model with the PCQMv2 dataset on IPU.
constants:
  name: &name pcqm4m_gcn
  entity: lmueller # Change to your W&B entity
  seed: &seed 42
  raise_train_error: true   # Whether the code should raise an error if it crashes during training

accelerator:
  type: gpu  # cpu or ipu or gpu
  config_override:
    datamodule:
      args:
        # Data handling-related
        batch_size_training: 256 # 128, 256, 384, 512, 1024 # NOTE: There is roughly a 3 in 1000 chance that a vcap or mcf7 graph
        # is in the batch and hence a 1 in 1000 chance that both are. 1024 is chosen such that each
        # batch is likely to contain at least one graph of each dataset.
        batch_size_inference: 256 # 128, 256, 384, 512, 1024
    predictor:
      optim_kwargs: {}
      #loss_scaling: 1024 Not a parameter for AdamW optimizer
      metrics_every_n_steps: 5000
    trainer:
      trainer:
        precision: 32
        accumulate_grad_batches: 1
        #profiler: simple # Enable to get a breakdown of computation times of different parts of training and inference

  ipu_config:
    - deviceIterations(10) # IPU would require large batches to be ready for the model.
    - replicationFactor(16)
    # - enableProfiling("graph_analyser")       # The folder where the profile will be stored
    # - enableExecutableCaching("pop_compiler_cache")
    - TensorLocations.numIOTiles(128)
    - _Popart.set("defaultBufferingDepth", 128)
    - Precision.enableStochasticRounding(True)

# accelerator:
#   type: cpu  # cpu or ipu or gpu
#   config_override:
#     datamodule:
#       batch_size_training: 64
#       batch_size_inference: 256
#     trainer:
#       trainer:
#         precision: 32
#         accumulate_grad_batches: 1

datamodule:
  module_type: "MultitaskFromSmilesDataModule"
  # module_type: "FakeDataModule"  # Option to use generated data
  args: # Matches that in the test_multitask_datamodule.py case.
    task_specific_args:   # To be replaced by a new class "DatasetParams"
      pcqm4m_g25:
        df: null
        df_path: expts/data/neurips2023/large-dataset/PCQM4M_G25_N4.parquet
        # wget https://storage.googleapis.com/graphium-public/datasets/neurips_2023/Large-dataset/PCQM4M_G25_N4.parquet
        # or set path as the URL directly
        smiles_col: "ordered_smiles"
        label_cols: graph_alpha_gap  # graph_* means all columns starting with "graph_"
        # sample_size: 2000 # use sample_size for test
        task_level: graph
        splits_path: /volume/luis.mueller/graphium/datacache/splits/pcqm4m_g25_n4_random_splits.pt  # Download with `wget https://storage.googleapis.com/graphium-public/datasets/neurips_2023/Large-dataset/pcqm4m_g25_n4_random_splits.pt`
        label_normalization:
          normalize_val_test: True
          method: "normal"

    # Featurization
    prepare_dict_or_graph: pyg:graph
    featurization_n_jobs: 4
    featurization_progress: True
    featurization_backend: "loky"
    cache_data_path: /volume/luis.mueller/datacache/cache/pcqm4m
    #processed_graph_data_path: "/volume/luis.mueller/graphium/datacache/pcqm4m/"
    featurization:
    # OGB: ['atomic_num', 'degree', 'possible_formal_charge', 'possible_numH' (total-valence),
    # 'possible_number_radical_e', 'possible_is_aromatic', 'possible_is_in_ring',
    # 'num_chiral_centers (not included yet)']
      atom_property_list_onehot: [atomic-number, group, period, total-valence]
      atom_property_list_float: [degree, formal-charge, radical-electron, aromatic, in-ring]
      # OGB: ['possible_bond_type', 'possible_bond_stereo', 'possible_is_in_ring']
      edge_property_list: [bond-type-onehot, stereo, in-ring]
      add_self_loop: False
      explicit_H: False # if H is included
      use_bonds_weights: False
      pos_encoding_as_features: # encoder dropout 0.18
        pos_types:
          rw_pos: # use same name as pe_encoder
            pos_level: node
            pos_type: rw_return_probs
            ksteps: 16

    # cache_data_path: .
    num_workers: 2 # 4, 8, 16, 24 # -1 to use all
    persistent_workers: False # if use persistent worker at the start of each epoch.
    # Using persistent_workers false might make the start of each epoch very long.
    featurization_backend: "loky"

    # Issues with num_workers and CPU memory
    # https://github.com/pytorch/pytorch/issues/13246#issuecomment-905703662


architecture:
  model_type: FullGraphMultiTaskNetwork
  mup_base_path: null
  pre_nn: null
  pre_nn_edges: null

  pe_encoders:
    out_dim: 20
    pool: "sum" #"mean" "max"
    last_norm: None #"batch_norm", "layer_norm"
    encoders: #la_pos |  rw_pos
      rw_pos:
        encoder_type: "mlp"
        input_keys: ["rw_return_probs"]
        output_keys: ["feat"]
        hidden_dim: 64
        out_dim: 20
        num_layers: 1
        dropout: 0.1
        normalization: "layer_norm" #"batch_norm" or "layer_norm"
        first_normalization: "layer_norm" #"batch_norm" or "layer_norm"



  gnn:  # Set as null to avoid a post-nn network
    in_dim: 105 # or otherwise the correct value
    out_dim: &gnn_dim 304
    hidden_dims: *gnn_dim
    depth: 1
    activation: gelu
    last_activation: none
    dropout: 0.1
    normalization: "layer_norm"
    last_normalization: &normalization layer_norm
    residual_type: simple
    virtual_node: 'none'
    layer_type: 'pyg:gcn' #pyg:gine #'pyg:gps' # pyg:gated-gcn, pyg:gine,pyg:gps



  graph_output_nn:
    graph:
      pooling: [mean]
      out_dim: *gnn_dim
      hidden_dims: *gnn_dim
      depth: 1
      activation: relu
      last_activation: none
      dropout: &dropout 0.0
      normalization: *normalization
      last_normalization: "none"
      residual_type: none

  task_heads:
    pcqm4m_g25:
      task_level: graph
      out_dim: 1
      hidden_dims: 304
      depth: 2
      activation: relu
      last_activation: none
      dropout: *dropout
      normalization: *normalization
      last_normalization: "none"
      residual_type: none

#Task-specific
predictor:
  metrics_on_progress_bar:
    pcqm4m_g25: []
  metrics_on_training_set:
    pcqm4m_g25: []
  loss_fun:
    pcqm4m_g25: mae_ipu
  random_seed: *seed
  optim_kwargs:
    lr: 0.0005 # warmup can be scheduled using torch_scheduler_kwargs
    # weight_decay: 1.e-7
  torch_scheduler_kwargs:
    module_type: WarmUpLinearLR
    max_num_epochs: &max_epochs 1
    warmup_epochs: 0
    verbose: False
  scheduler_kwargs:
  #  monitor: &monitor qm9/mae/train
  #  mode: min
  #  frequency: 1
  target_nan_mask: null # null: no mask, 0: 0 mask, ignore-flatten, ignore-mean-per-label
  multitask_handling: flatten # flatten, mean-per-label

# Task-specific
metrics:
  pcqm4m_g25: &pcqm_metrics
    - name: mae
      metric: mae_ipu
      target_nan_mask: null
      multitask_handling: flatten
      threshold_kwargs: null
    - name: pearsonr
      metric: pearsonr_ipu
      threshold_kwargs: null
      target_nan_mask: null
      multitask_handling: mean-per-label
    - name: r2
      metric: r2_score_ipu
      threshold_kwargs: null
      target_nan_mask: null
      multitask_handling: mean-per-label

trainer:
  seed: *seed
  logger:
    save_dir: logs/pcqm4m-gcn/
    name: *name
    project: *name
  model_checkpoint:
    dirpath: models_checkpoints/pcqm4m-gcn/
    filename: *name
    # monitor: *monitor
    # mode: *mode
    # save_top_k: 1
    save_last: True
  trainer:
    max_epochs: *max_epochs
    min_epochs: 1
    check_val_every_n_epoch: 1
