# @package _global_

predictor:
  random_seed: ${constants.seed}
  optim_kwargs:
    lr: 1.e-4 # warmup can be scheduled using torch_scheduler_kwargs
    # weight_decay: 1.e-7
  torch_scheduler_kwargs:
    module_type: WarmUpLinearLR
    max_num_epochs: ${constants.max_epochs}
    warmup_epochs: 10
    verbose: False
  scheduler_kwargs: null
  target_nan_mask: null # null: no mask, 0: 0 mask, ignore-flatten, ignore-mean-per-label
  multitask_handling: flatten # flatten, mean-per-label

trainer:
  seed: ${constants.seed}
  model_checkpoint:
    dirpath: model_checkpoints/large-dataset/
    filename: best
    save_last: True         # saving last model
    save_top_k: 1           # and best model
    monitor: loss/val       # wrt validation loss
  trainer:
    precision: 16-mixed
    max_epochs: ${constants.max_epochs}
    min_epochs: 1
    check_val_every_n_epoch: 1