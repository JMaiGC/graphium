# @package _global_

# == Fine-tuning configs in Graphium ==
# 
# A fine-tuning config is a appendum to a (pre-)training config.
# Since many things (e.g. the architecture), will stay constant between (pre-)training and fine-tuning,
# this config should be as minimal as possible to avoid unnecessary duplication. It only specifies
# what to override with regards to the config used for (pre-)training.
# 
# Given the following training command: 
# >>> graphium --cfg /path/to/train.yaml
# 
# Fine-tuning now is as easy as: 
# >>> graphium --cfg /path/to/train.yaml +finetune=admet

## == Overrides == 

defaults: 
  - override /tasks: admet

constants:
  name: finetuning_lipophilicity_gcn
  wandb:
    name: ${constants.name}
    project: lipophilicity
    entity: multitask-gnn
    save_dir: logs/lipophilicity
  seed: 42
  max_epochs: 100
  data_dir: expts/data/admet/lipophilicity
  raise_train_error: true

predictor:
  optim_kwargs:
    lr: 4.e-5

# == Fine-tuning config == 

finetuning:

  # For now, we assume a model is always fine-tuned on a single task at a time.
  task: lipophilicity_astrazeneca
  level: graph

  # Pretrained model
  pretrained_model: dummy-pretrained-model-cpu
  finetuning_module: task_heads                
  task_head_from_pretrained: zinc 

  # Changes to finetuning_module                                                 
  drop_depth: 1
  new_out_dim: 8
  added_depth: 2

  # Training
  unfreeze_pretrained_depth: 0
  epoch_unfreeze_all: none

  # Optional finetuning head appended to model after finetuning_module
  finetuning_head:
    task: lipophilicity_astrazeneca
    previous_module: task_heads
    incoming_level: graph
    model_type: mlp
    in_dim: 8
    out_dim: 1
    hidden_dims: 8
    depth: 2
    last_layer_is_readout: true
