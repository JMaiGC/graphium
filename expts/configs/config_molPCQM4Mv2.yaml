# Testing the multitask pipeline with the QM9 dataset on IPU, by splitting it up into three tasks: homo, alpha and cv.
constants:
  name: &name pcqm4mv2_goli_ipu
  seed: &seed 42
  raise_train_error: true   # Whether the code should raise an error if it crashes during training
  accelerator:
    type: ipu  #cpu or ipu or gpu


datamodule:
  module_type: "GraphOGBDataModule"
  args: # Matches that in the test_multitask_datamodule.py case.
    task_specific_args:   # To be replaced by a new class "DatasetParams"
      pcqm4mv2:
        df: null
        dataset_name: "ogbg-lsc-pcqm4mv2"
        sample_size: null

    # Featurization
    prepare_dict_or_graph: pyg:graph
    featurization_n_jobs: 32
    featurization_progress: True
    # featurization_backend: "threads"
    featurization:
      atom_property_list_onehot: [atomic-number, valence]
      atom_property_list_float: [mass, electronegativity, in-ring]
      edge_property_list: [bond-type-onehot, stereo, in-ring]
      add_self_loop: False
      explicit_H: False
      use_bonds_weights: False
      pos_encoding_as_features:
        pos_types:
          la_pos: &pos_enc  #use same name as pe_encoder
            pos_type: laplacian_eigvec_eigval #laplacian_eigvec
            num_pos: 3
            normalization: "none"
            disconnected_comp: True
          rw_pos: #use same name as pe_encoder
            pos_type: rwse
            ksteps: 16
      # pos_encoding_as_directions: *pos_enc # Only for DGN or directional pooling

    # Data handling-related
    batch_size_training: 30
    batch_size_inference: 30
    # cache_data_path: null

    num_workers: 4

    ipu_dataloader_training_opts:
      max_num_nodes_per_graph: &max_nodes 16
      max_num_edges_per_graph: &max_edges 34

    ipu_dataloader_inference_opts:
      max_num_nodes_per_graph: *max_nodes
      max_num_edges_per_graph: *max_edges

architecture:
  model_type: FullGraphMultiTaskNetwork
  pre_nn:   # Set as null to avoid a pre-nn network
    out_dim: &gnn_dim 400
    hidden_dims: *gnn_dim
    depth: 3
    activation: relu
    last_activation: none
    dropout: &dropout 0.1
    normalization: &normalization "none"
    last_normalization: *normalization
    residual_type: none

  pre_nn_edges:   # Set as null to avoid a pre-nn network
    out_dim: 32
    hidden_dims: 32
    depth: 3
    activation: relu
    last_activation: none
    dropout: *dropout
    normalization: *normalization
    last_normalization: *normalization
    residual_type: none

  pe_encoders:
    out_dim: 100
    pool: "sum" #"mean" "max"
    last_norm: None #"batch_norm", "layer_norm"
    encoders: #la_pos |  rw_pos
      la_pos:  # Set as null to avoid a pre-nn network
        encoder_type: "laplacian_pe"
        input_keys: ["eigvecs", "eigvals"]
        hidden_dim: 64
        model_type: 'DeepSet' #'Transformer' or 'DeepSet'
        num_layers: 1
        num_layers_post: 0 # Num. layers to apply after pooling
        dropout: 0.1
        first_normalization: "none" #"batch_norm" or "layer_norm"
      rw_pos:
        encoder_type: "mlp"
        input_keys: ["rwse"]
        output_keys: "node"
        hidden_dim: 64
        num_layers: 1
        dropout: 0.1
        normalization: "none" #"batch_norm" or "layer_norm"
        first_normalization: "none" #"batch_norm" or "layer_norm"


  gnn:  # Set as null to avoid a post-nn network
    out_dim: *gnn_dim
    hidden_dims: *gnn_dim
    depth: 16
    activation: relu
    last_activation: none
    dropout: *dropout
    normalization: *normalization
    last_normalization: *normalization
    residual_type: simple
    pooling: [sum, mean, max]
    virtual_node: 'none'
    layer_type: 'pyg:gps' #pyg:gine #'pyg:gps' # pyg:gated-gcn, pyg:gine,pyg:gps
    layer_kwargs:  # Parameters for the model itself. You could define dropout_attn: 0.1
      mpnn_type: 'pyg:gine'
      mpnn_kwargs: null
        #out_dim_edges: 10
      attn_type: "full-attention" #"none"
      attn_kwargs: null


      # out_dim_edges: 16
      # aggregators: [mean, max]
      # scalers: [identity, amplification, attenuation]
      # num_heads: 3

  post_nn: null
    #out_dim: 1
    #hidden_dims: 32
    #depth: 2
    #activation: relu
    #last_activation: none
    #dropout: *dropout
    #normalization: *normalization
    #last_normalization: "none"
    #residual_type: none

  task_heads:
    - task_name: "pcqm4mv2"
      out_dim: 1
      hidden_dims: *gnn_dim
      depth: 3                          # Not needed if we have hidden_dims
      activation: relu
      last_activation: none
      dropout: *dropout
      normalization: *normalization
      last_normalization: "none"
      residual_type: none

#Task-specific
predictor:
  metrics_on_progress_bar:
    pcqm4mv2: ["mae", "pearsonr"]
  loss_fun:
    pcqm4mv2: mse_ipu
  random_seed: *seed
  optim_kwargs:
    lr: 1.e-5
    # weight_decay: 1.e-7
  torch_scheduler_kwargs:
    #module_type: ReduceLROnPlateau
    #factor: 0.5
    #patience: 7
  scheduler_kwargs: null
  #  monitor: &monitor loss/val
  #  mode: min
  #  frequency: 1
  target_nan_mask: null # null: no mask, 0: 0 mask, ignore: ignore nan values from loss
  flag_kwargs:
    n_steps: 0 #1
    alpha: 0.0 #0.01

# Task-specific
metrics:
  pcqm4mv2:
    - name: mae
      metric: mae
      threshold_kwargs: null
    - name: pearsonr
      metric: pearsonr
      threshold_kwargs: null
      target_nan_mask: ignore-mean-label

trainer:
  logger:
    save_dir: logs/QM9
    name: *name
  #early_stopping:
  #  monitor: *monitor
  #  min_delta: 0
  #  patience: 10
  #  mode: &mode min
  model_checkpoint:
    dirpath: models_checkpoints/QM9/
    filename: *name
    #monitor: *monitor
    #mode: *mode
    save_top_k: 1
    every_n_epochs: 1
  trainer:
    precision: 32
    max_epochs: 20 #50
    min_epochs: 1 #5
    accumulate_grad_batches: 4
