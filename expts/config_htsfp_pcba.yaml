constants:
  seed: &seed 42
  raise_train_error: True   # Whether the code should raise an error if it crashes during training

datamodule:
  module_type: "DGLFromSmilesDataModule"
  args:
    df_path: https://storage.googleapis.com/goli-public/datasets/goli-htsfp-pcba.csv.gz

    cache_data_path: goli/data/cache/htsfp-pcba/full.cache
    label_cols: "assayID-*"
    smiles_col: SMILES

    # Weights
    weights_type: null
    sample_size: null

    # Featurization
    featurization_n_jobs: -1
    featurization_progress: True
    featurization:
      atom_property_list_onehot: [atomic-number, valence]
      atom_property_list_float: [mass, electronegativity, in-ring, hybridization, chirality, aromatic, degree, formal-charge, single-bond, double-bond, radical-electron, vdw-radius, covalent-radius, metal]
      edge_property_list: [bond-type-onehot, bond-type-float, stereo, in-ring, conjugated, estimated-bond-length]
      add_self_loop: False
      explicit_H: False
      use_bonds_weights: False
      pos_encoding_as_features: &pos_enc
        pos_type: laplacian_eigvec
        num_pos: 3
        normalization: "none"
        disconnected_comp: True
      pos_encoding_as_directions: *pos_enc

    # Train, val, test parameters
    split_val: 0.1
    split_test: 0.1
    split_seed: *seed
    batch_size_train_val: 1024
    batch_size_test: 1024

    # Data loading
    num_workers: 0  # On many dataset, no improvement was noticed from more workers, but memory accumulates and explodes.
    pin_memory: True
    persistent_workers: False  # Keep True on Windows if running multiple workers


architecture:
  model_type: fulldglnetwork
  pre_nn:   # Set as null to avoid a pre-nn network
    out_dim: &hidden_dim 400
    hidden_dims: *hidden_dim
    depth: 3
    activation: relu
    last_activation: none
    dropout: &dropout_mlp 0.3
    last_dropout: *dropout_mlp
    batch_norm: &batch_norm True
    last_batch_norm: *batch_norm
    residual_type: simple

  pre_nn_edges:   # Set as null to avoid a pre-nn network
    out_dim: 32
    hidden_dims: 32
    depth: 2
    activation: relu
    last_activation: none
    dropout: *dropout_mlp
    last_dropout: *dropout_mlp
    batch_norm: *batch_norm
    last_batch_norm: *batch_norm
    residual_type: simple

  gnn:  # Set as null to avoid a post-nn network
    out_dim: &out_mlp_dim 700
    hidden_dims: *hidden_dim
    depth: 5
    activation: none
    last_activation: none
    dropout: &dropout_gnn 0.2
    last_dropout: *dropout_gnn
    batch_norm: *batch_norm 
    last_batch_norm: *batch_norm
    residual_type: simple
    pooling: ['sum', 'max']
    virtual_node: 'none'
    layer_type: 'dgn-msgpass'
    layer_kwargs:
      # num_heads: 3
      aggregators: [mean, max, sum, dir1/dx_abs]
      scalers: [identity]
  
  post_nn:
    out_dim: 689
    hidden_dims: *out_mlp_dim
    depth: 3
    activation: relu
    last_activation: sigmoid
    dropout: *dropout_mlp
    last_dropout: 0.
    batch_norm: *batch_norm
    last_batch_norm: False
    residual_type: simple

predictor:
  metrics_on_progress_bar: ["mae", "averageprecision", "auroc"]
  metrics_on_training_set: ["mae", "averageprecision"]
  loss_fun: bce
  random_seed: *seed
  optim_kwargs:
    lr: 5.e-3
    weight_decay: 0
  lr_reduce_on_plateau_kwargs:
    factor: 0.5
    patience: 13
    min_lr: 2.e-4
  scheduler_kwargs:
    monitor: &monitor averageprecision/val
    mode: &mode max
    frequency: 1
  target_nan_mask: ignore # null: no mask, 0: 0 mask, ignore: ignore nan values from loss


metrics:
  - name: mae
    metric: mae
    threshold_kwargs: null
    target_nan_mask: ignore-flatten

  - name: averageprecision
    metric: averageprecision
    target_nan_mask: ignore-mean-label
    threshold_kwargs:
      threshold: null
      th_on_preds: False
      th_on_target: False
      target_to_int: True

  - name: auroc
    metric: auroc
    target_nan_mask: ignore-mean-label
    threshold_kwargs:
      threshold: null
      th_on_preds: False
      th_on_target: False
      target_to_int: True

  - name: f1 > 0.5
    metric: f1
    num_classes: 2
    average: micro
    target_nan_mask: ignore-mean-label
    threshold_kwargs: &threshold_1
      operator: greater
      threshold: 0.5
      th_on_preds: True
      th_on_target: False
      target_to_int: True

  - name: f1 > 0.25
    metric: f1
    num_classes: 2
    average: micro
    target_nan_mask: ignore-mean-label
    threshold_kwargs: &threshold_2
      operator: greater
      threshold: 0.25
      th_on_preds: True
      th_on_target: False
      target_to_int: True

  - name: recall > 0.5
    metric: recall
    target_nan_mask: ignore-mean-label
    threshold_kwargs: *threshold_1

  - name: recall > 0.25
    metric: recall
    target_nan_mask: ignore-mean-label
    threshold_kwargs: *threshold_2

trainer:
  logger:
    save_dir: logs/htsfp-pcba
  early_stopping:
    monitor: *monitor
    min_delta: 0
    patience: 40
    mode: *mode
  model_checkpoint:
    dirpath: models_checkpoints/htsfp-pcba/
    filename: "model"
    monitor: *monitor
    mode: *mode
    save_top_k: 1
    period: 1
  trainer:
    max_epochs: 1000
    min_epochs: 100
    gpus: 1
    accumulate_grad_batches: 1

    