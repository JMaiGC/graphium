constants:
  seed: 42
  exp_name: 'no-name'
  dtype: float32    # float16, float32 or float64
  raise_train_error: true   # Whether the code should raise an error if it crashes during training
  data_device: cpu      # "cpu", "cuda:#" or null
  model_device: cpu  # "cpu", "cuda:#" or null

datasets:
  label_cols: ['score']
  smiles_col: SMILES
  subset_max_size: null    # Maximum number of molecules to take as subset. Useful for debugging purposes
  cache_data_path: null   # TODO: NOT IMPLEMENTED Path where the pre-processed data is saved
  num_workers: -1
  featurization_n_jobs: -1
  featurization:
    atom_property_list_onehot: [atomic-number, valence]
    atom_property_list_float: [mass, electronegativity, in-ring]
    edge_property_list: [bond-type-onehot, stereo, in-ring]
    add_self_loop: False
    explicit_H: False
    use_bonds_weights: False
  train:
    batch_size: 128
    path: data/micro_ZINC.csv
    kwargs:
      sep: ','
    idxfile_or_split: 0.7     # Ignored if a different path is provided. Either the path to a file containing indexes, or the float specifying the split
  val:
    path: data/micro_ZINC.csv
    kwargs:
      sep: ','
    idxfile_or_split: 0.15     # Ignored if a different path is provided. Either the path to a file containing indexes, or the float specifying the split
  test:
    batch_size: 256
    path: data/micro_ZINC.csv
    kwargs:
      sep: ','
    idxfile_or_split: 0.15     # Ignored if a different path is provided. Either the path to a file containing indexes, or the float specifying the split


architecture:
  model_type: fulldglnetwork
  pre_nn_kwargs:
    out_dim: 32
    hidden_dims: 32
    depth: 1
    activation: &activation relu
    last_activation: &last_activation none
    dropout: &dropout 0.1
    batch_norm: &batch_norm True
    residual_type: none
  
  post_nn_kwargs:
    out_dim: 32
    hidden_dims: 32
    depth: 2
    activation: *activation 
    last_activation: *last_activation
    dropout: *dropout 
    batch_norm: *batch_norm 
    residual_type: none

  gnn_kwargs:
    out_dim: 32
    hidden_dims: 32
    depth: 4
    activation: *activation
    last_activation: *last_activation
    dropout: *dropout
    batch_norm: *batch_norm 
    residual_type: simple
    pooling: 'sum'
    virtual_node: 'sum'
    layer_type: 'pna-msgpass'
    aggregators: [mean, max, min, std]
    scalers: [identity, amplification, attenuation]



predictor:
  loss_fun: mse
  optim_kwargs:
    lr: 1.e-3
    weight_decay: 1e-7
  lr_reduce_on_plateau_kwargs:
    factor: 0.5
    patience: 7
  scheduler_kwargs:
    monitor: &monitor val_loss
    frequency: 1
  target_nan_mask: 0 # null: no mask, 0: 0 mask, ignore: ignore nan values from loss


metrics:

  metrics_on_progress_bar: [mae]

  thresholds: &thresholds
    above_th: &above_th True
    ths: &ths [5.]
    th_on_preds: &th_on_preds True
    th_on_target: &th_on_target True

  metrics_dict:
    mae:
      threshold: null
    f1:
      # class_reduction: micro
      threshold: *thresholds
    precision:
      # class_reduction: micro
      threshold: *thresholds
    # auroc:
    #   pos_label: 1
    #   threshold:
    #     above_th: *above_th
    #     ths: *ths
    #     th_on_preds: False
    #     th_on_target: *th_on_target


trainer:
  early_stopping:
    monitor: *monitor
    min_delta: 0
    patience: 10
    mode: &mode min
  model_checkpoint:
    filepath: models_checkpoints/model.ckpt
    monitor: *monitor
    mode: *mode
    save_top_k: 1
    period: 1
  trainer:
    max_epochs: 12
    min_epochs: 5
  tensorboard_logs:
    save_dir: logs

    