
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A deep learning library focused on graph representation learning for real-world chemical tasks.">
      
      
      
        <link rel="canonical" href="https://github.com/valence-discovery/goli/api/goli.nn.html">
      
      <link rel="icon" href="../images/logo.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.5.2">
    
    
      
        <title>goli.nn - goli</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.9f9400aa.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.cbb835fc.min.css">
        
          
          
          <meta name="theme-color" content="#ef5552">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../_assets/css/custom.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="red" data-md-color-accent="indigo">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#goli.nn.architectures.dgl_architectures" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../index.html" title="goli" class="md-header__button md-logo" aria-label="goli" data-md-component="logo">
      
  <img src="../images/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            goli
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              goli.nn
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/valence-discovery/goli" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    valence-discovery/goli
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../index.html" title="goli" class="md-nav__button md-logo" aria-label="goli" data-md-component="logo">
      
  <img src="../images/logo.png" alt="logo">

    </a>
    goli
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/valence-discovery/goli" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    valence-discovery/goli
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../index.html" class="md-nav__link">
        Overview
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          Tutorials
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Tutorials" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Tutorials
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/basics/using_gnn_layers.html" class="md-nav__link">
        Using GNN layers
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/basics/implementing_gnn_layers.html" class="md-nav__link">
        Creating GNN layers
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/basics/making_gnn_networks.html" class="md-nav__link">
        Making GNN Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/model_training/simple-molecular-model.html" class="md-nav__link">
        Building and training a simple model from configurations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/model_training/ipu_training.html" class="md-nav__link">
        Building and training on IPU from configurations
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../design.html" class="md-nav__link">
        Design
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../datasets.html" class="md-nav__link">
        Datasets
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../pretrained_models.html" class="md-nav__link">
        Pretrained Models
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../contribute.html" class="md-nav__link">
        Contribute
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../license.html" class="md-nav__link">
        License
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8" type="checkbox" id="__nav_8" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_8">
          API
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="API" data-md-level="1">
        <label class="md-nav__title" for="__nav_8">
          <span class="md-nav__icon md-icon"></span>
          API
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          goli.nn
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="goli.nn.html" class="md-nav__link md-nav__link--active">
        goli.nn
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#goli.nn.architectures.dgl_architectures" class="md-nav__link">
    dgl_architectures
  </a>
  
    <nav class="md-nav" aria-label="dgl_architectures">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.dgl_architectures.FeedForwardDGL" class="md-nav__link">
    FeedForwardDGL
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures" class="md-nav__link">
    global_architectures
  </a>
  
    <nav class="md-nav" aria-label="global_architectures">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FeedForwardGraphBase" class="md-nav__link">
    FeedForwardGraphBase
  </a>
  
    <nav class="md-nav" aria-label="FeedForwardGraphBase">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FeedForwardGraphBase.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FeedForwardGraphBase.__repr__" class="md-nav__link">
    __repr__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FeedForwardGraphBase.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FeedForwardNN" class="md-nav__link">
    FeedForwardNN
  </a>
  
    <nav class="md-nav" aria-label="FeedForwardNN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FeedForwardNN.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FeedForwardNN.__repr__" class="md-nav__link">
    __repr__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FeedForwardNN.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphMultiTaskNetwork" class="md-nav__link">
    FullGraphMultiTaskNetwork
  </a>
  
    <nav class="md-nav" aria-label="FullGraphMultiTaskNetwork">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphMultiTaskNetwork.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphMultiTaskNetwork.out_dim" class="md-nav__link">
    out_dim()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphNetwork" class="md-nav__link">
    FullGraphNetwork
  </a>
  
    <nav class="md-nav" aria-label="FullGraphNetwork">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.__repr__" class="md-nav__link">
    __repr__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.concat_last_layers" class="md-nav__link">
    concat_last_layers()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.drop_post_nn_layers" class="md-nav__link">
    drop_post_nn_layers()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.dtype" class="md-nav__link">
    dtype()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.extend_post_nn_layers" class="md-nav__link">
    extend_post_nn_layers()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.forward_node_positional_encoding" class="md-nav__link">
    forward_node_positional_encoding()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.forward_simple_pooling" class="md-nav__link">
    forward_simple_pooling()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.in_dim" class="md-nav__link">
    in_dim()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.in_dim_edges" class="md-nav__link">
    in_dim_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.out_dim" class="md-nav__link">
    out_dim()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.TaskHead" class="md-nav__link">
    TaskHead
  </a>
  
    <nav class="md-nav" aria-label="TaskHead">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.TaskHead.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.TaskHeads" class="md-nav__link">
    TaskHeads
  </a>
  
    <nav class="md-nav" aria-label="TaskHeads">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.TaskHeads.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.TaskHeads.out_dim" class="md-nav__link">
    out_dim()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.architectures.pyg_architectures" class="md-nav__link">
    pyg_architectures
  </a>
  
    <nav class="md-nav" aria-label="pyg_architectures">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.pyg_architectures.FeedForwardPyg" class="md-nav__link">
    FeedForwardPyg
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_dgl" class="md-nav__link">
    dgn_dgl
  </a>
  
    <nav class="md-nav" aria-label="dgn_dgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_dgl.BaseDGNDgl" class="md-nav__link">
    BaseDGNDgl
  </a>
  
    <nav class="md-nav" aria-label="BaseDGNDgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_dgl.BaseDGNDgl.message_func" class="md-nav__link">
    message_func()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_dgl.BaseDGNDgl.parse_aggregators" class="md-nav__link">
    parse_aggregators()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_dgl.BaseDGNDgl.reduce_func" class="md-nav__link">
    reduce_func()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_dgl.DGNConvolutionalDgl" class="md-nav__link">
    DGNConvolutionalDgl
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_dgl.DGNMessagePassingDgl" class="md-nav__link">
    DGNMessagePassingDgl
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_operations" class="md-nav__link">
    dgn_operations
  </a>
  
    <nav class="md-nav" aria-label="dgn_operations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_operations.aggregate_dir_backward" class="md-nav__link">
    aggregate_dir_backward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_operations.aggregate_dir_dx_abs" class="md-nav__link">
    aggregate_dir_dx_abs()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_operations.aggregate_dir_dx_abs_balanced" class="md-nav__link">
    aggregate_dir_dx_abs_balanced()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_operations.aggregate_dir_dx_no_abs" class="md-nav__link">
    aggregate_dir_dx_no_abs()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_operations.aggregate_dir_forward" class="md-nav__link">
    aggregate_dir_forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_operations.aggregate_dir_smooth" class="md-nav__link">
    aggregate_dir_smooth()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_operations.get_grad_of_pos" class="md-nav__link">
    get_grad_of_pos()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gat_dgl" class="md-nav__link">
    gat_dgl
  </a>
  
    <nav class="md-nav" aria-label="gat_dgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gat_dgl.GATDgl" class="md-nav__link">
    GATDgl
  </a>
  
    <nav class="md-nav" aria-label="GATDgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gat_dgl.GATDgl.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gat_dgl.GATDgl.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gat_dgl.GATDgl.layer_inputs_edges" class="md-nav__link">
    layer_inputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gat_dgl.GATDgl.layer_outputs_edges" class="md-nav__link">
    layer_outputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gat_dgl.GATDgl.layer_supports_edges" class="md-nav__link">
    layer_supports_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gat_dgl.GATDgl.out_dim_factor" class="md-nav__link">
    out_dim_factor()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gated_gcn_dgl" class="md-nav__link">
    gated_gcn_dgl
  </a>
  
    <nav class="md-nav" aria-label="gated_gcn_dgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl" class="md-nav__link">
    GatedGCNDgl
  </a>
  
    <nav class="md-nav" aria-label="GatedGCNDgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl.layer_inputs_edges" class="md-nav__link">
    layer_inputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl.layer_outputs_edges" class="md-nav__link">
    layer_outputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl.layer_supports_edges" class="md-nav__link">
    layer_supports_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl.out_dim_factor" class="md-nav__link">
    out_dim_factor()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gcn_dgl" class="md-nav__link">
    gcn_dgl
  </a>
  
    <nav class="md-nav" aria-label="gcn_dgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gcn_dgl.GCNDgl" class="md-nav__link">
    GCNDgl
  </a>
  
    <nav class="md-nav" aria-label="GCNDgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gcn_dgl.GCNDgl.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gcn_dgl.GCNDgl.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gcn_dgl.GCNDgl.layer_inputs_edges" class="md-nav__link">
    layer_inputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gcn_dgl.GCNDgl.layer_outputs_edges" class="md-nav__link">
    layer_outputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gcn_dgl.GCNDgl.layer_supports_edges" class="md-nav__link">
    layer_supports_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gcn_dgl.GCNDgl.out_dim_factor" class="md-nav__link">
    out_dim_factor()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gin_dgl" class="md-nav__link">
    gin_dgl
  </a>
  
    <nav class="md-nav" aria-label="gin_dgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gin_dgl.GINDgl" class="md-nav__link">
    GINDgl
  </a>
  
    <nav class="md-nav" aria-label="GINDgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gin_dgl.GINDgl.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gin_dgl.GINDgl.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gin_dgl.GINDgl.layer_inputs_edges" class="md-nav__link">
    layer_inputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gin_dgl.GINDgl.layer_outputs_edges" class="md-nav__link">
    layer_outputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gin_dgl.GINDgl.layer_supports_edges" class="md-nav__link">
    layer_supports_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gin_dgl.GINDgl.message_func" class="md-nav__link">
    message_func()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gin_dgl.GINDgl.out_dim_factor" class="md-nav__link">
    out_dim_factor()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl" class="md-nav__link">
    pna_dgl
  </a>
  
    <nav class="md-nav" aria-label="pna_dgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl" class="md-nav__link">
    BasePNADgl
  </a>
  
    <nav class="md-nav" aria-label="BasePNADgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.add_virtual_graph_if_no_edges" class="md-nav__link">
    add_virtual_graph_if_no_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.layer_inputs_edges" class="md-nav__link">
    layer_inputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.layer_outputs_edges" class="md-nav__link">
    layer_outputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.layer_supports_edges" class="md-nav__link">
    layer_supports_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.message_func" class="md-nav__link">
    message_func()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.out_dim_factor" class="md-nav__link">
    out_dim_factor()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.reduce_func" class="md-nav__link">
    reduce_func()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.remove_virtual_graph_if_no_edges" class="md-nav__link">
    remove_virtual_graph_if_no_edges()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.PNAConvolutionalDgl" class="md-nav__link">
    PNAConvolutionalDgl
  </a>
  
    <nav class="md-nav" aria-label="PNAConvolutionalDgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.PNAConvolutionalDgl.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.PNAConvolutionalDgl.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.PNAConvolutionalDgl.pretrans_edges" class="md-nav__link">
    pretrans_edges()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.PNAMessagePassingDgl" class="md-nav__link">
    PNAMessagePassingDgl
  </a>
  
    <nav class="md-nav" aria-label="PNAMessagePassingDgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.PNAMessagePassingDgl.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.PNAMessagePassingDgl.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.PNAMessagePassingDgl.layer_supports_edges" class="md-nav__link">
    layer_supports_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.PNAMessagePassingDgl.pretrans_edges" class="md-nav__link">
    pretrans_edges()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_operations" class="md-nav__link">
    pna_operations
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl" class="md-nav__link">
    pooling_dgl
  </a>
  
    <nav class="md-nav" aria-label="pooling_dgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl.DirPoolingDgl" class="md-nav__link">
    DirPoolingDgl
  </a>
  
    <nav class="md-nav" aria-label="DirPoolingDgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl.DirPoolingDgl.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl.LogSumPoolingDgl" class="md-nav__link">
    LogSumPoolingDgl
  </a>
  
    <nav class="md-nav" aria-label="LogSumPoolingDgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl.LogSumPoolingDgl.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl.MinPoolingDgl" class="md-nav__link">
    MinPoolingDgl
  </a>
  
    <nav class="md-nav" aria-label="MinPoolingDgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl.MinPoolingDgl.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl.S2SReadoutDgl" class="md-nav__link">
    S2SReadoutDgl
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl.StdPoolingDgl" class="md-nav__link">
    StdPoolingDgl
  </a>
  
    <nav class="md-nav" aria-label="StdPoolingDgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl.StdPoolingDgl.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl.VirtualNodeDgl" class="md-nav__link">
    VirtualNodeDgl
  </a>
  
    <nav class="md-nav" aria-label="VirtualNodeDgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl.VirtualNodeDgl.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl.VirtualNodeDgl.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl.parse_pooling_layer_dgl" class="md-nav__link">
    parse_pooling_layer_dgl()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.encoders.laplace_pos_encoder" class="md-nav__link">
    laplace_pos_encoder
  </a>
  
    <nav class="md-nav" aria-label="laplace_pos_encoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.encoders.laplace_pos_encoder.LapPENodeEncoder" class="md-nav__link">
    LapPENodeEncoder
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.encoders.mlp_encoder" class="md-nav__link">
    mlp_encoder
  </a>
  
    <nav class="md-nav" aria-label="mlp_encoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.encoders.mlp_encoder.MLPEncoder" class="md-nav__link">
    MLPEncoder
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.encoders.signnet_pos_encoder" class="md-nav__link">
    signnet_pos_encoder
  </a>
  
    <nav class="md-nav" aria-label="signnet_pos_encoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.encoders.signnet_pos_encoder.GINDeepSigns" class="md-nav__link">
    GINDeepSigns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.encoders.signnet_pos_encoder.MaskedGINDeepSigns" class="md-nav__link">
    MaskedGINDeepSigns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.encoders.signnet_pos_encoder.SignNetNodeEncoder" class="md-nav__link">
    SignNetNodeEncoder
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gated_gcn_pyg" class="md-nav__link">
    gated_gcn_pyg
  </a>
  
    <nav class="md-nav" aria-label="gated_gcn_pyg">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg" class="md-nav__link">
    GatedGCNPyg
  </a>
  
    <nav class="md-nav" aria-label="GatedGCNPyg">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.aggregate" class="md-nav__link">
    aggregate()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.layer_inputs_edges" class="md-nav__link">
    layer_inputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.layer_outputs_edges" class="md-nav__link">
    layer_outputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.layer_supports_edges" class="md-nav__link">
    layer_supports_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.message" class="md-nav__link">
    message()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.out_dim_factor" class="md-nav__link">
    out_dim_factor()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.update" class="md-nav__link">
    update()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gin_pyg" class="md-nav__link">
    gin_pyg
  </a>
  
    <nav class="md-nav" aria-label="gin_pyg">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gin_pyg.GINConvPyg" class="md-nav__link">
    GINConvPyg
  </a>
  
    <nav class="md-nav" aria-label="GINConvPyg">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gin_pyg.GINConvPyg.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gin_pyg.GINConvPyg.layer_inputs_edges" class="md-nav__link">
    layer_inputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gin_pyg.GINConvPyg.layer_outputs_edges" class="md-nav__link">
    layer_outputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gin_pyg.GINConvPyg.layer_supports_edges" class="md-nav__link">
    layer_supports_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gin_pyg.GINConvPyg.out_dim_factor" class="md-nav__link">
    out_dim_factor()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gin_pyg.GINEConvPyg" class="md-nav__link">
    GINEConvPyg
  </a>
  
    <nav class="md-nav" aria-label="GINEConvPyg">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gin_pyg.GINEConvPyg.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gin_pyg.GINEConvPyg.layer_inputs_edges" class="md-nav__link">
    layer_inputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gin_pyg.GINEConvPyg.layer_outputs_edges" class="md-nav__link">
    layer_outputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gin_pyg.GINEConvPyg.layer_supports_edges" class="md-nav__link">
    layer_supports_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gin_pyg.GINEConvPyg.out_dim_factor" class="md-nav__link">
    out_dim_factor()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gps_pyg" class="md-nav__link">
    gps_pyg
  </a>
  
    <nav class="md-nav" aria-label="gps_pyg">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gps_pyg.GPSLayerPyg" class="md-nav__link">
    GPSLayerPyg
  </a>
  
    <nav class="md-nav" aria-label="GPSLayerPyg">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gps_pyg.GPSLayerPyg.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gps_pyg.GPSLayerPyg.layer_inputs_edges" class="md-nav__link">
    layer_inputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gps_pyg.GPSLayerPyg.layer_outputs_edges" class="md-nav__link">
    layer_outputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gps_pyg.GPSLayerPyg.layer_supports_edges" class="md-nav__link">
    layer_supports_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gps_pyg.GPSLayerPyg.out_dim_factor" class="md-nav__link">
    out_dim_factor()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pna_pyg" class="md-nav__link">
    pna_pyg
  </a>
  
    <nav class="md-nav" aria-label="pna_pyg">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg" class="md-nav__link">
    PNAMessagePassingPyg
  </a>
  
    <nav class="md-nav" aria-label="PNAMessagePassingPyg">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.layer_inputs_edges" class="md-nav__link">
    layer_inputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.layer_outputs_edges" class="md-nav__link">
    layer_outputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.layer_supports_edges" class="md-nav__link">
    layer_supports_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.out_dim_factor" class="md-nav__link">
    out_dim_factor()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pooling_pyg" class="md-nav__link">
    pooling_pyg
  </a>
  
    <nav class="md-nav" aria-label="pooling_pyg">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pooling_pyg.VirtualNodePyg" class="md-nav__link">
    VirtualNodePyg
  </a>
  
    <nav class="md-nav" aria-label="VirtualNodePyg">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pooling_pyg.VirtualNodePyg.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pooling_pyg.VirtualNodePyg.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pooling_pyg.parse_pooling_layer_pyg" class="md-nav__link">
    parse_pooling_layer_pyg()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pooling_pyg.scatter_logsum_pool" class="md-nav__link">
    scatter_logsum_pool()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pooling_pyg.scatter_std_pool" class="md-nav__link">
    scatter_std_pool()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.base_graph_layer" class="md-nav__link">
    base_graph_layer
  </a>
  
    <nav class="md-nav" aria-label="base_graph_layer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_graph_layer.BaseGraphModule" class="md-nav__link">
    BaseGraphModule
  </a>
  
    <nav class="md-nav" aria-label="BaseGraphModule">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_graph_layer.BaseGraphModule.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_graph_layer.BaseGraphStructure" class="md-nav__link">
    BaseGraphStructure
  </a>
  
    <nav class="md-nav" aria-label="BaseGraphStructure">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_graph_layer.BaseGraphStructure.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_graph_layer.BaseGraphStructure.__repr__" class="md-nav__link">
    __repr__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_graph_layer.BaseGraphStructure.apply_norm_activation_dropout" class="md-nav__link">
    apply_norm_activation_dropout()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_graph_layer.BaseGraphStructure.layer_inputs_edges" class="md-nav__link">
    layer_inputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_graph_layer.BaseGraphStructure.layer_outputs_edges" class="md-nav__link">
    layer_outputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_graph_layer.BaseGraphStructure.layer_supports_edges" class="md-nav__link">
    layer_supports_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_graph_layer.BaseGraphStructure.out_dim_factor" class="md-nav__link">
    out_dim_factor()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_graph_layer.check_intpus_allow_int" class="md-nav__link">
    check_intpus_allow_int()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.base_layers" class="md-nav__link">
    base_layers
  </a>
  
    <nav class="md-nav" aria-label="base_layers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.FCLayer" class="md-nav__link">
    FCLayer
  </a>
  
    <nav class="md-nav" aria-label="FCLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.FCLayer.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.FCLayer.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.FCLayer.in_channels" class="md-nav__link">
    in_channels()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.FCLayer.out_channels" class="md-nav__link">
    out_channels()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.GRU" class="md-nav__link">
    GRU
  </a>
  
    <nav class="md-nav" aria-label="GRU">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.GRU.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.GRU.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.MLP" class="md-nav__link">
    MLP
  </a>
  
    <nav class="md-nav" aria-label="MLP">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.MLP.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.MLP.__repr__" class="md-nav__link">
    __repr__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.MLP.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.get_activation" class="md-nav__link">
    get_activation()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.get_norm" class="md-nav__link">
    get_norm()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.residual_connections" class="md-nav__link">
    residual_connections
  </a>
  
    <nav class="md-nav" aria-label="residual_connections">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionBase" class="md-nav__link">
    ResidualConnectionBase
  </a>
  
    <nav class="md-nav" aria-label="ResidualConnectionBase">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionBase.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionBase.__repr__" class="md-nav__link">
    __repr__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionBase.h_dim_increase_type" class="md-nav__link">
    h_dim_increase_type()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionBase.has_weights" class="md-nav__link">
    has_weights()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionConcat" class="md-nav__link">
    ResidualConnectionConcat
  </a>
  
    <nav class="md-nav" aria-label="ResidualConnectionConcat">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionConcat.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionConcat.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionConcat.h_dim_increase_type" class="md-nav__link">
    h_dim_increase_type()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionConcat.has_weights" class="md-nav__link">
    has_weights()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionDenseNet" class="md-nav__link">
    ResidualConnectionDenseNet
  </a>
  
    <nav class="md-nav" aria-label="ResidualConnectionDenseNet">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionDenseNet.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionDenseNet.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionDenseNet.h_dim_increase_type" class="md-nav__link">
    h_dim_increase_type()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionDenseNet.has_weights" class="md-nav__link">
    has_weights()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionNone" class="md-nav__link">
    ResidualConnectionNone
  </a>
  
    <nav class="md-nav" aria-label="ResidualConnectionNone">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionNone.__repr__" class="md-nav__link">
    __repr__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionNone.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionNone.h_dim_increase_type" class="md-nav__link">
    h_dim_increase_type()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionNone.has_weights" class="md-nav__link">
    has_weights()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionRandom" class="md-nav__link">
    ResidualConnectionRandom
  </a>
  
    <nav class="md-nav" aria-label="ResidualConnectionRandom">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionRandom.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionRandom.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionRandom.h_dim_increase_type" class="md-nav__link">
    h_dim_increase_type()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionRandom.has_weights" class="md-nav__link">
    has_weights()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionSimple" class="md-nav__link">
    ResidualConnectionSimple
  </a>
  
    <nav class="md-nav" aria-label="ResidualConnectionSimple">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionSimple.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionSimple.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionSimple.h_dim_increase_type" class="md-nav__link">
    h_dim_increase_type()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionSimple.has_weights" class="md-nav__link">
    has_weights()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionWeighted" class="md-nav__link">
    ResidualConnectionWeighted
  </a>
  
    <nav class="md-nav" aria-label="ResidualConnectionWeighted">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionWeighted.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionWeighted.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionWeighted.h_dim_increase_type" class="md-nav__link">
    h_dim_increase_type()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionWeighted.has_weights" class="md-nav__link">
    has_weights()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="goli.features.html" class="md-nav__link">
        goli.features
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="goli.trainer.html" class="md-nav__link">
        goli.trainer
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="goli.data.html" class="md-nav__link">
        goli.data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="goli.utils.html" class="md-nav__link">
        goli.utils
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="goli.config.html" class="md-nav__link">
        goli.config
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="goli.ipu.html" class="md-nav__link">
        goli.ipu
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../cli_references.html" class="md-nav__link">
        CLI
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#goli.nn.architectures.dgl_architectures" class="md-nav__link">
    dgl_architectures
  </a>
  
    <nav class="md-nav" aria-label="dgl_architectures">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.dgl_architectures.FeedForwardDGL" class="md-nav__link">
    FeedForwardDGL
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures" class="md-nav__link">
    global_architectures
  </a>
  
    <nav class="md-nav" aria-label="global_architectures">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FeedForwardGraphBase" class="md-nav__link">
    FeedForwardGraphBase
  </a>
  
    <nav class="md-nav" aria-label="FeedForwardGraphBase">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FeedForwardGraphBase.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FeedForwardGraphBase.__repr__" class="md-nav__link">
    __repr__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FeedForwardGraphBase.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FeedForwardNN" class="md-nav__link">
    FeedForwardNN
  </a>
  
    <nav class="md-nav" aria-label="FeedForwardNN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FeedForwardNN.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FeedForwardNN.__repr__" class="md-nav__link">
    __repr__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FeedForwardNN.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphMultiTaskNetwork" class="md-nav__link">
    FullGraphMultiTaskNetwork
  </a>
  
    <nav class="md-nav" aria-label="FullGraphMultiTaskNetwork">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphMultiTaskNetwork.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphMultiTaskNetwork.out_dim" class="md-nav__link">
    out_dim()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphNetwork" class="md-nav__link">
    FullGraphNetwork
  </a>
  
    <nav class="md-nav" aria-label="FullGraphNetwork">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.__repr__" class="md-nav__link">
    __repr__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.concat_last_layers" class="md-nav__link">
    concat_last_layers()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.drop_post_nn_layers" class="md-nav__link">
    drop_post_nn_layers()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.dtype" class="md-nav__link">
    dtype()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.extend_post_nn_layers" class="md-nav__link">
    extend_post_nn_layers()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.forward_node_positional_encoding" class="md-nav__link">
    forward_node_positional_encoding()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.forward_simple_pooling" class="md-nav__link">
    forward_simple_pooling()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.in_dim" class="md-nav__link">
    in_dim()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.in_dim_edges" class="md-nav__link">
    in_dim_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.out_dim" class="md-nav__link">
    out_dim()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.TaskHead" class="md-nav__link">
    TaskHead
  </a>
  
    <nav class="md-nav" aria-label="TaskHead">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.TaskHead.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.TaskHeads" class="md-nav__link">
    TaskHeads
  </a>
  
    <nav class="md-nav" aria-label="TaskHeads">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.TaskHeads.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.global_architectures.TaskHeads.out_dim" class="md-nav__link">
    out_dim()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.architectures.pyg_architectures" class="md-nav__link">
    pyg_architectures
  </a>
  
    <nav class="md-nav" aria-label="pyg_architectures">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.architectures.pyg_architectures.FeedForwardPyg" class="md-nav__link">
    FeedForwardPyg
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_dgl" class="md-nav__link">
    dgn_dgl
  </a>
  
    <nav class="md-nav" aria-label="dgn_dgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_dgl.BaseDGNDgl" class="md-nav__link">
    BaseDGNDgl
  </a>
  
    <nav class="md-nav" aria-label="BaseDGNDgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_dgl.BaseDGNDgl.message_func" class="md-nav__link">
    message_func()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_dgl.BaseDGNDgl.parse_aggregators" class="md-nav__link">
    parse_aggregators()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_dgl.BaseDGNDgl.reduce_func" class="md-nav__link">
    reduce_func()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_dgl.DGNConvolutionalDgl" class="md-nav__link">
    DGNConvolutionalDgl
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_dgl.DGNMessagePassingDgl" class="md-nav__link">
    DGNMessagePassingDgl
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_operations" class="md-nav__link">
    dgn_operations
  </a>
  
    <nav class="md-nav" aria-label="dgn_operations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_operations.aggregate_dir_backward" class="md-nav__link">
    aggregate_dir_backward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_operations.aggregate_dir_dx_abs" class="md-nav__link">
    aggregate_dir_dx_abs()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_operations.aggregate_dir_dx_abs_balanced" class="md-nav__link">
    aggregate_dir_dx_abs_balanced()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_operations.aggregate_dir_dx_no_abs" class="md-nav__link">
    aggregate_dir_dx_no_abs()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_operations.aggregate_dir_forward" class="md-nav__link">
    aggregate_dir_forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_operations.aggregate_dir_smooth" class="md-nav__link">
    aggregate_dir_smooth()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.dgn_operations.get_grad_of_pos" class="md-nav__link">
    get_grad_of_pos()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gat_dgl" class="md-nav__link">
    gat_dgl
  </a>
  
    <nav class="md-nav" aria-label="gat_dgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gat_dgl.GATDgl" class="md-nav__link">
    GATDgl
  </a>
  
    <nav class="md-nav" aria-label="GATDgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gat_dgl.GATDgl.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gat_dgl.GATDgl.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gat_dgl.GATDgl.layer_inputs_edges" class="md-nav__link">
    layer_inputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gat_dgl.GATDgl.layer_outputs_edges" class="md-nav__link">
    layer_outputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gat_dgl.GATDgl.layer_supports_edges" class="md-nav__link">
    layer_supports_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gat_dgl.GATDgl.out_dim_factor" class="md-nav__link">
    out_dim_factor()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gated_gcn_dgl" class="md-nav__link">
    gated_gcn_dgl
  </a>
  
    <nav class="md-nav" aria-label="gated_gcn_dgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl" class="md-nav__link">
    GatedGCNDgl
  </a>
  
    <nav class="md-nav" aria-label="GatedGCNDgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl.layer_inputs_edges" class="md-nav__link">
    layer_inputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl.layer_outputs_edges" class="md-nav__link">
    layer_outputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl.layer_supports_edges" class="md-nav__link">
    layer_supports_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl.out_dim_factor" class="md-nav__link">
    out_dim_factor()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gcn_dgl" class="md-nav__link">
    gcn_dgl
  </a>
  
    <nav class="md-nav" aria-label="gcn_dgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gcn_dgl.GCNDgl" class="md-nav__link">
    GCNDgl
  </a>
  
    <nav class="md-nav" aria-label="GCNDgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gcn_dgl.GCNDgl.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gcn_dgl.GCNDgl.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gcn_dgl.GCNDgl.layer_inputs_edges" class="md-nav__link">
    layer_inputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gcn_dgl.GCNDgl.layer_outputs_edges" class="md-nav__link">
    layer_outputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gcn_dgl.GCNDgl.layer_supports_edges" class="md-nav__link">
    layer_supports_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gcn_dgl.GCNDgl.out_dim_factor" class="md-nav__link">
    out_dim_factor()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gin_dgl" class="md-nav__link">
    gin_dgl
  </a>
  
    <nav class="md-nav" aria-label="gin_dgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gin_dgl.GINDgl" class="md-nav__link">
    GINDgl
  </a>
  
    <nav class="md-nav" aria-label="GINDgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gin_dgl.GINDgl.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gin_dgl.GINDgl.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gin_dgl.GINDgl.layer_inputs_edges" class="md-nav__link">
    layer_inputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gin_dgl.GINDgl.layer_outputs_edges" class="md-nav__link">
    layer_outputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gin_dgl.GINDgl.layer_supports_edges" class="md-nav__link">
    layer_supports_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gin_dgl.GINDgl.message_func" class="md-nav__link">
    message_func()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.gin_dgl.GINDgl.out_dim_factor" class="md-nav__link">
    out_dim_factor()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl" class="md-nav__link">
    pna_dgl
  </a>
  
    <nav class="md-nav" aria-label="pna_dgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl" class="md-nav__link">
    BasePNADgl
  </a>
  
    <nav class="md-nav" aria-label="BasePNADgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.add_virtual_graph_if_no_edges" class="md-nav__link">
    add_virtual_graph_if_no_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.layer_inputs_edges" class="md-nav__link">
    layer_inputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.layer_outputs_edges" class="md-nav__link">
    layer_outputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.layer_supports_edges" class="md-nav__link">
    layer_supports_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.message_func" class="md-nav__link">
    message_func()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.out_dim_factor" class="md-nav__link">
    out_dim_factor()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.reduce_func" class="md-nav__link">
    reduce_func()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.remove_virtual_graph_if_no_edges" class="md-nav__link">
    remove_virtual_graph_if_no_edges()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.PNAConvolutionalDgl" class="md-nav__link">
    PNAConvolutionalDgl
  </a>
  
    <nav class="md-nav" aria-label="PNAConvolutionalDgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.PNAConvolutionalDgl.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.PNAConvolutionalDgl.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.PNAConvolutionalDgl.pretrans_edges" class="md-nav__link">
    pretrans_edges()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.PNAMessagePassingDgl" class="md-nav__link">
    PNAMessagePassingDgl
  </a>
  
    <nav class="md-nav" aria-label="PNAMessagePassingDgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.PNAMessagePassingDgl.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.PNAMessagePassingDgl.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.PNAMessagePassingDgl.layer_supports_edges" class="md-nav__link">
    layer_supports_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_dgl.PNAMessagePassingDgl.pretrans_edges" class="md-nav__link">
    pretrans_edges()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pna_operations" class="md-nav__link">
    pna_operations
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl" class="md-nav__link">
    pooling_dgl
  </a>
  
    <nav class="md-nav" aria-label="pooling_dgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl.DirPoolingDgl" class="md-nav__link">
    DirPoolingDgl
  </a>
  
    <nav class="md-nav" aria-label="DirPoolingDgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl.DirPoolingDgl.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl.LogSumPoolingDgl" class="md-nav__link">
    LogSumPoolingDgl
  </a>
  
    <nav class="md-nav" aria-label="LogSumPoolingDgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl.LogSumPoolingDgl.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl.MinPoolingDgl" class="md-nav__link">
    MinPoolingDgl
  </a>
  
    <nav class="md-nav" aria-label="MinPoolingDgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl.MinPoolingDgl.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl.S2SReadoutDgl" class="md-nav__link">
    S2SReadoutDgl
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl.StdPoolingDgl" class="md-nav__link">
    StdPoolingDgl
  </a>
  
    <nav class="md-nav" aria-label="StdPoolingDgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl.StdPoolingDgl.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl.VirtualNodeDgl" class="md-nav__link">
    VirtualNodeDgl
  </a>
  
    <nav class="md-nav" aria-label="VirtualNodeDgl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl.VirtualNodeDgl.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl.VirtualNodeDgl.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.dgl_layers.pooling_dgl.parse_pooling_layer_dgl" class="md-nav__link">
    parse_pooling_layer_dgl()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.encoders.laplace_pos_encoder" class="md-nav__link">
    laplace_pos_encoder
  </a>
  
    <nav class="md-nav" aria-label="laplace_pos_encoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.encoders.laplace_pos_encoder.LapPENodeEncoder" class="md-nav__link">
    LapPENodeEncoder
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.encoders.mlp_encoder" class="md-nav__link">
    mlp_encoder
  </a>
  
    <nav class="md-nav" aria-label="mlp_encoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.encoders.mlp_encoder.MLPEncoder" class="md-nav__link">
    MLPEncoder
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.encoders.signnet_pos_encoder" class="md-nav__link">
    signnet_pos_encoder
  </a>
  
    <nav class="md-nav" aria-label="signnet_pos_encoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.encoders.signnet_pos_encoder.GINDeepSigns" class="md-nav__link">
    GINDeepSigns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.encoders.signnet_pos_encoder.MaskedGINDeepSigns" class="md-nav__link">
    MaskedGINDeepSigns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.encoders.signnet_pos_encoder.SignNetNodeEncoder" class="md-nav__link">
    SignNetNodeEncoder
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gated_gcn_pyg" class="md-nav__link">
    gated_gcn_pyg
  </a>
  
    <nav class="md-nav" aria-label="gated_gcn_pyg">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg" class="md-nav__link">
    GatedGCNPyg
  </a>
  
    <nav class="md-nav" aria-label="GatedGCNPyg">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.aggregate" class="md-nav__link">
    aggregate()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.layer_inputs_edges" class="md-nav__link">
    layer_inputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.layer_outputs_edges" class="md-nav__link">
    layer_outputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.layer_supports_edges" class="md-nav__link">
    layer_supports_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.message" class="md-nav__link">
    message()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.out_dim_factor" class="md-nav__link">
    out_dim_factor()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.update" class="md-nav__link">
    update()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gin_pyg" class="md-nav__link">
    gin_pyg
  </a>
  
    <nav class="md-nav" aria-label="gin_pyg">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gin_pyg.GINConvPyg" class="md-nav__link">
    GINConvPyg
  </a>
  
    <nav class="md-nav" aria-label="GINConvPyg">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gin_pyg.GINConvPyg.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gin_pyg.GINConvPyg.layer_inputs_edges" class="md-nav__link">
    layer_inputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gin_pyg.GINConvPyg.layer_outputs_edges" class="md-nav__link">
    layer_outputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gin_pyg.GINConvPyg.layer_supports_edges" class="md-nav__link">
    layer_supports_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gin_pyg.GINConvPyg.out_dim_factor" class="md-nav__link">
    out_dim_factor()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gin_pyg.GINEConvPyg" class="md-nav__link">
    GINEConvPyg
  </a>
  
    <nav class="md-nav" aria-label="GINEConvPyg">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gin_pyg.GINEConvPyg.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gin_pyg.GINEConvPyg.layer_inputs_edges" class="md-nav__link">
    layer_inputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gin_pyg.GINEConvPyg.layer_outputs_edges" class="md-nav__link">
    layer_outputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gin_pyg.GINEConvPyg.layer_supports_edges" class="md-nav__link">
    layer_supports_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gin_pyg.GINEConvPyg.out_dim_factor" class="md-nav__link">
    out_dim_factor()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gps_pyg" class="md-nav__link">
    gps_pyg
  </a>
  
    <nav class="md-nav" aria-label="gps_pyg">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gps_pyg.GPSLayerPyg" class="md-nav__link">
    GPSLayerPyg
  </a>
  
    <nav class="md-nav" aria-label="GPSLayerPyg">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gps_pyg.GPSLayerPyg.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gps_pyg.GPSLayerPyg.layer_inputs_edges" class="md-nav__link">
    layer_inputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gps_pyg.GPSLayerPyg.layer_outputs_edges" class="md-nav__link">
    layer_outputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gps_pyg.GPSLayerPyg.layer_supports_edges" class="md-nav__link">
    layer_supports_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.gps_pyg.GPSLayerPyg.out_dim_factor" class="md-nav__link">
    out_dim_factor()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pna_pyg" class="md-nav__link">
    pna_pyg
  </a>
  
    <nav class="md-nav" aria-label="pna_pyg">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg" class="md-nav__link">
    PNAMessagePassingPyg
  </a>
  
    <nav class="md-nav" aria-label="PNAMessagePassingPyg">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.layer_inputs_edges" class="md-nav__link">
    layer_inputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.layer_outputs_edges" class="md-nav__link">
    layer_outputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.layer_supports_edges" class="md-nav__link">
    layer_supports_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.out_dim_factor" class="md-nav__link">
    out_dim_factor()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pooling_pyg" class="md-nav__link">
    pooling_pyg
  </a>
  
    <nav class="md-nav" aria-label="pooling_pyg">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pooling_pyg.VirtualNodePyg" class="md-nav__link">
    VirtualNodePyg
  </a>
  
    <nav class="md-nav" aria-label="VirtualNodePyg">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pooling_pyg.VirtualNodePyg.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pooling_pyg.VirtualNodePyg.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pooling_pyg.parse_pooling_layer_pyg" class="md-nav__link">
    parse_pooling_layer_pyg()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pooling_pyg.scatter_logsum_pool" class="md-nav__link">
    scatter_logsum_pool()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.pyg_layers.pooling_pyg.scatter_std_pool" class="md-nav__link">
    scatter_std_pool()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.base_graph_layer" class="md-nav__link">
    base_graph_layer
  </a>
  
    <nav class="md-nav" aria-label="base_graph_layer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_graph_layer.BaseGraphModule" class="md-nav__link">
    BaseGraphModule
  </a>
  
    <nav class="md-nav" aria-label="BaseGraphModule">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_graph_layer.BaseGraphModule.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_graph_layer.BaseGraphStructure" class="md-nav__link">
    BaseGraphStructure
  </a>
  
    <nav class="md-nav" aria-label="BaseGraphStructure">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_graph_layer.BaseGraphStructure.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_graph_layer.BaseGraphStructure.__repr__" class="md-nav__link">
    __repr__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_graph_layer.BaseGraphStructure.apply_norm_activation_dropout" class="md-nav__link">
    apply_norm_activation_dropout()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_graph_layer.BaseGraphStructure.layer_inputs_edges" class="md-nav__link">
    layer_inputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_graph_layer.BaseGraphStructure.layer_outputs_edges" class="md-nav__link">
    layer_outputs_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_graph_layer.BaseGraphStructure.layer_supports_edges" class="md-nav__link">
    layer_supports_edges()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_graph_layer.BaseGraphStructure.out_dim_factor" class="md-nav__link">
    out_dim_factor()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_graph_layer.check_intpus_allow_int" class="md-nav__link">
    check_intpus_allow_int()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.base_layers" class="md-nav__link">
    base_layers
  </a>
  
    <nav class="md-nav" aria-label="base_layers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.FCLayer" class="md-nav__link">
    FCLayer
  </a>
  
    <nav class="md-nav" aria-label="FCLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.FCLayer.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.FCLayer.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.FCLayer.in_channels" class="md-nav__link">
    in_channels()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.FCLayer.out_channels" class="md-nav__link">
    out_channels()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.GRU" class="md-nav__link">
    GRU
  </a>
  
    <nav class="md-nav" aria-label="GRU">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.GRU.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.GRU.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.MLP" class="md-nav__link">
    MLP
  </a>
  
    <nav class="md-nav" aria-label="MLP">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.MLP.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.MLP.__repr__" class="md-nav__link">
    __repr__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.MLP.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.get_activation" class="md-nav__link">
    get_activation()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.base_layers.get_norm" class="md-nav__link">
    get_norm()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#goli.nn.residual_connections" class="md-nav__link">
    residual_connections
  </a>
  
    <nav class="md-nav" aria-label="residual_connections">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionBase" class="md-nav__link">
    ResidualConnectionBase
  </a>
  
    <nav class="md-nav" aria-label="ResidualConnectionBase">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionBase.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionBase.__repr__" class="md-nav__link">
    __repr__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionBase.h_dim_increase_type" class="md-nav__link">
    h_dim_increase_type()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionBase.has_weights" class="md-nav__link">
    has_weights()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionConcat" class="md-nav__link">
    ResidualConnectionConcat
  </a>
  
    <nav class="md-nav" aria-label="ResidualConnectionConcat">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionConcat.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionConcat.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionConcat.h_dim_increase_type" class="md-nav__link">
    h_dim_increase_type()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionConcat.has_weights" class="md-nav__link">
    has_weights()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionDenseNet" class="md-nav__link">
    ResidualConnectionDenseNet
  </a>
  
    <nav class="md-nav" aria-label="ResidualConnectionDenseNet">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionDenseNet.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionDenseNet.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionDenseNet.h_dim_increase_type" class="md-nav__link">
    h_dim_increase_type()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionDenseNet.has_weights" class="md-nav__link">
    has_weights()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionNone" class="md-nav__link">
    ResidualConnectionNone
  </a>
  
    <nav class="md-nav" aria-label="ResidualConnectionNone">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionNone.__repr__" class="md-nav__link">
    __repr__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionNone.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionNone.h_dim_increase_type" class="md-nav__link">
    h_dim_increase_type()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionNone.has_weights" class="md-nav__link">
    has_weights()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionRandom" class="md-nav__link">
    ResidualConnectionRandom
  </a>
  
    <nav class="md-nav" aria-label="ResidualConnectionRandom">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionRandom.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionRandom.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionRandom.h_dim_increase_type" class="md-nav__link">
    h_dim_increase_type()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionRandom.has_weights" class="md-nav__link">
    has_weights()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionSimple" class="md-nav__link">
    ResidualConnectionSimple
  </a>
  
    <nav class="md-nav" aria-label="ResidualConnectionSimple">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionSimple.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionSimple.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionSimple.h_dim_increase_type" class="md-nav__link">
    h_dim_increase_type()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionSimple.has_weights" class="md-nav__link">
    has_weights()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionWeighted" class="md-nav__link">
    ResidualConnectionWeighted
  </a>
  
    <nav class="md-nav" aria-label="ResidualConnectionWeighted">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionWeighted.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionWeighted.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionWeighted.h_dim_increase_type" class="md-nav__link">
    h_dim_increase_type()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goli.nn.residual_connections.ResidualConnectionWeighted.has_weights" class="md-nav__link">
    has_weights()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  <a href="https://github.com/valence-discovery/goli/edit/master/docs/api/goli.nn.md" title="Edit this page" class="md-content__button md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
  </a>


  <h1>goli.nn</h1>

<div class="doc doc-object doc-module">



<h3 id="goli.nn.architectures.dgl_architectures" class="doc doc-heading">
          <code>goli.nn.architectures.dgl_architectures</code>


<a href="#goli.nn.architectures.dgl_architectures" class="headerlink" title="Permanent link">&para;</a></h3>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h4 id="goli.nn.architectures.dgl_architectures.FeedForwardDGL" class="doc doc-heading">
        <code>FeedForwardDGL</code>


<a href="#goli.nn.architectures.dgl_architectures.FeedForwardDGL" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="goli.nn.architectures.global_architectures.FeedForwardGraphBase" href="#goli.nn.architectures.global_architectures.FeedForwardGraphBase">FeedForwardGraphBase</a></code></p>

  
      <p>A flexible neural network architecture, with variable hidden dimensions,
support for multiple layer types, and support for different residual
connections.</p>
<p>This class is meant to work with different DGL-based graph neural networks
layers. Any layer must inherit from <code>goli.nn.base_graph_layer.BaseGraphStructure</code>
or <code>goli.nn.base_graph_layer.BaseGraphLayer</code>.</p>



  

  <div class="doc doc-children">











  </div>

  </div>

</div>




  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="goli.nn.architectures.global_architectures" class="doc doc-heading">
          <code>goli.nn.architectures.global_architectures</code>


<a href="#goli.nn.architectures.global_architectures" class="headerlink" title="Permanent link">&para;</a></h3>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h4 id="goli.nn.architectures.global_architectures.FeedForwardGraphBase" class="doc doc-heading">
        <code>FeedForwardGraphBase</code>


<a href="#goli.nn.architectures.global_architectures.FeedForwardGraphBase" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="goli.nn.architectures.global_architectures.FeedForwardNN" href="#goli.nn.architectures.global_architectures.FeedForwardNN">FeedForwardNN</a></code></p>




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.architectures.global_architectures.FeedForwardGraphBase.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">hidden_dims</span><span class="p">,</span> <span class="n">layer_type</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">last_activation</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">last_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">first_normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">last_normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">residual_type</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">residual_skip_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">in_dim_edges</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">hidden_dims_edges</span><span class="o">=</span><span class="p">[],</span> <span class="n">pooling</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;sum&#39;</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;GNN&#39;</span><span class="p">,</span> <span class="n">layer_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">virtual_node</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span></code>

<a href="#goli.nn.architectures.global_architectures.FeedForwardGraphBase.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p><strong>Astract class, must be inherited to override the following methods:</strong>
- <code>_graph_layer_forward</code>
- <code>_parse_virtual_node_class</code>
- <code>_parse_pooling_layer</code>
- <code>_get_node_feats
-</code>_get_edge_feats<code>-</code>_set_node_feats<code>-</code>_set_edge_feats`</p>
<p>A flexible neural network architecture, with variable hidden dimensions,
support for multiple layer types, and support for different residual
connections.</p>
<p>This class is meant to work with different graph neural networks
layers. Any layer must inherit from <code>goli.nn.base_graph_layer.BaseGraphStructure</code>
or <code>goli.nn.base_graph_layer.BaseGraphLayer</code>.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>in_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Input feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>out_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Output feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>hidden_dims</code></td>
          <td>
                <code><span title="typing.List">List</span>[int]</code>
          </td>
          <td><p>List of dimensions in the hidden layers.
Be careful, the "simple" residual type only supports
hidden dimensions of the same value.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>depth</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[int]</code>
          </td>
          <td><p>If <code>hidden_dims</code> is an integer, <code>depth</code> is 1 + the number of
hidden layers to use. If <code>hidden_dims</code> is a <code>list</code>, <code>depth</code> must
be <code>None</code>.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>activation function to use in the hidden layers.</p></td>
          <td>
                <code>&#39;relu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>last_activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>activation function to use in the last layer.</p></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>dropout</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The ratio of units to dropout. Must be between 0 and 1</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>last_dropout</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The ratio of units to dropout for the last layer. Must be between 0 and 1</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>normalization</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Normalization to use. Choices:</p>
<ul>
<li>"none" or <code>None</code>: No normalization</li>
<li>"batch_norm": Batch normalization</li>
<li>"layer_norm": Layer normalization</li>
<li><code>Callable</code>: Any callable function</li>
</ul></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>first_normalization</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Whether to use batch normalization <strong>before</strong> the first layer</p></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>last_normalization</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Whether to use batch normalization in the last layer</p></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>residual_type</code></td>
          <td>
                <code>str</code>
          </td>
          <td><ul>
<li>"none": No residual connection</li>
<li>"simple": Residual connection similar to the ResNet architecture.
  See class <code>ResidualConnectionSimple</code></li>
<li>"weighted": Residual connection similar to the Resnet architecture,
  but with weights applied before the summation. See class <code>ResidualConnectionWeighted</code></li>
<li>"concat": Residual connection where the residual is concatenated instead
  of being added.</li>
<li>"densenet": Residual connection where the residual of all previous layers
  are concatenated. This leads to a strong increase in the number of parameters
  if there are multiple hidden layers.</li>
</ul></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>residual_skip_steps</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The number of steps to skip between each residual connection.
If <code>1</code>, all the layers are connected. If <code>2</code>, half of the
layers are connected.</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>in_dim_edges</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Input edge-feature dimensions of the network. Keep at 0 if not using
edge features, or if the layer doesn't support edges.</p></td>
          <td>
                <code>0</code>
          </td>
        </tr>
        <tr>
          <td><code>hidden_dims_edges</code></td>
          <td>
                <code><span title="typing.List">List</span>[int]</code>
          </td>
          <td><p>Hidden dimensions for the edges. Most models don't support it, so it
should only be used for those that do, i.e. <code>GatedGCNLayer</code></p></td>
          <td>
                <code>[]</code>
          </td>
        </tr>
        <tr>
          <td><code>pooling</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="typing.List">List</span>[str], <span title="typing.List">List</span>[<span title="typing.Callable">Callable</span>]]</code>
          </td>
          <td><p>The pooling types to use. Multiple pooling can be used, and their
results will be concatenated.
For node feature predictions, use <code>["none"]</code>.
For graph feature predictions see <code>goli.nn.dgl_layers.pooling.parse_pooling_layer</code>.
The list must either contain Callables, or the string below</p>
<ul>
<li>"none": No pooling is applied</li>
<li>"sum": <code>SumPooling</code></li>
<li>"mean": <code>MeanPooling</code></li>
<li>"max": <code>MaxPooling</code></li>
<li>"min": <code>MinPooling</code></li>
<li>"std": <code>StdPooling</code></li>
<li>"s2s": <code>Set2Set</code></li>
</ul></td>
          <td>
                <code>[&#39;sum&#39;]</code>
          </td>
        </tr>
        <tr>
          <td><code>name</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>Name attributed to the current network, for display and printing
purposes.</p></td>
          <td>
                <code>&#39;GNN&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>layer_type</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span>]</code>
          </td>
          <td><p>The type of layers to use in the network.
A class that inherits from <code>goli.nn.dgl_layers.BaseDGLLayer</code>,
or one of the following strings</p>
<ul>
<li>"dgl:gcn": GCNDgl</li>
<li>"dgl:gin": GINDgl</li>
<li>"dgl:gat": GATDgl</li>
<li>"dgl:gated-gcn": GatedGCNDgl</li>
<li>"dgl:pna-conv": PNAConvolutionalDgl</li>
<li>"dgl:pna-msgpass": PNAMessagePassingDgl</li>
<li>"dgl:dgn-conv": DGNConvolutionalDgl</li>
<li>"dgl:dgn-msgpass": DGNMessagePassingDgl</li>
<li>"pyg:gin": GINConvPyg</li>
<li>"pyg:gine": GINEConvPyg</li>
<li>"pyg:gated-gcn": GatedGCNPyg</li>
<li>"pyg:pna-msgpass": PNAMessagePassingPyg</li>
</ul></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>layer_kwargs</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>]</code>
          </td>
          <td><p>The arguments to be used in the initialization of the layer provided by <code>layer_type</code></p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>virtual_node</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>A string associated to the type of virtual node to use,
either <code>None</code>, "none", "mean", "sum", "max", "logsum".
See <code>goli.nn.dgl_layers.VirtualNode</code>.</p>
<p>The virtual node will not use any residual connection if <code>residual_type</code>
is "none". Otherwise, it will use a simple ResNet like residual
connection.</p></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.architectures.global_architectures.FeedForwardGraphBase.__repr__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__repr__</span><span class="p">()</span></code>

<a href="#goli.nn.architectures.global_architectures.FeedForwardGraphBase.__repr__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Controls how the class is printed</p>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.architectures.global_architectures.FeedForwardGraphBase.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">g</span><span class="p">)</span></code>

<a href="#goli.nn.architectures.global_architectures.FeedForwardGraphBase.forward" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Apply the full graph neural network on the input graph and node features.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>g</code></td>
          <td>
          </td>
          <td><p>batched graphs on which the convolution is done with the keys:</p>
<ul>
<li>
<p><code>"h"</code>: torch.Tensor[..., N, Din]
  Node feature tensor, before convolution.
  <code>N</code> is the number of nodes, <code>Din</code> is the input features</p>
</li>
<li>
<p><code>"edge_attr"</code> (torch.Tensor[..., N, Ein]):
  Edge feature tensor, before convolution.
  <code>N</code> is the number of nodes, <code>Ein</code> is the input edge features</p>
</li>
</ul></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>torch.Tensor[..., M, Dout]</code> or <code>torch.Tensor[..., N, Dout]</code>:
Node or graph feature tensor, after the network.
<code>N</code> is the number of nodes, <code>M</code> is the number of graphs,
<code>Dout</code> is the output dimension <code>self.out_dim</code>
If the <code>self.pooling</code> is [<code>None</code>], then it returns node features and the output dimension is <code>N</code>,
otherwise it returns graph features and the output dimension is <code>M</code></p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="goli.nn.architectures.global_architectures.FeedForwardNN" class="doc doc-heading">
        <code>FeedForwardNN</code>


<a href="#goli.nn.architectures.global_architectures.FeedForwardNN" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.architectures.global_architectures.FeedForwardNN.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">hidden_dims</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">last_activation</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">last_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">first_normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">last_normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">residual_type</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">residual_skip_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;LNN&#39;</span><span class="p">,</span> <span class="n">layer_type</span><span class="o">=</span><span class="s1">&#39;fc&#39;</span><span class="p">,</span> <span class="n">layer_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#goli.nn.architectures.global_architectures.FeedForwardNN.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>A flexible neural network architecture, with variable hidden dimensions,
support for multiple layer types, and support for different residual
connections.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>in_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Input feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>out_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Output feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>hidden_dims</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="typing.List">List</span>[int], int]</code>
          </td>
          <td><p>Either an integer specifying all the hidden dimensions,
or a list of dimensions in the hidden layers.
Be careful, the "simple" residual type only supports
hidden dimensions of the same value.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>depth</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[int]</code>
          </td>
          <td><p>If <code>hidden_dims</code> is an integer, <code>depth</code> is 1 + the number of
hidden layers to use. If <code>hidden_dims</code> is a <code>list</code>, <code>depth</code> must
be <code>None</code>.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>activation function to use in the hidden layers.</p></td>
          <td>
                <code>&#39;relu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>last_activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>activation function to use in the last layer.</p></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>dropout</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The ratio of units to dropout. Must be between 0 and 1</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>last_dropout</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The ratio of units to dropout for the last_layer. Must be between 0 and 1</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>normalization</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Normalization to use. Choices:</p>
<ul>
<li>"none" or <code>None</code>: No normalization</li>
<li>"batch_norm": Batch normalization</li>
<li>"layer_norm": Layer normalization</li>
<li><code>Callable</code>: Any callable function</li>
</ul></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>first_normalization</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Whether to use batch normalization <strong>before</strong> the first layer</p></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>last_normalization</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Whether to use batch normalization in the last layer</p></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>residual_type</code></td>
          <td>
                <code>str</code>
          </td>
          <td><ul>
<li>"none": No residual connection</li>
<li>"simple": Residual connection similar to the ResNet architecture.
  See class <code>ResidualConnectionSimple</code></li>
<li>"weighted": Residual connection similar to the Resnet architecture,
  but with weights applied before the summation. See class <code>ResidualConnectionWeighted</code></li>
<li>"concat": Residual connection where the residual is concatenated instead
  of being added.</li>
<li>"densenet": Residual connection where the residual of all previous layers
  are concatenated. This leads to a strong increase in the number of parameters
  if there are multiple hidden layers.</li>
</ul></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>residual_skip_steps</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The number of steps to skip between each residual connection.
If <code>1</code>, all the layers are connected. If <code>2</code>, half of the
layers are connected.</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>name</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>Name attributed to the current network, for display and printing
purposes.</p></td>
          <td>
                <code>&#39;LNN&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>layer_type</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span>]</code>
          </td>
          <td><p>The type of layers to use in the network.
Either "fc" as the <code>FCLayer</code>, or a class representing the <code>nn.Module</code>
to use.</p></td>
          <td>
                <code>&#39;fc&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>layer_kwargs</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>]</code>
          </td>
          <td><p>The arguments to be used in the initialization of the layer provided by <code>layer_type</code></p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.architectures.global_architectures.FeedForwardNN.__repr__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__repr__</span><span class="p">()</span></code>

<a href="#goli.nn.architectures.global_architectures.FeedForwardNN.__repr__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Controls how the class is printed</p>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.architectures.global_architectures.FeedForwardNN.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">h</span><span class="p">)</span></code>

<a href="#goli.nn.architectures.global_architectures.FeedForwardNN.forward" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Apply the neural network on the input features.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>h</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>torch.Tensor[..., Din]</code>:
Input feature tensor, before the network.
<code>Din</code> is the number of input features</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>torch.Tensor[..., Dout]</code>:
Output feature tensor, after the network.
<code>Dout</code> is the number of output features</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="goli.nn.architectures.global_architectures.FullGraphMultiTaskNetwork" class="doc doc-heading">
        <code>FullGraphMultiTaskNetwork</code>


<a href="#goli.nn.architectures.global_architectures.FullGraphMultiTaskNetwork" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="goli.nn.architectures.global_architectures.FullGraphNetwork" href="#goli.nn.architectures.global_architectures.FullGraphNetwork">FullGraphNetwork</a></code></p>

  
      <p>Class that allows to implement a full multi-task graph neural network architecture,
including the pre-processing MLP, post-processing MLP and the task-specific heads.</p>
<p>In this model, the tasks share a full DGL network as a "trunk", and then they have task-specific MLPs.</p>
<p>Each molecular graph is associated with a variety of tasks, so the network should output the task-specific preedictions for a graph.</p>



  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.architectures.global_architectures.FullGraphMultiTaskNetwork.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">task_heads_kwargs_list</span><span class="p">,</span> <span class="n">gnn_kwargs</span><span class="p">,</span> <span class="n">pre_nn_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pe_encoders_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pre_nn_edges_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">post_nn_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_inference_to_average</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Multitask_GNN&#39;</span><span class="p">)</span></code>

<a href="#goli.nn.architectures.global_architectures.FullGraphMultiTaskNetwork.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Class that allows to implement a full multi-task graph neural network architecture,
including the pre-processing MLP, post-processing MLP and the task-specific heads.</p>
<p>In this model, the tasks share a full DGL network as a "trunk", and additionally have task-specific MLPs.
Each molecular graph is associated with a variety of tasks, so the network outputs the task-specific preedictions for a graph.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>task_heads_kwargs_list</code></td>
          <td>
                <code><span title="typing.List">List</span>[<span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]]</code>
          </td>
          <td><p>This argument is a list of dictionaries containing the arguments for task heads. Each argument is used to
initialize a task-specific MLP.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>gnn_kwargs</code></td>
          <td>
                <code><span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]</code>
          </td>
          <td><p>key-word arguments to use for the initialization of the pre-processing
GNN network using the class <code>FeedForwardDGL</code>.
It must respect the following criteria:</p>
<ul>
<li>gnn_kwargs["in_dim"] must be equal to pre_nn_kwargs["out_dim"]</li>
<li>gnn_kwargs["out_dim"] must be equal to post_nn_kwargs["in_dim"]</li>
</ul></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>pre_nn_kwargs</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]]</code>
          </td>
          <td><p>key-word arguments to use for the initialization of the pre-processing
MLP network of the node features before the GNN, using the class <code>FeedForwardNN</code>.
If <code>None</code>, there won't be a pre-processing MLP.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>pre_nn_edges_kwargs</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]]</code>
          </td>
          <td><p>key-word arguments to use for the initialization of the pre-processing
MLP network of the edge features before the GNN, using the class <code>FeedForwardNN</code>.
If <code>None</code>, there won't be a pre-processing MLP.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>post_nn_kwargs</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]]</code>
          </td>
          <td><p>key-word arguments to use for the initialization of the post-processing
MLP network after the GNN, using the class <code>FeedForwardNN</code>.
If <code>None</code>, there won't be a post-processing MLP.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>num_inference_to_average</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of inferences to average at val/test time. This is used to avoid the noise introduced
by positional encodings with sign-flips. In case no such encoding is given,
this parameter is ignored.
NOTE: The inference time will be slowed-down proportionaly to this parameter.</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>name</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>Name attributed to the current network, for display and printing
purposes.</p></td>
          <td>
                <code>&#39;Multitask_GNN&#39;</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.architectures.global_architectures.FullGraphMultiTaskNetwork.out_dim" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_dim</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.architectures.global_architectures.FullGraphMultiTaskNetwork.out_dim" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Returns the output dimension of the network for each task</p>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="goli.nn.architectures.global_architectures.FullGraphNetwork" class="doc doc-heading">
        <code>FullGraphNetwork</code>


<a href="#goli.nn.architectures.global_architectures.FullGraphNetwork" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.architectures.global_architectures.FullGraphNetwork.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">gnn_kwargs</span><span class="p">,</span> <span class="n">pre_nn_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pre_nn_edges_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pe_encoders_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">post_nn_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_inference_to_average</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;DGL_GNN&#39;</span><span class="p">)</span></code>

<a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Class that allows to implement a full graph neural network architecture,
including the pre-processing MLP and the post processing MLP.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>gnn_kwargs</code></td>
          <td>
                <code><span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]</code>
          </td>
          <td><p>key-word arguments to use for the initialization of the pre-processing
GNN network using the class <code>FeedForwardDGL</code>.
It must respect the following criteria:</p>
<ul>
<li>gnn_kwargs["in_dim"] must be equal to pre_nn_kwargs["out_dim"]</li>
<li>gnn_kwargs["out_dim"] must be equal to post_nn_kwargs["in_dim"]</li>
</ul></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>pe_encoders_kwargs</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]]</code>
          </td>
          <td><p>key-word arguments to use for the initialization of all positional encoding encoders
can use the class PE_ENCODERS_DICT: "la_encoder"(tested) , "mlp_encoder" (not tested), "signnet_encoder" (not tested)</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>pre_nn_kwargs</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]]</code>
          </td>
          <td><p>key-word arguments to use for the initialization of the pre-processing
MLP network of the node features before the GNN, using the class <code>FeedForwardNN</code>.
If <code>None</code>, there won't be a pre-processing MLP.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>pre_nn_edges_kwargs</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]]</code>
          </td>
          <td><p>key-word arguments to use for the initialization of the pre-processing
MLP network of the edge features before the GNN, using the class <code>FeedForwardNN</code>.
If <code>None</code>, there won't be a pre-processing MLP.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>post_nn_kwargs</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]]</code>
          </td>
          <td><p>key-word arguments to use for the initialization of the post-processing
MLP network after the GNN, using the class <code>FeedForwardNN</code>.
If <code>None</code>, there won't be a post-processing MLP.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>num_inference_to_average</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of inferences to average at val/test time. This is used to avoid the noise introduced
by positional encodings with sign-flips. In case no such encoding is given,
this parameter is ignored.
NOTE: The inference time will be slowed-down proportionaly to this parameter.</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>name</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>Name attributed to the current network, for display and printing
purposes.</p></td>
          <td>
                <code>&#39;DGL_GNN&#39;</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.architectures.global_architectures.FullGraphNetwork.__repr__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__repr__</span><span class="p">()</span></code>

<a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.__repr__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Controls how the class is printed</p>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.architectures.global_architectures.FullGraphNetwork.concat_last_layers" class="doc doc-heading">
<code class="highlight language-python"><span class="n">concat_last_layers</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-writable"><code>writable</code></small>
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.concat_last_layers" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Property to control the output of the <code>self.forward</code>.
If set to a list of integer, the <code>forward</code> function will
concatenate the output of different layers.</p>
<p>If set to <code>None</code>, the output of the last layer is returned.</p>
<p>NOTE: The indexes are inverted. 0 is the last layer, 1 is the second last, etc.</p>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.architectures.global_architectures.FullGraphNetwork.drop_post_nn_layers" class="doc doc-heading">
<code class="highlight language-python"><span class="n">drop_post_nn_layers</span><span class="p">(</span><span class="n">num_layers_to_drop</span><span class="p">)</span></code>

<a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.drop_post_nn_layers" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Remove the last layers of the model. Useful for Transfer Learning.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>num_layers_to_drop</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The number of layers to drop from the <code>self.post_nn</code> network.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.architectures.global_architectures.FullGraphNetwork.dtype" class="doc doc-heading">
<code class="highlight language-python"><span class="n">dtype</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.dtype" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Get the dtype of the current network, based on the weights of linear layers within the GNN</p>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.architectures.global_architectures.FullGraphNetwork.extend_post_nn_layers" class="doc doc-heading">
<code class="highlight language-python"><span class="n">extend_post_nn_layers</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span></code>

<a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.extend_post_nn_layers" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Add layers at the end of the model. Useful for Transfer Learning.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>layers</code></td>
          <td>
                <code><span title="torch.nn">nn</span>.<span title="torch.nn.ModuleList">ModuleList</span></code>
          </td>
          <td><p>A ModuleList of all the layers to extend</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.architectures.global_architectures.FullGraphNetwork.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">g</span><span class="p">)</span></code>

<a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.forward" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Apply the pre-processing neural network, the graph neural network,
and the post-processing neural network on the graph features.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>g</code></td>
          <td>
                <code><span title="typing.Any">Any</span></code>
          </td>
          <td><p>graph on which the convolution is done.
Must contain the following elements:</p>
<ul>
<li>
<p>Node key <code>"feat"</code>: <code>torch.Tensor[..., N, Din]</code>.
  Input node feature tensor, before the network.
  <code>N</code> is the number of nodes, <code>Din</code> is the input features dimension <code>self.pre_nn.in_dim</code></p>
</li>
<li>
<p>Edge key <code>"edge_feat"</code>: <code>torch.Tensor[..., N, Ein]</code> <strong>Optional</strong>.
  The edge features to use. It will be ignored if the
  model doesn't supporte edge features or if
  <code>self.in_dim_edges==0</code>.</p>
</li>
<li>
<p>Other keys related to positional encodings <code>"pos_enc_feats_sign_flip"</code>,
  <code>"pos_enc_feats_no_flip"</code>.</p>
</li>
</ul></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>torch.Tensor[..., M, Dout]</code> or <code>torch.Tensor[..., N, Dout]</code>:
Node or graph feature tensor, after the network.
<code>N</code> is the number of nodes, <code>M</code> is the number of graphs,
<code>Dout</code> is the output dimension <code>self.post_nn.out_dim</code>
If the <code>self.gnn.pooling</code> is [<code>None</code>], then it returns node features and the output dimension is <code>N</code>,
otherwise it returns graph features and the output dimension is <code>M</code></p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.architectures.global_architectures.FullGraphNetwork.forward_node_positional_encoding" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward_node_positional_encoding</span><span class="p">(</span><span class="n">g</span><span class="p">)</span></code>

<a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.forward_node_positional_encoding" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Forward pass for the positional encodings (PE) on the nodes,
with each PE having it's own encoder defined in <code>self.pe_encoders</code>.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>g</code></td>
          <td>
                <code><span title="typing.Any">Any</span></code>
          </td>
          <td><p>graph containing the node positional encodings</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>pe_node_pooled</code></td>          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>
          </td>
          <td><p>The positional / structural encodings go through</p></td>
        </tr>
        <tr>
<td></td>          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>
          </td>
          <td><p>encoders, then are pooled together</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.architectures.global_architectures.FullGraphNetwork.forward_simple_pooling" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward_simple_pooling</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">pooling</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span></code>

<a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.forward_simple_pooling" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Apply sum, mean, or max pooling on a Tensor.</p>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.architectures.global_architectures.FullGraphNetwork.in_dim" class="doc doc-heading">
<code class="highlight language-python"><span class="n">in_dim</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.in_dim" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Returns the input dimension of the network</p>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.architectures.global_architectures.FullGraphNetwork.in_dim_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">in_dim_edges</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.in_dim_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Returns the input edge dimension of the network</p>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.architectures.global_architectures.FullGraphNetwork.out_dim" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_dim</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.architectures.global_architectures.FullGraphNetwork.out_dim" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Returns the output dimension of the network</p>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="goli.nn.architectures.global_architectures.TaskHead" class="doc doc-heading">
        <code>TaskHead</code>


<a href="#goli.nn.architectures.global_architectures.TaskHead" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="goli.nn.architectures.global_architectures.FeedForwardNN" href="#goli.nn.architectures.global_architectures.FeedForwardNN">FeedForwardNN</a></code></p>




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.architectures.global_architectures.TaskHead.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">task_name</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">hidden_dims</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">last_activation</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">last_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">last_normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">residual_type</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">residual_skip_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;LNN&#39;</span><span class="p">,</span> <span class="n">layer_type</span><span class="o">=</span><span class="s1">&#39;fc&#39;</span><span class="p">,</span> <span class="n">layer_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#goli.nn.architectures.global_architectures.TaskHead.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>This class instantiates a task head, and it is identical to the FeedForwardNN class with the addition of a <code>task_name</code> attribute.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>task_name</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>The name of the task for which the current output head performs predictions.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>in_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Input feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>out_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Output feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>hidden_dims</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="typing.List">List</span>[int], int]</code>
          </td>
          <td><p>Either an integer specifying all the hidden dimensions,
or a list of dimensions in the hidden layers.
Be careful, the "simple" residual type only supports
hidden dimensions of the same value.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>depth</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[int]</code>
          </td>
          <td><p>If <code>hidden_dims</code> is an integer, <code>depth</code> is 1 + the number of
hidden layers to use. If <code>hidden_dims</code> is a <code>list</code>, <code>depth</code> must
be <code>None</code>.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>activation function to use in the hidden layers.</p></td>
          <td>
                <code>&#39;relu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>last_activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>activation function to use in the last layer.</p></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>dropout</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The ratio of units to dropout. Must be between 0 and 1</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>last_dropout</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The ratio of units to dropout for the last_layer. Must be between 0 and 1</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>normalization</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Normalization to use. Choices:
- "none" or <code>None</code>: No normalization
- "batch_norm": Batch normalization
- "layer_norm": Layer normalization
- <code>Callable</code>: Any callable function</p></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>last_normalization</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Whether to use batch normalization in the last layer</p></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>residual_type</code></td>
          <td>
                <code>str</code>
          </td>
          <td><ul>
<li>"none": No residual connection</li>
<li>"simple": Residual connection similar to the ResNet architecture.
  See class <code>ResidualConnectionSimple</code></li>
<li>"weighted": Residual connection similar to the Resnet architecture,
  but with weights applied before the summation. See class <code>ResidualConnectionWeighted</code></li>
<li>"concat": Residual connection where the residual is concatenated instead
  of being added.</li>
<li>"densenet": Residual connection where the residual of all previous layers
  are concatenated. This leads to a strong increase in the number of parameters
  if there are multiple hidden layers.</li>
</ul></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>residual_skip_steps</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The number of steps to skip between each residual connection.
If <code>1</code>, all the layers are connected. If <code>2</code>, half of the
layers are connected.</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>name</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>Name attributed to the current network, for display and printing
purposes.</p></td>
          <td>
                <code>&#39;LNN&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>layer_type</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span>]</code>
          </td>
          <td><p>The type of layers to use in the network.
Either "fc" as the <code>FCLayer</code>, or a class representing the <code>nn.Module</code>
to use.</p></td>
          <td>
                <code>&#39;fc&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>layer_kwargs</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>]</code>
          </td>
          <td><p>The arguments to be used in the initialization of the layer provided by <code>layer_type</code></p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="goli.nn.architectures.global_architectures.TaskHeads" class="doc doc-heading">
        <code>TaskHeads</code>


<a href="#goli.nn.architectures.global_architectures.TaskHeads" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.architectures.global_architectures.TaskHeads.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">task_heads_kwargs_list</span><span class="p">)</span></code>

<a href="#goli.nn.architectures.global_architectures.TaskHeads.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Class that groups all multi-task output heads together to provide the task-specific outputs.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>in_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Input feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>task_heads_kwargs_list</code></td>
          <td>
                <code><span title="typing.List">List</span>[<span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]]</code>
          </td>
          <td><p>This argument is a list of dictionaries corresponding to the arguments for a TaskHead. Each dict of arguments is used to
initialize a task-specific MLP.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.architectures.global_architectures.TaskHeads.out_dim" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_dim</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.architectures.global_architectures.TaskHeads.out_dim" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Returns the output dimension of each task head</p>

  </div>

</div>



  </div>

  </div>

</div>




  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="goli.nn.architectures.pyg_architectures" class="doc doc-heading">
          <code>goli.nn.architectures.pyg_architectures</code>


<a href="#goli.nn.architectures.pyg_architectures" class="headerlink" title="Permanent link">&para;</a></h3>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h4 id="goli.nn.architectures.pyg_architectures.FeedForwardPyg" class="doc doc-heading">
        <code>FeedForwardPyg</code>


<a href="#goli.nn.architectures.pyg_architectures.FeedForwardPyg" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="goli.nn.architectures.global_architectures.FeedForwardGraphBase" href="#goli.nn.architectures.global_architectures.FeedForwardGraphBase">FeedForwardGraphBase</a></code></p>

  
      <p>A flexible neural network architecture, with variable hidden dimensions,
support for multiple layer types, and support for different residual
connections.</p>
<p>This class is meant to work with different PyG-based graph neural networks
layers. Any layer must inherit from <code>goli.nn.base_graph_layer.BaseGraphStructure</code>
or <code>goli.nn.base_graph_layer.BaseGraphLayer</code>.</p>



  

  <div class="doc doc-children">











  </div>

  </div>

</div>




  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="goli.nn.dgl_layers.dgn_dgl" class="doc doc-heading">
          <code>goli.nn.dgl_layers.dgn_dgl</code>


<a href="#goli.nn.dgl_layers.dgn_dgl" class="headerlink" title="Permanent link">&para;</a></h3>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h4 id="goli.nn.dgl_layers.dgn_dgl.BaseDGNDgl" class="doc doc-heading">
        <code>BaseDGNDgl</code>


<a href="#goli.nn.dgl_layers.dgn_dgl.BaseDGNDgl" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.dgn_dgl.BaseDGNDgl.message_func" class="doc doc-heading">
<code class="highlight language-python"><span class="n">message_func</span><span class="p">(</span><span class="n">edges</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.dgn_dgl.BaseDGNDgl.message_func" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>The message function to generate messages along the edges.</p>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.dgn_dgl.BaseDGNDgl.parse_aggregators" class="doc doc-heading">
<code class="highlight language-python"><span class="n">parse_aggregators</span><span class="p">(</span><span class="n">aggregators_name</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.dgn_dgl.BaseDGNDgl.parse_aggregators" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Parse the aggregators from a list of strings into a list of callables.</p>
<p>The possibilities are:</p>
<ul>
<li><code>"mean"</code></li>
<li><code>"sum"</code></li>
<li><code>"min"</code></li>
<li><code>"max"</code></li>
<li><code>"std"</code></li>
<li><code>"dir{dir_idx:int}/smooth/{Optional[temperature:float]}"</code></li>
<li><code>"dir{dir_idx:int}/dx_abs/{Optional[temperature:float]}"</code></li>
<li><code>"dir{dir_idx:int}/dx_no_abs/{Optional[temperature:float]}"</code></li>
<li><code>"dir{dir_idx:int}/dx_abs_balanced/{Optional[temperature:float]}"</code></li>
<li><code>"dir{dir_idx:int}/forward/{Optional[temperature:float]}"</code></li>
<li><code>"dir{dir_idx:int}/backward/{Optional[temperature:float]}"</code></li>
</ul>
<p><code>dir_idx</code> is an integer specifying the index of the positional encoding
to use for direction. In the case of eigenvector-based directions, <code>dir_idx=1</code>
is chosen for the first non-trivial eigenvector and <code>dir_idx=2</code> for the second.</p>
<p><code>temperature</code> is used to harden the direction using a softmax on the directional
matrices. If it is not provided, then no softmax is applied. The larger the temperature,
the more weight is attributed to the dominant direction.</p>
<p>The graph. Must have the key <code>graph.ndata["pos_dir"]</code></p>

<details class="example">
  <summary>Example</summary>
  <div class="highlight"><pre><span></span><code>In:     self.parse_aggregators([&quot;dir1/dx_abs&quot;, &quot;dir2/smooth/0.2&quot;])
Out:    [partial(aggregate_dir_dx_abs, dir_idx=1, temperature=None),
         partial(aggregate_dir_smooth, dir_idx=2, temperature=0.2)]
</code></pre></div>
</details>
  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>aggregators_name</code></td>
          <td>
                <code><span title="typing.List">List</span>[str]</code>
          </td>
          <td><p>The list of all aggregators names to use, selected
from the list of possible strings.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>aggregators</code></td>          <td>
                <code><span title="typing.List">List</span>[<span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>The list of all callable aggregators.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.dgn_dgl.BaseDGNDgl.reduce_func" class="doc doc-heading">
<code class="highlight language-python"><span class="n">reduce_func</span><span class="p">(</span><span class="n">nodes</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.dgn_dgl.BaseDGNDgl.reduce_func" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>The reduce function to aggregate the messages.
Apply the aggregators and scalers, and concatenate the results.</p>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="goli.nn.dgl_layers.dgn_dgl.DGNConvolutionalDgl" class="doc doc-heading">
        <code>DGNConvolutionalDgl</code>


<a href="#goli.nn.dgl_layers.dgn_dgl.DGNConvolutionalDgl" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="goli.nn.dgl_layers.dgn_dgl.BaseDGNDgl" href="#goli.nn.dgl_layers.dgn_dgl.BaseDGNDgl">BaseDGNDgl</a></code>, <code><a class="autorefs autorefs-internal" title="goli.nn.dgl_layers.pna_dgl.PNAConvolutionalDgl" href="#goli.nn.dgl_layers.pna_dgl.PNAConvolutionalDgl">PNAConvolutionalDgl</a></code></p>

  
      <p>Implementation of the convolutional architecture of the DGN layer,
previously known as <code>DGNSimpleLayer</code>. This layer aggregates the
neighbouring messages using multiple aggregators and scalers,
concatenates their results, then applies an MLP on the concatenated
features.</p>
<p>The graph. Must have the key <code>graph.ndata["pos_dir"]</code></p>
<p>DGN: Directional Graph Networks
Dominique Beaini, Saro Passaro, Vincent Létourneau, William L. Hamilton, Gabriele Corso, Pietro Liò
<a href="https://arxiv.org/pdf/2010.02863.pdf">https://arxiv.org/pdf/2010.02863.pdf</a></p>



  

  <div class="doc doc-children">











  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="goli.nn.dgl_layers.dgn_dgl.DGNMessagePassingDgl" class="doc doc-heading">
        <code>DGNMessagePassingDgl</code>


<a href="#goli.nn.dgl_layers.dgn_dgl.DGNMessagePassingDgl" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="goli.nn.dgl_layers.dgn_dgl.BaseDGNDgl" href="#goli.nn.dgl_layers.dgn_dgl.BaseDGNDgl">BaseDGNDgl</a></code>, <code><a class="autorefs autorefs-internal" title="goli.nn.dgl_layers.pna_dgl.PNAMessagePassingDgl" href="#goli.nn.dgl_layers.pna_dgl.PNAMessagePassingDgl">PNAMessagePassingDgl</a></code></p>

  
      <p>Implementation of the message passing architecture of the DGN message passing layer,
previously known as <code>DGNLayerComplex</code>. This layer applies an MLP as
pretransformation to the concatenation of <span class="arithmatex">\([h_u, h_v, e_{uv}]\)</span> to generate
the messages, with <span class="arithmatex">\(h_u\)</span> the node feature, <span class="arithmatex">\(h_v\)</span> the neighbour node features,
and <span class="arithmatex">\(e_{uv}\)</span> the edge feature between the nodes <span class="arithmatex">\(u\)</span> and <span class="arithmatex">\(v\)</span>.</p>
<p>After the pre-transformation, it aggregates the messages using
multiple aggregators and scalers,
concatenates their results, then applies an MLP on the concatenated
features.</p>
<p>The graph. Must have the key <code>graph.ndata["pos_dir"]</code></p>
<p>DGN: Directional Graph Networks
Dominique Beaini, Saro Passaro, Vincent Létourneau, William L. Hamilton, Gabriele Corso, Pietro Liò
<a href="https://arxiv.org/pdf/2010.02863.pdf">https://arxiv.org/pdf/2010.02863.pdf</a></p>



  

  <div class="doc doc-children">











  </div>

  </div>

</div>




  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="goli.nn.dgl_layers.dgn_operations" class="doc doc-heading">
          <code>goli.nn.dgl_layers.dgn_operations</code>


<a href="#goli.nn.dgl_layers.dgn_operations" class="headerlink" title="Permanent link">&para;</a></h3>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h4 id="goli.nn.dgl_layers.dgn_operations.aggregate_dir_backward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">aggregate_dir_backward</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">source_pos</span><span class="p">,</span> <span class="n">dest_pos</span><span class="p">,</span> <span class="n">h_in</span><span class="p">,</span> <span class="n">dir_idx</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.dgn_operations.aggregate_dir_backward" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>The aggregation is the following:</p>
<div class="arithmatex">\[y^{(l)} = \hat{F}^-_k h^{(l)}\]</div>
<ul>
<li><span class="arithmatex">\(\hat{F}^-_k\)</span> is the normalized positive component of the directional field <em>k-th</em> directional field <span class="arithmatex">\(F_k\)</span></li>
<li><span class="arithmatex">\(y^{(l)}\)</span> is the returned aggregated result at the <em>l-th</em> layer.</li>
<li><span class="arithmatex">\(h^{(l)}\)</span> is the node features at the <em>l-th</em> layer.</li>
</ul>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>h</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The features to aggregate <span class="arithmatex">\(h^{(l)}\)</span></p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>source_pos</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The positional encoding at the source node, used to compute the directional field</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>dest_pos</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The positional encoding at the destination node, used to compute the directional field</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>h_in</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The input features of the layer, before any operation.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>dir_idx</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The index of the positional encoding (<span class="arithmatex">\(k\)</span> in the equation above)</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>temperature</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[float]</code>
          </td>
          <td><p>The temperature to use in the softmax of the directional field.
If <code>None</code>, then the softmax is not applied on the field</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>h_mod</code></td>          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The aggregated features <span class="arithmatex">\(y^{(l)}\)</span></p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="goli.nn.dgl_layers.dgn_operations.aggregate_dir_dx_abs" class="doc doc-heading">
<code class="highlight language-python"><span class="n">aggregate_dir_dx_abs</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">source_pos</span><span class="p">,</span> <span class="n">dest_pos</span><span class="p">,</span> <span class="n">h_in</span><span class="p">,</span> <span class="n">dir_idx</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.dgn_operations.aggregate_dir_dx_abs" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>The aggregation is the following:</p>
<div class="arithmatex">\[y^{(l)} = |B_{dx}^k h^{(l)}|\]</div>
<div class="arithmatex">\[B_{dx}^k = \hat{F}_k - diag \left(\sum_j{\hat{F}_{k_{(:, j)}}} \right)\]</div>
<ul>
<li><span class="arithmatex">\(\hat{F}^+_k\)</span> is the normalized positive component of the directional field <em>k-th</em> directional field <span class="arithmatex">\(F_k\)</span></li>
<li><span class="arithmatex">\(y^{(l)}\)</span> is the returned aggregated result at the <em>l-th</em> layer.</li>
<li><span class="arithmatex">\(h^{(l)}\)</span> is the node features at the <em>l-th</em> layer.</li>
</ul>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>h</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The features to aggregate <span class="arithmatex">\(h^{(l)}\)</span></p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>source_pos</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The positional encoding at the source node, used to compute the directional field</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>dest_pos</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The positional encoding at the destination node, used to compute the directional field</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>h_in</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The input features of the layer, before any operation.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>dir_idx</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The index of the positional encoding (<span class="arithmatex">\(k\)</span> in the equation above)</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>temperature</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[float]</code>
          </td>
          <td><p>The temperature to use in the softmax of the directional field.
If <code>None</code>, then the softmax is not applied on the field</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>h_mod</code></td>          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The aggregated features <span class="arithmatex">\(y^{(l)}\)</span></p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="goli.nn.dgl_layers.dgn_operations.aggregate_dir_dx_abs_balanced" class="doc doc-heading">
<code class="highlight language-python"><span class="n">aggregate_dir_dx_abs_balanced</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">source_pos</span><span class="p">,</span> <span class="n">dest_pos</span><span class="p">,</span> <span class="n">h_in</span><span class="p">,</span> <span class="n">dir_idx</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.dgn_operations.aggregate_dir_dx_abs_balanced" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>The aggregation is the same as <code>aggregate_dir_dx_no_abs</code>, but the positive and
negative parts of the field are normalized separately.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>h</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The features to aggregate <span class="arithmatex">\(h^{(l)}\)</span></p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>source_pos</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The positional encoding at the source node, used to compute the directional field</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>dest_pos</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The positional encoding at the destination node, used to compute the directional field</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>h_in</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The input features of the layer, before any operation.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>dir_idx</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The index of the positional encoding (<span class="arithmatex">\(k\)</span> in the equation above)</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>temperature</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[float]</code>
          </td>
          <td><p>The temperature to use in the softmax of the directional field.
If <code>None</code>, then the softmax is not applied on the field</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>h_mod</code></td>          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The aggregated features <span class="arithmatex">\(y^{(l)}\)</span></p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="goli.nn.dgl_layers.dgn_operations.aggregate_dir_dx_no_abs" class="doc doc-heading">
<code class="highlight language-python"><span class="n">aggregate_dir_dx_no_abs</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">source_pos</span><span class="p">,</span> <span class="n">dest_pos</span><span class="p">,</span> <span class="n">h_in</span><span class="p">,</span> <span class="n">dir_idx</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.dgn_operations.aggregate_dir_dx_no_abs" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>The aggregation is the following:</p>
<div class="arithmatex">\[y^{(l)} = B_{dx}^k h^{(l)}\]</div>
<div class="arithmatex">\[B_{dx}^k = \hat{F}_k - diag \left(\sum_j{\hat{F}_{k_{(:, j)}}} \right)\]</div>
<ul>
<li><span class="arithmatex">\(\hat{F}^+_k\)</span> is the normalized positive component of the directional field <em>k-th</em> directional field <span class="arithmatex">\(F_k\)</span></li>
<li><span class="arithmatex">\(y^{(l)}\)</span> is the returned aggregated result at the <em>l-th</em> layer.</li>
<li><span class="arithmatex">\(h^{(l)}\)</span> is the node features at the <em>l-th</em> layer.</li>
</ul>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>h</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The features to aggregate <span class="arithmatex">\(h^{(l)}\)</span></p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>source_pos</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The positional encoding at the source node, used to compute the directional field</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>dest_pos</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The positional encoding at the destination node, used to compute the directional field</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>h_in</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The input features of the layer, before any operation.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>dir_idx</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The index of the positional encoding (<span class="arithmatex">\(k\)</span> in the equation above)</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>temperature</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[float]</code>
          </td>
          <td><p>The temperature to use in the softmax of the directional field.
If <code>None</code>, then the softmax is not applied on the field</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>h_mod</code></td>          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The aggregated features <span class="arithmatex">\(y^{(l)}\)</span></p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="goli.nn.dgl_layers.dgn_operations.aggregate_dir_forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">aggregate_dir_forward</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">source_pos</span><span class="p">,</span> <span class="n">dest_pos</span><span class="p">,</span> <span class="n">h_in</span><span class="p">,</span> <span class="n">dir_idx</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.dgn_operations.aggregate_dir_forward" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>The aggregation is the following:</p>
<div class="arithmatex">\[y^{(l)} = \hat{F}^+_k h^{(l)}\]</div>
<ul>
<li><span class="arithmatex">\(\hat{F}^+_k\)</span> is the normalized positive component of the directional field <em>k-th</em> directional field <span class="arithmatex">\(F_k\)</span></li>
<li><span class="arithmatex">\(y^{(l)}\)</span> is the returned aggregated result at the <em>l-th</em> layer.</li>
<li><span class="arithmatex">\(h^{(l)}\)</span> is the node features at the <em>l-th</em> layer.</li>
</ul>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>h</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The features to aggregate <span class="arithmatex">\(h^{(l)}\)</span></p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>source_pos</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The positional encoding at the source node, used to compute the directional field</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>dest_pos</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The positional encoding at the destination node, used to compute the directional field</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>h_in</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The input features of the layer, before any operation.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>dir_idx</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The index of the positional encoding (<span class="arithmatex">\(k\)</span> in the equation above)</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>temperature</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[float]</code>
          </td>
          <td><p>The temperature to use in the softmax of the directional field.
If <code>None</code>, then the softmax is not applied on the field</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>h_mod</code></td>          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The aggregated features <span class="arithmatex">\(y^{(l)}\)</span></p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="goli.nn.dgl_layers.dgn_operations.aggregate_dir_smooth" class="doc doc-heading">
<code class="highlight language-python"><span class="n">aggregate_dir_smooth</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">source_pos</span><span class="p">,</span> <span class="n">dest_pos</span><span class="p">,</span> <span class="n">h_in</span><span class="p">,</span> <span class="n">dir_idx</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.dgn_operations.aggregate_dir_smooth" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>The aggregation is the following:</p>
<div class="arithmatex">\[y^{(l)} = |\hat{F}_k| h^{(l)}\]</div>
<ul>
<li><span class="arithmatex">\(\hat{F}^+_k\)</span> is the normalized directional field <em>k-th</em> directional field <span class="arithmatex">\(F_k\)</span></li>
<li><span class="arithmatex">\(y^{(l)}\)</span> is the returned aggregated result at the <em>l-th</em> layer.</li>
<li><span class="arithmatex">\(h^{(l)}\)</span> is the node features at the <em>l-th</em> layer.</li>
</ul>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>h</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The features to aggregate <span class="arithmatex">\(h^{(l)}\)</span></p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>source_pos</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The positional encoding at the source node, used to compute the directional field</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>dest_pos</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The positional encoding at the destination node, used to compute the directional field</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>h_in</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The input features of the layer, before any operation.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>dir_idx</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The index of the positional encoding (<span class="arithmatex">\(k\)</span> in the equation above)</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>temperature</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[float]</code>
          </td>
          <td><p>The temperature to use in the softmax of the directional field.
If <code>None</code>, then the softmax is not applied on the field</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>h_mod</code></td>          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The aggregated features <span class="arithmatex">\(y^{(l)}\)</span></p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="goli.nn.dgl_layers.dgn_operations.get_grad_of_pos" class="doc doc-heading">
<code class="highlight language-python"><span class="n">get_grad_of_pos</span><span class="p">(</span><span class="n">source_pos</span><span class="p">,</span> <span class="n">dest_pos</span><span class="p">,</span> <span class="n">dir_idx</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.dgn_operations.get_grad_of_pos" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Get the vector field associated to the gradient of the positional
encoding.</p>
<div class="arithmatex">\[F_k = \nabla pos_k\]</div>
<p>or, if a temperature <span class="arithmatex">\(T\)</span> is provided</p>
<div class="arithmatex">\[F_k = softmax((\nabla pos_k)^+) - softmax((\nabla pos_k)^-)\]</div>
<p>Where <span class="arithmatex">\(F_k\)</span> is the <em>k-th</em> directional field associated to the <span class="arithmatex">\(k-th\)</span> positional
encoding.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>source_pos</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The positional encoding at the source node, used to compute the directional field</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>dest_pos</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The positional encoding at the destination node, used to compute the directional field</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>h_in</code></td>
          <td>
          </td>
          <td><p>The input features of the layer, before any operation.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>dir_idx</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The index of the positional encoding (<span class="arithmatex">\(k\)</span> in the equation above)</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>temperature</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[float]</code>
          </td>
          <td><p>The temperature to use in the softmax of the directional field.
If <code>None</code>, then the softmax is not applied on the field</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="goli.nn.dgl_layers.gat_dgl" class="doc doc-heading">
          <code>goli.nn.dgl_layers.gat_dgl</code>


<a href="#goli.nn.dgl_layers.gat_dgl" class="headerlink" title="Permanent link">&para;</a></h3>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h4 id="goli.nn.dgl_layers.gat_dgl.GATDgl" class="doc doc-heading">
        <code>GATDgl</code>


<a href="#goli.nn.dgl_layers.gat_dgl.GATDgl" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="goli.nn.base_graph_layer.BaseGraphModule" href="#goli.nn.base_graph_layer.BaseGraphModule">BaseGraphModule</a></code></p>




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.gat_dgl.GATDgl.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;elu&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.gat_dgl.GATDgl.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>GAT: Graph Attention Network
Graph Attention Networks (Veličković et al., ICLR 2018)
<a href="https://arxiv.org/abs/1710.10903">https://arxiv.org/abs/1710.10903</a></p>
<p>The implementation is built on top of the DGL <code>GATCONV</code> layer</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>in_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>int
Input feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>out_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>int
Output feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>num_heads</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>int
Number of heads in Multi-Head Attention</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>activation</code></td>
          <td>
          </td>
          <td><p>str, Callable
activation function to use in the layer</p></td>
          <td>
                <code>&#39;elu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>dropout</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>float
The ratio of units to dropout. Must be between 0 and 1</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>normalization</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Normalization to use. Choices:</p>
<ul>
<li>"none" or <code>None</code>: No normalization</li>
<li>"batch_norm": Batch normalization</li>
<li>"layer_norm": Layer normalization in the hidden layers.</li>
<li><code>Callable</code>: Any callable function</li>
</ul></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.gat_dgl.GATDgl.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.gat_dgl.GATDgl.forward" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Apply the graph convolutional layer, with the specified activations,
normalizations and dropout.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>g</code></td>
          <td>
                <code><span title="dgl.DGLGraph">DGLGraph</span></code>
          </td>
          <td><p>dgl.DGLGraph
graph on which the convolution is done</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>h</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>torch.Tensor[..., N, Din]</code>
Node feature tensor, before convolution.
N is the number of nodes, Din is the input dimension <code>self.in_dim</code></p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>torch.Tensor[..., N, Dout]</code>:
Node feature tensor, after convolution.
N is the number of nodes, Dout is the output dimension <code>self.out_dim</code></p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.gat_dgl.GATDgl.layer_inputs_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_inputs_edges</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.dgl_layers.gat_dgl.GATDgl.layer_inputs_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Return a boolean specifying if the layer type
uses edges as input or not.
It is different from <code>layer_supports_edges</code> since a layer that
supports edges can decide to not use them.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>uses_edges</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>bool
Always <code>False</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.gat_dgl.GATDgl.layer_outputs_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_outputs_edges</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.dgl_layers.gat_dgl.GATDgl.layer_outputs_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Abstract method. Return a boolean specifying if the layer type
uses edges as input or not.
It is different from <code>layer_supports_edges</code> since a layer that
supports edges can decide to not use them.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>uses_edges</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>bool
Always <code>False</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.gat_dgl.GATDgl.layer_supports_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_supports_edges</span><span class="p">()</span></code>

<a href="#goli.nn.dgl_layers.gat_dgl.GATDgl.layer_supports_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Return a boolean specifying if the layer type supports edges or not.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>supports_edges</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>bool
Always <code>False</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.gat_dgl.GATDgl.out_dim_factor" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_dim_factor</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.dgl_layers.gat_dgl.GATDgl.out_dim_factor" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Get the factor by which the output dimension is multiplied for
the next layer.</p>
<p>For standard layers, this will return <code>1</code>.</p>
<p>But for others, such as <code>GatLayer</code>, the output is the concatenation
of the outputs from each head, so the out_dim gets multiplied by
the number of heads, and this function should return the number
of heads.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>dim_factor</code></td>          <td>
                <code>int</code>
          </td>
          <td><p>int
Always <code>self.num_heads</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>




  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="goli.nn.dgl_layers.gated_gcn_dgl" class="doc doc-heading">
          <code>goli.nn.dgl_layers.gated_gcn_dgl</code>


<a href="#goli.nn.dgl_layers.gated_gcn_dgl" class="headerlink" title="Permanent link">&para;</a></h3>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h4 id="goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl" class="doc doc-heading">
        <code>GatedGCNDgl</code>


<a href="#goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="goli.nn.base_graph_layer.BaseGraphModule" href="#goli.nn.base_graph_layer.BaseGraphModule">BaseGraphModule</a></code></p>




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">in_dim_edges</span><span class="p">,</span> <span class="n">out_dim_edges</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>ResGatedGCN: Residual Gated Graph ConvNets
An Experimental Study of Neural Networks for Variable Graphs (Xavier Bresson and Thomas Laurent, ICLR 2018)
<a href="https://arxiv.org/pdf/1711.07553v2.pdf">https://arxiv.org/pdf/1711.07553v2.pdf</a></p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>in_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Input feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>out_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Output feature dimensions of the layer, and for the edges</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>in_dim_edges</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Input edge-feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="typing.Callable">Callable</span>, str]</code>
          </td>
          <td><p>activation function to use in the layer</p></td>
          <td>
                <code>&#39;relu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>dropout</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The ratio of units to dropout. Must be between 0 and 1</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>normalization</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Normalization to use. Choices:</p>
<ul>
<li>"none" or <code>None</code>: No normalization</li>
<li>"batch_norm": Batch normalization</li>
<li>"layer_norm": Layer normalization</li>
<li><code>Callable</code>: Any callable function</li>
</ul></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl.forward" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Apply the graph convolutional layer, with the specified activations,
normalizations and dropout.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>g</code></td>
          <td>
                <code><span title="dgl.DGLGraph">DGLGraph</span></code>
          </td>
          <td><p>graph on which the convolution is done</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>h</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>torch.Tensor[..., N, Din]</code>
Node feature tensor, before convolution.
N is the number of nodes, Din is the input dimension <code>self.in_dim</code></p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>e</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>torch.Tensor[..., N, Din_edges]</code>
Edge feature tensor, before convolution.
N is the number of nodes, Din is the input edge dimension  <code>self.in_dim_edges</code></p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>torch.Tensor[..., N, Dout]</code>:
Node feature tensor, after convolution.
N is the number of nodes, Dout is the output dimension <code>self.out_dim</code></p></td>
        </tr>
        <tr>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>torch.Tensor[..., N, Dout]</code>:
Edge feature tensor, after convolution.
N is the number of nodes, Dout_edges is the output edge dimension <code>self.out_dim</code></p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl.layer_inputs_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_inputs_edges</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl.layer_inputs_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Return a boolean specifying if the layer type
uses edges as input or not.
It is different from <code>layer_supports_edges</code> since a layer that
supports edges can decide to not use them.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>bool</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Always <code>True</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl.layer_outputs_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_outputs_edges</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl.layer_outputs_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Abstract method. Return a boolean specifying if the layer type
uses edges as input or not.
It is different from <code>layer_supports_edges</code> since a layer that
supports edges can decide to not use them.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>bool</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Always <code>True</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl.layer_supports_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_supports_edges</span><span class="p">()</span></code>

<a href="#goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl.layer_supports_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Return a boolean specifying if the layer type supports edges or not.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>bool</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Always <code>True</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl.out_dim_factor" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_dim_factor</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.dgl_layers.gated_gcn_dgl.GatedGCNDgl.out_dim_factor" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Get the factor by which the output dimension is multiplied for
the next layer.</p>
<p>For standard layers, this will return <code>1</code>.</p>
<p>But for others, such as <code>GatLayer</code>, the output is the concatenation
of the outputs from each head, so the out_dim gets multiplied by
the number of heads, and this function should return the number
of heads.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>int</code></td>          <td>
                <code>int</code>
          </td>
          <td><p>Always <code>1</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>




  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="goli.nn.dgl_layers.gcn_dgl" class="doc doc-heading">
          <code>goli.nn.dgl_layers.gcn_dgl</code>


<a href="#goli.nn.dgl_layers.gcn_dgl" class="headerlink" title="Permanent link">&para;</a></h3>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h4 id="goli.nn.dgl_layers.gcn_dgl.GCNDgl" class="doc doc-heading">
        <code>GCNDgl</code>


<a href="#goli.nn.dgl_layers.gcn_dgl.GCNDgl" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="goli.nn.base_graph_layer.BaseGraphModule" href="#goli.nn.base_graph_layer.BaseGraphModule">BaseGraphModule</a></code></p>




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.gcn_dgl.GCNDgl.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.gcn_dgl.GCNDgl.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Graph convolutional network (GCN) layer from
Thomas N. Kipf, Max Welling, Semi-Supervised Classification with Graph Convolutional Networks (ICLR 2017)
<a href="http://arxiv.org/abs/1609.02907">http://arxiv.org/abs/1609.02907</a></p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>in_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Input feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>out_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Output feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>activation function to use in the layer</p></td>
          <td>
                <code>&#39;relu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>dropout</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The ratio of units to dropout. Must be between 0 and 1</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>normalization</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Normalization to use. Choices:</p>
<ul>
<li>"none" or <code>None</code>: No normalization</li>
<li>"batch_norm": Batch normalization</li>
<li>"layer_norm": Layer normalization</li>
<li><code>Callable</code>: Any callable function</li>
</ul></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.gcn_dgl.GCNDgl.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.gcn_dgl.GCNDgl.forward" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Apply the graph convolutional layer, with the specified activations,
normalizations and dropout.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>g</code></td>
          <td>
                <code><span title="dgl.DGLGraph">DGLGraph</span></code>
          </td>
          <td><p>graph on which the convolution is done</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>h</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>torch.Tensor[..., N, Din]</code>
Node feature tensor, before convolution.
N is the number of nodes, Din is the input dimension <code>self.in_dim</code></p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>torch.Tensor[..., N, Dout]</code>:
Node feature tensor, after convolution.
N is the number of nodes, Dout is the output dimension <code>self.out_dim</code></p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.gcn_dgl.GCNDgl.layer_inputs_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_inputs_edges</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.dgl_layers.gcn_dgl.GCNDgl.layer_inputs_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Return a boolean specifying if the layer type
uses edges as input or not.
It is different from <code>layer_supports_edges</code> since a layer that
supports edges can decide to not use them.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>bool</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Always <code>False</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.gcn_dgl.GCNDgl.layer_outputs_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_outputs_edges</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.dgl_layers.gcn_dgl.GCNDgl.layer_outputs_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Abstract method. Return a boolean specifying if the layer type
uses edges as input or not.
It is different from <code>layer_supports_edges</code> since a layer that
supports edges can decide to not use them.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>bool</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Always <code>False</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.gcn_dgl.GCNDgl.layer_supports_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_supports_edges</span><span class="p">()</span></code>

<a href="#goli.nn.dgl_layers.gcn_dgl.GCNDgl.layer_supports_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Return a boolean specifying if the layer type supports edges or not.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>bool</code>
          </td>
          <td><p>bool
Always <code>False</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.gcn_dgl.GCNDgl.out_dim_factor" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_dim_factor</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.dgl_layers.gcn_dgl.GCNDgl.out_dim_factor" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Get the factor by which the output dimension is multiplied for
the next layer.</p>
<p>For standard layers, this will return <code>1</code>.</p>
<p>But for others, such as <code>GatLayer</code>, the output is the concatenation
of the outputs from each head, so the out_dim gets multiplied by
the number of heads, and this function should return the number
of heads.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>int</code></td>          <td>
                <code>int</code>
          </td>
          <td><p>Always <code>1</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>




  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="goli.nn.dgl_layers.gin_dgl" class="doc doc-heading">
          <code>goli.nn.dgl_layers.gin_dgl</code>


<a href="#goli.nn.dgl_layers.gin_dgl" class="headerlink" title="Permanent link">&para;</a></h3>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h4 id="goli.nn.dgl_layers.gin_dgl.GINDgl" class="doc doc-heading">
        <code>GINDgl</code>


<a href="#goli.nn.dgl_layers.gin_dgl.GINDgl" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="goli.nn.base_graph_layer.BaseGraphModule" href="#goli.nn.base_graph_layer.BaseGraphModule">BaseGraphModule</a></code></p>




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.gin_dgl.GINDgl.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">init_eps</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">learn_eps</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.gin_dgl.GINDgl.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>GIN: Graph Isomorphism Networks
HOW POWERFUL ARE GRAPH NEURAL NETWORKS? (Keyulu Xu, Weihua Hu, Jure Leskovec and Stefanie Jegelka, ICLR 2019)
<a href="https://arxiv.org/pdf/1810.00826.pdf">https://arxiv.org/pdf/1810.00826.pdf</a></p>
<p>[!] code adapted from dgl implementation of GINConv</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>in_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Input feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>out_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Output feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="typing.Callable">Callable</span>, str]</code>
          </td>
          <td><p>activation function to use in the layer</p></td>
          <td>
                <code>&#39;relu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>dropout</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The ratio of units to dropout. Must be between 0 and 1</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>normalization</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Normalization to use. Choices:</p>
<ul>
<li>"none" or <code>None</code>: No normalization</li>
<li>"batch_norm": Batch normalization</li>
<li>"layer_norm": Layer normalization</li>
<li><code>Callable</code>: Any callable function</li>
</ul></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>init_eps</code></td>
          <td>
          </td>
          <td><p>Initial :math:<code>\epsilon</code> value, default: <code>0</code>.</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>learn_eps</code></td>
          <td>
          </td>
          <td><p>If True, :math:<code>\epsilon</code> will be a learnable parameter.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.gin_dgl.GINDgl.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.gin_dgl.GINDgl.forward" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Apply the GIN convolutional layer, with the specified activations,
normalizations and dropout.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>g</code></td>
          <td>
                <code><span title="dgl.DGLGraph">DGLGraph</span></code>
          </td>
          <td><p>graph on which the convolution is done</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>h</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>torch.Tensor[..., N, Din]</code>
Node feature tensor, before convolution.
N is the number of nodes, Din is the input dimension <code>self.in_dim</code></p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>torch.Tensor[..., N, Dout]</code>:
Node feature tensor, after convolution.
N is the number of nodes, Dout is the output dimension <code>self.out_dim</code></p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.gin_dgl.GINDgl.layer_inputs_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_inputs_edges</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.dgl_layers.gin_dgl.GINDgl.layer_inputs_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Return a boolean specifying if the layer type
uses edges as input or not.
It is different from <code>layer_supports_edges</code> since a layer that
supports edges can decide to not use them.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>bool</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Always <code>False</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.gin_dgl.GINDgl.layer_outputs_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_outputs_edges</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.dgl_layers.gin_dgl.GINDgl.layer_outputs_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Abstract method. Return a boolean specifying if the layer type
uses edges as input or not.
It is different from <code>layer_supports_edges</code> since a layer that
supports edges can decide to not use them.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>bool</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Always <code>False</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.gin_dgl.GINDgl.layer_supports_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_supports_edges</span><span class="p">()</span></code>

<a href="#goli.nn.dgl_layers.gin_dgl.GINDgl.layer_supports_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Return a boolean specifying if the layer type supports edges or not.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>supports_edges</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>bool
Always <code>False</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.gin_dgl.GINDgl.message_func" class="doc doc-heading">
<code class="highlight language-python"><span class="n">message_func</span><span class="p">(</span><span class="n">g</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.gin_dgl.GINDgl.message_func" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>If edge weights are provided, use them to weight the messages</p>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.gin_dgl.GINDgl.out_dim_factor" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_dim_factor</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.dgl_layers.gin_dgl.GINDgl.out_dim_factor" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Get the factor by which the output dimension is multiplied for
the next layer.</p>
<p>For standard layers, this will return <code>1</code>.</p>
<p>But for others, such as <code>GatLayer</code>, the output is the concatenation
of the outputs from each head, so the out_dim gets multiplied by
the number of heads, and this function should return the number
of heads.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>int</code></td>          <td>
                <code>int</code>
          </td>
          <td><p>Always <code>1</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>




  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="goli.nn.dgl_layers.pna_dgl" class="doc doc-heading">
          <code>goli.nn.dgl_layers.pna_dgl</code>


<a href="#goli.nn.dgl_layers.pna_dgl" class="headerlink" title="Permanent link">&para;</a></h3>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h4 id="goli.nn.dgl_layers.pna_dgl.BasePNADgl" class="doc doc-heading">
        <code>BasePNADgl</code>


<a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="goli.nn.base_graph_layer.BaseGraphModule" href="#goli.nn.base_graph_layer.BaseGraphModule">BaseGraphModule</a></code></p>




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.pna_dgl.BasePNADgl.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">aggregators</span><span class="p">,</span> <span class="n">scalers</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">avg_d</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">last_activation</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">in_dim_edges</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Abstract class used to standardize the implementation of PNA layers
in the current library.</p>
<p>PNA: Principal Neighbourhood Aggregation
Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Lio, Petar Velickovic
<a href="https://arxiv.org/abs/2004.05718">https://arxiv.org/abs/2004.05718</a></p>
<p>Method <code>layer_inputs_edges()</code> needs to be implemented in children classes</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>in_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Input feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>out_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Output feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>aggregators</code></td>
          <td>
                <code><span title="typing.List">List</span>[str]</code>
          </td>
          <td><p>Set of aggregation function identifiers,
e.g. "mean", "max", "min", "std", "sum", "var", "moment3".
The results from all aggregators will be concatenated.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>scalers</code></td>
          <td>
                <code><span title="typing.List">List</span>[str]</code>
          </td>
          <td><p>Set of scaling functions identifiers
e.g. "identidy", "amplification", "attenuation"
The results from all scalers will be concatenated</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="typing.Callable">Callable</span>, str]</code>
          </td>
          <td><p>activation function to use in the layer</p></td>
          <td>
                <code>&#39;relu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>dropout</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The ratio of units to dropout. Must be between 0 and 1</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>normalization</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Normalization to use. Choices:</p>
<ul>
<li>"none" or <code>None</code>: No normalization</li>
<li>"batch_norm": Batch normalization</li>
<li>"layer_norm": Layer normalization</li>
<li><code>Callable</code>: Any callable function</li>
</ul></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>avg_d</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>Average degree of nodes in the training set, used by scalers to normalize</p></td>
          <td>
                <code>1.0</code>
          </td>
        </tr>
        <tr>
          <td><code>last_activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="typing.Callable">Callable</span>, str]</code>
          </td>
          <td><p>activation function to use in the last layer of the internal MLP</p></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>in_dim_edges</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>size of the edge features. If 0, edges are ignored</p></td>
          <td>
                <code>0</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.pna_dgl.BasePNADgl.add_virtual_graph_if_no_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">add_virtual_graph_if_no_edges</span><span class="p">(</span><span class="n">g</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.add_virtual_graph_if_no_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>When all elements of a given batch don't have any edges
(e.g. molecule with a single atom), the message function will
be skipped, and the number of features will be inconsistent due
to the variable number of aggregators.</p>
<p>To fix this issue, this method creates a new graph with self-loop
and appends it to the batch, only if all the elements of the batch
have degree 0.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>g</code></td>
          <td>
                <code><span title="dgl.DGLGraph">DGLGraph</span></code>
          </td>
          <td><p>The batched graphs</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>g</code></td>          <td>
                <code><span title="dgl.DGLGraph">DGLGraph</span></code>
          </td>
          <td><p>The batched graphs, with a possible new graph appended at the end</p></td>
        </tr>
        <tr>
<td><code>no_edges</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Whether a graph was appended to the end of the batch
if all the elements of the batch had no edges.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.pna_dgl.BasePNADgl.layer_inputs_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_inputs_edges</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.layer_inputs_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Return a boolean specifying if the layer type
uses edges as input or not.
It is different from <code>layer_supports_edges</code> since a layer that
supports edges can decide to not use them.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>bool</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Returns <code>self.edge_features</code></p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.pna_dgl.BasePNADgl.layer_outputs_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_outputs_edges</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.layer_outputs_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Abstract method. Return a boolean specifying if the layer type
uses edges as input or not.
It is different from <code>layer_supports_edges</code> since a layer that
supports edges can decide to not use them.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>bool</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Always <code>False</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.pna_dgl.BasePNADgl.layer_supports_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_supports_edges</span><span class="p">()</span></code>

<a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.layer_supports_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Return a boolean specifying if the layer type supports edges or not.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>bool</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Always <code>True</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.pna_dgl.BasePNADgl.message_func" class="doc doc-heading">
<code class="highlight language-python"><span class="n">message_func</span><span class="p">(</span><span class="n">edges</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.message_func" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>The message function to generate messages along the edges.</p>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.pna_dgl.BasePNADgl.out_dim_factor" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_dim_factor</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.out_dim_factor" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Get the factor by which the output dimension is multiplied for
the next layer.</p>
<p>For standard layers, this will return <code>1</code>.</p>
<p>But for others, such as <code>GatLayer</code>, the output is the concatenation
of the outputs from each head, so the out_dim gets multiplied by
the number of heads, and this function should return the number
of heads.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>int</code></td>          <td>
                <code>int</code>
          </td>
          <td><p>Always <code>1</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.pna_dgl.BasePNADgl.reduce_func" class="doc doc-heading">
<code class="highlight language-python"><span class="n">reduce_func</span><span class="p">(</span><span class="n">nodes</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.reduce_func" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>The reduce function to aggregate the messages.
Apply the aggregators and scalers, and concatenate the results.</p>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.pna_dgl.BasePNADgl.remove_virtual_graph_if_no_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">remove_virtual_graph_if_no_edges</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">no_edges</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl.remove_virtual_graph_if_no_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>This removes the added graph from the method
<code>add_virtual_graph_if_no_edges</code>.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>g</code></td>
          <td>
                <code><span title="dgl.DGLGraph">DGLGraph</span></code>
          </td>
          <td><p>The batched graphs</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>no_edges</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to remove the last graph of the batch
if all the elements of the batch had no edges.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>g</code></td>          <td>
                <code><span title="dgl.DGLGraph">DGLGraph</span></code>
          </td>
          <td><p>The batched graphs, with a possible new graph appended at the end</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="goli.nn.dgl_layers.pna_dgl.PNAConvolutionalDgl" class="doc doc-heading">
        <code>PNAConvolutionalDgl</code>


<a href="#goli.nn.dgl_layers.pna_dgl.PNAConvolutionalDgl" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="goli.nn.dgl_layers.pna_dgl.BasePNADgl" href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl">BasePNADgl</a></code></p>

  
      <p>Implementation of the convolutional architecture of the PNA layer,
previously known as <code>PNASimpleLayer</code>. This layer aggregates the
neighbouring messages using multiple aggregators and scalers,
concatenates their results, then applies an MLP on the concatenated
features.</p>
<p>PNA: Principal Neighbourhood Aggregation
Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Lio, Petar Velickovic
<a href="https://arxiv.org/abs/2004.05718">https://arxiv.org/abs/2004.05718</a></p>



  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.pna_dgl.PNAConvolutionalDgl.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">aggregators</span><span class="p">,</span> <span class="n">scalers</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">avg_d</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;log&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">},</span> <span class="n">last_activation</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">posttrans_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">in_dim_edges</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.pna_dgl.PNAConvolutionalDgl.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>in_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Input feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>out_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Output feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>aggregators</code></td>
          <td>
                <code><span title="typing.List">List</span>[str]</code>
          </td>
          <td><p>Set of aggregation function identifiers,
e.g. "mean", "max", "min", "std", "sum", "var", "moment3".
The results from all aggregators will be concatenated.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>scalers</code></td>
          <td>
                <code><span title="typing.List">List</span>[str]</code>
          </td>
          <td><p>Set of scaling functions identifiers
e.g. "identidy", "amplification", "attenuation"
The results from all scalers will be concatenated</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="typing.Callable">Callable</span>, str]</code>
          </td>
          <td><p>activation function to use in the layer</p></td>
          <td>
                <code>&#39;relu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>dropout</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The ratio of units to dropout. Must be between 0 and 1</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>normalization</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Normalization to use. Choices:</p>
<ul>
<li>"none" or <code>None</code>: No normalization</li>
<li>"batch_norm": Batch normalization</li>
<li>"layer_norm": Layer normalization</li>
<li><code>Callable</code>: Any callable function</li>
</ul></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>avg_d</code></td>
          <td>
                <code><span title="typing.Dict">Dict</span>[str, float]</code>
          </td>
          <td><p>Average degree of nodes in the training set, used by scalers to normalize</p></td>
          <td>
                <code>{&#39;log&#39;: 1.0}</code>
          </td>
        </tr>
        <tr>
          <td><code>last_activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="typing.Callable">Callable</span>, str]</code>
          </td>
          <td><p>activation function to use in the last layer of the internal MLP</p></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>posttrans_layers</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>number of layers in the MLP transformation after the aggregation</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>in_dim_edges</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>size of the edge features. If 0, edges are ignored</p></td>
          <td>
                <code>0</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.pna_dgl.PNAConvolutionalDgl.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">e</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.pna_dgl.PNAConvolutionalDgl.forward" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Apply the PNA convolutional layer, with the specified post transformation</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>g</code></td>
          <td>
                <code><span title="dgl.DGLGraph">DGLGraph</span></code>
          </td>
          <td><p>graph on which the convolution is done</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>h</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>torch.Tensor[..., N, Din]</code>
Node feature tensor, before convolution.
N is the number of nodes, Din is the input dimension <code>self.in_dim</code></p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>e</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>torch.Tensor[..., N, Din_edges]</code> or <code>None</code>
Edge feature tensor, before convolution.
N is the number of nodes, Din is the input edge dimension</p>
<p>Can be set to None if the layer does not use edge features
i.e. <code>self.layer_inputs_edges -&gt; False</code></p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>torch.Tensor[..., N, Dout]</code>:
Node feature tensor, after convolution.
N is the number of nodes, Dout is the output dimension <code>self.out_dim</code></p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.pna_dgl.PNAConvolutionalDgl.pretrans_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">pretrans_edges</span><span class="p">(</span><span class="n">edges</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.pna_dgl.PNAConvolutionalDgl.pretrans_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Return a mapping to the features of the source nodes, concatenated to the
edge data.</p>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="goli.nn.dgl_layers.pna_dgl.PNAMessagePassingDgl" class="doc doc-heading">
        <code>PNAMessagePassingDgl</code>


<a href="#goli.nn.dgl_layers.pna_dgl.PNAMessagePassingDgl" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="goli.nn.dgl_layers.pna_dgl.BasePNADgl" href="#goli.nn.dgl_layers.pna_dgl.BasePNADgl">BasePNADgl</a></code></p>

  
      <p>Implementation of the message passing architecture of the PNA message passing layer,
previously known as <code>PNALayerComplex</code>. This layer applies an MLP as
pretransformation to the concatenation of <span class="arithmatex">\([h_u, h_v, e_{uv}]\)</span> to generate
the messages, with <span class="arithmatex">\(h_u\)</span> the node feature, <span class="arithmatex">\(h_v\)</span> the neighbour node features,
and <span class="arithmatex">\(e_{uv}\)</span> the edge feature between the nodes <span class="arithmatex">\(u\)</span> and <span class="arithmatex">\(v\)</span>.</p>
<p>After the pre-transformation, it aggregates the messages
multiple aggregators and scalers,
concatenates their results, then applies an MLP on the concatenated
features.</p>
<p>PNA: Principal Neighbourhood Aggregation
Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Lio, Petar Velickovic
<a href="https://arxiv.org/abs/2004.05718">https://arxiv.org/abs/2004.05718</a></p>



  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.pna_dgl.PNAMessagePassingDgl.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">aggregators</span><span class="p">,</span> <span class="n">scalers</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">avg_d</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;log&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">},</span> <span class="n">last_activation</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">posttrans_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pretrans_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">in_dim_edges</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.pna_dgl.PNAMessagePassingDgl.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>in_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Input feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>out_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Output feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>aggregators</code></td>
          <td>
                <code><span title="typing.List">List</span>[str]</code>
          </td>
          <td><p>Set of aggregation function identifiers,
e.g. "mean", "max", "min", "std", "sum", "var", "moment3".
The results from all aggregators will be concatenated.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>scalers</code></td>
          <td>
                <code><span title="typing.List">List</span>[str]</code>
          </td>
          <td><p>Set of scaling functions identifiers
e.g. "identidy", "amplification", "attenuation"
The results from all scalers will be concatenated</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="typing.Callable">Callable</span>, str]</code>
          </td>
          <td><p>activation function to use in the layer</p></td>
          <td>
                <code>&#39;relu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>dropout</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The ratio of units to dropout. Must be between 0 and 1</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>normalization</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Normalization to use. Choices:</p>
<ul>
<li>"none" or <code>None</code>: No normalization</li>
<li>"batch_norm": Batch normalization</li>
<li>"layer_norm": Layer normalization</li>
<li><code>Callable</code>: Any callable function</li>
</ul></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>avg_d</code></td>
          <td>
                <code><span title="typing.Dict">Dict</span>[str, float]</code>
          </td>
          <td><p>Average degree of nodes in the training set, used by scalers to normalize</p></td>
          <td>
                <code>{&#39;log&#39;: 1.0}</code>
          </td>
        </tr>
        <tr>
          <td><code>last_activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="typing.Callable">Callable</span>, str]</code>
          </td>
          <td><p>activation function to use in the last layer of the internal MLP</p></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>posttrans_layers</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>number of layers in the MLP transformation after the aggregation</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>pretrans_layers</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>number of layers in the transformation before the aggregation</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>in_dim_edges</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>size of the edge features. If 0, edges are ignored</p></td>
          <td>
                <code>0</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.pna_dgl.PNAMessagePassingDgl.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">e</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.pna_dgl.PNAMessagePassingDgl.forward" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Apply the PNA Message passing layer, with the specified pre/post transformations</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>g</code></td>
          <td>
                <code><span title="dgl.DGLGraph">DGLGraph</span></code>
          </td>
          <td><p>graph on which the convolution is done</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>h</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>torch.Tensor[..., N, Din]</code>
Node feature tensor, before convolution.
N is the number of nodes, Din is the input dimension <code>self.in_dim</code></p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>e</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>torch.Tensor[..., N, Din_edges]</code> or <code>None</code>
Edge feature tensor, before convolution.
N is the number of nodes, Din is the input edge dimension</p>
<p>Can be set to None if the layer does not use edge features
i.e. <code>self.layer_inputs_edges -&gt; False</code></p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>torch.Tensor[..., N, Dout]</code>:
Node feature tensor, after convolution.
N is the number of nodes, Dout is the output dimension <code>self.out_dim</code></p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.pna_dgl.PNAMessagePassingDgl.layer_supports_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_supports_edges</span><span class="p">()</span></code>

<a href="#goli.nn.dgl_layers.pna_dgl.PNAMessagePassingDgl.layer_supports_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Return a boolean specifying if the layer type supports edges or not.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>bool</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Always <code>True</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.pna_dgl.PNAMessagePassingDgl.pretrans_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">pretrans_edges</span><span class="p">(</span><span class="n">edges</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.pna_dgl.PNAMessagePassingDgl.pretrans_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Return a mapping to the concatenation of the features from
the source node, the destination node, and the edge between them (if applicable).</p>

  </div>

</div>



  </div>

  </div>

</div>




  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="goli.nn.dgl_layers.pna_operations" class="doc doc-heading">
          <code>goli.nn.dgl_layers.pna_operations</code>


<a href="#goli.nn.dgl_layers.pna_operations" class="headerlink" title="Permanent link">&para;</a></h3>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">











  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="goli.nn.dgl_layers.pooling_dgl" class="doc doc-heading">
          <code>goli.nn.dgl_layers.pooling_dgl</code>


<a href="#goli.nn.dgl_layers.pooling_dgl" class="headerlink" title="Permanent link">&para;</a></h3>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h4 id="goli.nn.dgl_layers.pooling_dgl.DirPoolingDgl" class="doc doc-heading">
        <code>DirPoolingDgl</code>


<a href="#goli.nn.dgl_layers.pooling_dgl.DirPoolingDgl" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>

  
      <p>Apply pooling over the nodes in the graph using a directional potential
with an inner product.</p>
<p>In most cases, this is a pooling using the Fiedler vector.
This is basically equivalent to computing a Fourier transform for the
Fiedler vector. Then, we use the absolute value due to the sign ambiguity</p>



  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.pooling_dgl.DirPoolingDgl.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">feat</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.pooling_dgl.DirPoolingDgl.forward" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Compute directional inner-product pooling, and return absolute value.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>graph</code></td>
          <td>
          </td>
          <td><p>DGLGraph
The graph. Must have the key <code>graph.ndata["pos_dir"]</code></p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>feat</code></td>
          <td>
          </td>
          <td><p>torch.Tensor
The input feature with shape :math:<code>(N, *)</code> where
:math:<code>N</code> is the number of nodes in the graph.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>readout</code></td>          <td>
          </td>
          <td><p>torch.Tensor
The output feature with shape :math:<code>(B, *)</code>, where
:math:<code>B</code> refers to the batch size.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="goli.nn.dgl_layers.pooling_dgl.LogSumPoolingDgl" class="doc doc-heading">
        <code>LogSumPoolingDgl</code>


<a href="#goli.nn.dgl_layers.pooling_dgl.LogSumPoolingDgl" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><span title="dgl.nn.pytorch.glob.AvgPooling">AvgPooling</span></code></p>

  
      <p>Apply pooling over the nodes in the graph using a mean aggregation,
but scaled by the log of the number of nodes. This gives the same
expressive power as the sum, but helps deal with graphs that are
significantly larger than others by using a logarithmic scale.</p>
<div class="arithmatex">\[r^{(i)} = \frac{\log N_i}{N_i}\sum_{k=1}^{N_i} x^{(i)}_k\]</div>



  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.pooling_dgl.LogSumPoolingDgl.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">feat</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.pooling_dgl.LogSumPoolingDgl.forward" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Compute log-sum pooling.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>graph</code></td>
          <td>
          </td>
          <td><p>DGLGraph
The graph.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>feat</code></td>
          <td>
          </td>
          <td><p>torch.Tensor
The input feature with shape :math:<code>(N, *)</code> where
:math:<code>N</code> is the number of nodes in the graph.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>readout</code></td>          <td>
          </td>
          <td><p>torch.Tensor
The output feature with shape :math:<code>(B, *)</code>, where
:math:<code>B</code> refers to the batch size.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="goli.nn.dgl_layers.pooling_dgl.MinPoolingDgl" class="doc doc-heading">
        <code>MinPoolingDgl</code>


<a href="#goli.nn.dgl_layers.pooling_dgl.MinPoolingDgl" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><span title="dgl.nn.pytorch.glob.MaxPooling">MaxPooling</span></code></p>

  
      <p>Apply min pooling over the nodes in the graph.</p>
<div class="arithmatex">\[r^{(i)} = \min_{k=1}^{N_i}\left( x^{(i)}_k \right)\]</div>



  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.pooling_dgl.MinPoolingDgl.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">feat</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.pooling_dgl.MinPoolingDgl.forward" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Compute max pooling.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>graph</code></td>
          <td>
          </td>
          <td><p>DGLGraph
The graph.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>feat</code></td>
          <td>
          </td>
          <td><p>torch.Tensor
The input feature with shape :math:<code>(N, *)</code> where
:math:<code>N</code> is the number of nodes in the graph.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>readout</code></td>          <td>
          </td>
          <td><p>torch.Tensor
The output feature with shape :math:<code>(B, *)</code>, where
:math:<code>B</code> refers to the batch size.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="goli.nn.dgl_layers.pooling_dgl.S2SReadoutDgl" class="doc doc-heading">
        <code>S2SReadoutDgl</code>


<a href="#goli.nn.dgl_layers.pooling_dgl.S2SReadoutDgl" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>

  
      <p>Performs a Set2Set aggregation of all the graph nodes' features followed by a series of fully connected layers</p>



  

  <div class="doc doc-children">











  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="goli.nn.dgl_layers.pooling_dgl.StdPoolingDgl" class="doc doc-heading">
        <code>StdPoolingDgl</code>


<a href="#goli.nn.dgl_layers.pooling_dgl.StdPoolingDgl" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>

  
      <p>Apply standard deviation pooling over the nodes in the graph.</p>
<div class="arithmatex">\[r^{(i)} = \sigma_{k=1}^{N_i}\left( x^{(i)}_k \right)\]</div>



  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.pooling_dgl.StdPoolingDgl.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">feat</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.pooling_dgl.StdPoolingDgl.forward" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Compute standard deviation pooling.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>graph</code></td>
          <td>
          </td>
          <td><p>DGLGraph
The graph.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>feat</code></td>
          <td>
          </td>
          <td><p>torch.Tensor
The input feature with shape :math:<code>(N, *)</code> where
:math:<code>N</code> is the number of nodes in the graph.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td><p>torch.Tensor
The output feature with shape :math:<code>(B, *)</code>, where
:math:<code>B</code> refers to the batch size.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="goli.nn.dgl_layers.pooling_dgl.VirtualNodeDgl" class="doc doc-heading">
        <code>VirtualNodeDgl</code>


<a href="#goli.nn.dgl_layers.pooling_dgl.VirtualNodeDgl" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.pooling_dgl.VirtualNodeDgl.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">vn_type</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">residual</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.pooling_dgl.VirtualNodeDgl.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>The VirtualNode is a layer that pool the features of the graph,
applies a neural network layer on the pooled features,
then add the result back to the node features of every node.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Input and output feature dimensions of the virtual node layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>activation function to use in the neural network layer.</p></td>
          <td>
                <code>&#39;relu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>dropout</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The ratio of units to dropout. Must be between 0 and 1</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>normalization</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Normalization to use. Choices:</p>
<ul>
<li>"none" or <code>None</code>: No normalization</li>
<li>"batch_norm": Batch normalization</li>
<li>"layer_norm": Layer normalization</li>
<li><code>Callable</code>: Any callable function</li>
</ul></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>bias</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to add a bias to the neural network</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
        <tr>
          <td><code>residual</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether all virtual nodes should be connected together
via a residual connection</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.dgl_layers.pooling_dgl.VirtualNodeDgl.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">vn_h</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.pooling_dgl.VirtualNodeDgl.forward" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Apply the virtual node layer.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>g</code></td>
          <td>
                <code>dgl.<span title="dgl.DGLGraph">DGLGraph</span></code>
          </td>
          <td><p>graph on which the convolution is done</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>h</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span>[..., N, Din]</code>
          </td>
          <td><p>Node feature tensor, before convolution.
<code>N</code> is the number of nodes, <code>Din</code> is the input features</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>vn_h</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span>[..., M, Din]</code>
          </td>
          <td><p>Graph feature of the previous virtual node, or <code>None</code>
<code>M</code> is the number of graphs, <code>Din</code> is the input features.
It is added to the result after the MLP, as a residual connection</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>h = torch.Tensor[..., N, Dout]</code>:
Node feature tensor, after convolution and residual.
<code>N</code> is the number of nodes, <code>Dout</code> is the output features of the layer and residual</p></td>
        </tr>
        <tr>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>vn_h = torch.Tensor[..., M, Dout]</code>:
Graph feature tensor to be used at the next virtual node, or <code>None</code>
<code>M</code> is the number of graphs, <code>Dout</code> is the output features</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>


<div class="doc doc-object doc-function">



<h4 id="goli.nn.dgl_layers.pooling_dgl.parse_pooling_layer_dgl" class="doc doc-heading">
<code class="highlight language-python"><span class="n">parse_pooling_layer_dgl</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">pooling</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#goli.nn.dgl_layers.pooling_dgl.parse_pooling_layer_dgl" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Select the pooling layers from a list of strings, and put them
in a Module that concatenates their outputs.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>in_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The dimension at the input layer of the pooling</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>pooling</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.List">List</span>[str]]</code>
          </td>
          <td><p>The list of pooling layers to use. The accepted strings are:</p>
<ul>
<li>"sum": <code>SumPooling</code></li>
<li>"mean": <code>MeanPooling</code></li>
<li>"max": <code>MaxPooling</code></li>
<li>"min": <code>MinPooling</code></li>
<li>"std": <code>StdPooling</code></li>
<li>"s2s": <code>Set2Set</code></li>
<li>"dir{int}": <code>DirPooling</code></li>
</ul></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="goli.nn.encoders.laplace_pos_encoder" class="doc doc-heading">
          <code>goli.nn.encoders.laplace_pos_encoder</code>


<a href="#goli.nn.encoders.laplace_pos_encoder" class="headerlink" title="Permanent link">&para;</a></h3>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h4 id="goli.nn.encoders.laplace_pos_encoder.LapPENodeEncoder" class="doc doc-heading">
        <code>LapPENodeEncoder</code>


<a href="#goli.nn.encoders.laplace_pos_encoder.LapPENodeEncoder" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code>torch.<span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>

  
      <p>Laplace Positional Embedding node encoder.</p>
<p>LapPE of size dim_pe will get appended to each node feature vector.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>dim_emb</code></td>
          <td>
          </td>
          <td><p>Size of final node embedding</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>expand_x</code></td>
          <td>
          </td>
          <td><p>Expand node features <code>x</code> from dim_in to (dim_emb - dim_pe)</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  

  <div class="doc doc-children">











  </div>

  </div>

</div>




  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="goli.nn.encoders.mlp_encoder" class="doc doc-heading">
          <code>goli.nn.encoders.mlp_encoder</code>


<a href="#goli.nn.encoders.mlp_encoder" class="headerlink" title="Permanent link">&para;</a></h3>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h4 id="goli.nn.encoders.mlp_encoder.MLPEncoder" class="doc doc-heading">
        <code>MLPEncoder</code>


<a href="#goli.nn.encoders.mlp_encoder.MLPEncoder" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code>torch.<span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>

  
      <p>Configurable kernel-based Positional Encoding node encoder.</p>
<p>The choice of which kernel-based statistics to use is configurable through
setting of <code>kernel_type</code>. Based on this, the appropriate config is selected,
and also the appropriate variable with precomputed kernel stats is then
selected from PyG Data graphs in <code>forward</code> function.
E.g., supported are 'RWSE', 'HKdiagSE', 'ElstaticSE'.</p>
<p>PE of size <code>dim_pe</code> will get appended to each node feature vector.
If <code>expand_x</code> set True, original node features will be first linearly
projected to (dim_emb - dim_pe) size and the concatenated with PE.</p>



  

  <div class="doc doc-children">











  </div>

  </div>

</div>




  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="goli.nn.encoders.signnet_pos_encoder" class="doc doc-heading">
          <code>goli.nn.encoders.signnet_pos_encoder</code>


<a href="#goli.nn.encoders.signnet_pos_encoder" class="headerlink" title="Permanent link">&para;</a></h3>

  <div class="doc doc-contents first">
  
      <p>SignNet <a href="https://arxiv.org/abs/2202.13013">https://arxiv.org/abs/2202.13013</a>
based on <a href="https://github.com/cptq/SignNet-BasisNet">https://github.com/cptq/SignNet-BasisNet</a></p>

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h4 id="goli.nn.encoders.signnet_pos_encoder.GINDeepSigns" class="doc doc-heading">
        <code>GINDeepSigns</code>


<a href="#goli.nn.encoders.signnet_pos_encoder.GINDeepSigns" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>

  
      <p>Sign invariant neural network with MLP aggregation.
f(v1, ..., vk) = rho(enc(v1) + enc(-v1), ..., enc(vk) + enc(-vk))</p>



  

  <div class="doc doc-children">











  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="goli.nn.encoders.signnet_pos_encoder.MaskedGINDeepSigns" class="doc doc-heading">
        <code>MaskedGINDeepSigns</code>


<a href="#goli.nn.encoders.signnet_pos_encoder.MaskedGINDeepSigns" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>

  
      <p>Sign invariant neural network with sum pooling and DeepSet.
f(v1, ..., vk) = rho(enc(v1) + enc(-v1), ..., enc(vk) + enc(-vk))</p>



  

  <div class="doc doc-children">











  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="goli.nn.encoders.signnet_pos_encoder.SignNetNodeEncoder" class="doc doc-heading">
        <code>SignNetNodeEncoder</code>


<a href="#goli.nn.encoders.signnet_pos_encoder.SignNetNodeEncoder" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code>torch.<span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>

  
      <p>SignNet Positional Embedding node encoder.
<a href="https://arxiv.org/abs/2202.13013">https://arxiv.org/abs/2202.13013</a>
<a href="https://github.com/cptq/SignNet-BasisNet">https://github.com/cptq/SignNet-BasisNet</a></p>
<p>Uses precomputated Laplacian eigen-decomposition, but instead
of eigen-vector sign flipping + DeepSet/Transformer, computes the PE as:
SignNetPE(v_1, ... , v_k) = \rho ( [\phi(v_i) + \rhi(-v_i)]^k_i=1 )
where \phi is GIN network applied to k first non-trivial eigenvectors, and
\rho is an MLP if k is a constant, but if all eigenvectors are used then
\rho is DeepSet with sum-pooling.</p>
<p>SignNetPE of size dim_pe will get appended to each node feature vector.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>dim_emb</code></td>
          <td>
          </td>
          <td><p>Size of final node embedding</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  

  <div class="doc doc-children">











  </div>

  </div>

</div>




  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="goli.nn.pyg_layers.gated_gcn_pyg" class="doc doc-heading">
          <code>goli.nn.pyg_layers.gated_gcn_pyg</code>


<a href="#goli.nn.pyg_layers.gated_gcn_pyg" class="headerlink" title="Permanent link">&para;</a></h3>

  <div class="doc doc-contents first">
  
      <p>Unit tests for the different layers of goli/nn/pyg_layers/...</p>
<p>The layers are not thoroughly tested due to the difficulty of testing them</p>
<p>adapated from <a href="https://github.com/rampasek/GraphGPS/blob/main/graphgps/layer/gps_layer.py">https://github.com/rampasek/GraphGPS/blob/main/graphgps/layer/gps_layer.py</a></p>

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h4 id="goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg" class="doc doc-heading">
        <code>GatedGCNPyg</code>


<a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><span title="torch_geometric.nn.conv.MessagePassing">MessagePassing</span></code>, <code><a class="autorefs autorefs-internal" title="goli.nn.base_graph_layer.BaseGraphStructure" href="#goli.nn.base_graph_layer.BaseGraphStructure">BaseGraphStructure</a></code></p>




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">in_dim_edges</span><span class="p">,</span> <span class="n">out_dim_edges</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span></code>

<a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>ResGatedGCN: Residual Gated Graph ConvNets
An Experimental Study of Neural Networks for Variable Graphs (Xavier Bresson and Thomas Laurent, ICLR 2018)
<a href="https://arxiv.org/pdf/1711.07553v2.pdf">https://arxiv.org/pdf/1711.07553v2.pdf</a></p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>in_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Input feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>out_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Output feature dimensions of the layer, and for the edges</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>in_dim_edges</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Input edge-feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="typing.Callable">Callable</span>, str]</code>
          </td>
          <td><p>activation function to use in the layer</p></td>
          <td>
                <code>&#39;relu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>dropout</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The ratio of units to dropout. Must be between 0 and 1</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>normalization</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Normalization to use. Choices:</p>
<ul>
<li>"none" or <code>None</code>: No normalization</li>
<li>"batch_norm": Batch normalization</li>
<li>"layer_norm": Layer normalization</li>
<li><code>Callable</code>: Any callable function</li>
</ul></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.aggregate" class="doc doc-heading">
<code class="highlight language-python"><span class="n">aggregate</span><span class="p">(</span><span class="n">sigma_ij</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">Bx_j</span><span class="p">,</span> <span class="n">Bx</span><span class="p">)</span></code>

<a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.aggregate" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>sigma_ij        : [n_edges, out_dim]  ; is the output from message() function
index           : [n_edges]
{}x_j           : [n_edges, out_dim]</p>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.layer_inputs_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_inputs_edges</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.layer_inputs_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Return a boolean specifying if the layer type
uses edges as input or not.
It is different from <code>layer_supports_edges</code> since a layer that
supports edges can decide to not use them.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>bool</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Always <code>True</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.layer_outputs_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_outputs_edges</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.layer_outputs_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Abstract method. Return a boolean specifying if the layer type
uses edges as input or not.
It is different from <code>layer_supports_edges</code> since a layer that
supports edges can decide to not use them.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>bool</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Always <code>True</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.layer_supports_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_supports_edges</span><span class="p">()</span></code>

<a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.layer_supports_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Return a boolean specifying if the layer type supports edges or not.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>bool</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Always <code>True</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.message" class="doc doc-heading">
<code class="highlight language-python"><span class="n">message</span><span class="p">(</span><span class="n">Dx_i</span><span class="p">,</span> <span class="n">Ex_j</span><span class="p">,</span> <span class="n">Ce</span><span class="p">)</span></code>

<a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.message" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>{}x_i           : [n_edges, out_dim]
{}x_j           : [n_edges, out_dim]
{}e             : [n_edges, out_dim]</p>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.out_dim_factor" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_dim_factor</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.out_dim_factor" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Get the factor by which the output dimension is multiplied for
the next layer.</p>
<p>For standard layers, this will return <code>1</code>.</p>
<p>But for others, such as <code>GatLayer</code>, the output is the concatenation
of the outputs from each head, so the out_dim gets multiplied by
the number of heads, and this function should return the number
of heads.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>int</code></td>          <td>
                <code>int</code>
          </td>
          <td><p>Always <code>1</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.update" class="doc doc-heading">
<code class="highlight language-python"><span class="n">update</span><span class="p">(</span><span class="n">aggr_out</span><span class="p">,</span> <span class="n">Ax</span><span class="p">)</span></code>

<a href="#goli.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.update" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>aggr_out        : [n_nodes, out_dim] ; is the output from aggregate() function after the aggregation
{}x             : [n_nodes, out_dim]</p>

  </div>

</div>



  </div>

  </div>

</div>




  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="goli.nn.pyg_layers.gin_pyg" class="doc doc-heading">
          <code>goli.nn.pyg_layers.gin_pyg</code>


<a href="#goli.nn.pyg_layers.gin_pyg" class="headerlink" title="Permanent link">&para;</a></h3>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h4 id="goli.nn.pyg_layers.gin_pyg.GINConvPyg" class="doc doc-heading">
        <code>GINConvPyg</code>


<a href="#goli.nn.pyg_layers.gin_pyg.GINConvPyg" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="goli.nn.base_graph_layer.BaseGraphModule" href="#goli.nn.base_graph_layer.BaseGraphModule">BaseGraphModule</a></code></p>




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.gin_pyg.GINConvPyg.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span></code>

<a href="#goli.nn.pyg_layers.gin_pyg.GINConvPyg.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>GIN: Graph Isomorphism Networks
HOW POWERFUL ARE GRAPH NEURAL NETWORKS? (Keyulu Xu, Weihua Hu, Jure Leskovec and Stefanie Jegelka, ICLR 2019)
<a href="https://arxiv.org/pdf/1810.00826.pdf">https://arxiv.org/pdf/1810.00826.pdf</a></p>
<p>[!] code uses the pytorch-geometric implementation of GINConv</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>in_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Input feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>out_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Output feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="typing.Callable">Callable</span>, str]</code>
          </td>
          <td><p>activation function to use in the layer</p></td>
          <td>
                <code>&#39;relu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>dropout</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The ratio of units to dropout. Must be between 0 and 1</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>normalization</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Normalization to use. Choices:</p>
<ul>
<li>"none" or <code>None</code>: No normalization</li>
<li>"batch_norm": Batch normalization</li>
<li>"layer_norm": Layer normalization</li>
<li><code>Callable</code>: Any callable function</li>
</ul></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>init_eps</code></td>
          <td>
          </td>
          <td><p>Initial :math:<code>\epsilon</code> value, default: <code>0</code>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>learn_eps</code></td>
          <td>
          </td>
          <td><p>If True, :math:<code>\epsilon</code> will be a learnable parameter.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.gin_pyg.GINConvPyg.layer_inputs_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_inputs_edges</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.pyg_layers.gin_pyg.GINConvPyg.layer_inputs_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Return a boolean specifying if the layer type
uses edges as input or not.
It is different from <code>layer_supports_edges</code> since a layer that
supports edges can decide to not use them.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>bool</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Always <code>False</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.gin_pyg.GINConvPyg.layer_outputs_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_outputs_edges</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.pyg_layers.gin_pyg.GINConvPyg.layer_outputs_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Abstract method. Return a boolean specifying if the layer type
uses edges as input or not.
It is different from <code>layer_supports_edges</code> since a layer that
supports edges can decide to not use them.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>bool</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Always <code>False</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.gin_pyg.GINConvPyg.layer_supports_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_supports_edges</span><span class="p">()</span></code>

<a href="#goli.nn.pyg_layers.gin_pyg.GINConvPyg.layer_supports_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Return a boolean specifying if the layer type supports edges or not.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>supports_edges</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>bool
Always <code>False</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.gin_pyg.GINConvPyg.out_dim_factor" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_dim_factor</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.pyg_layers.gin_pyg.GINConvPyg.out_dim_factor" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Get the factor by which the output dimension is multiplied for
the next layer.</p>
<p>For standard layers, this will return <code>1</code>.</p>
<p>But for others, such as <code>GatLayer</code>, the output is the concatenation
of the outputs from each head, so the out_dim gets multiplied by
the number of heads, and this function should return the number
of heads.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>int</code></td>          <td>
                <code>int</code>
          </td>
          <td><p>Always <code>1</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="goli.nn.pyg_layers.gin_pyg.GINEConvPyg" class="doc doc-heading">
        <code>GINEConvPyg</code>


<a href="#goli.nn.pyg_layers.gin_pyg.GINEConvPyg" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="goli.nn.base_graph_layer.BaseGraphModule" href="#goli.nn.base_graph_layer.BaseGraphModule">BaseGraphModule</a></code></p>




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.gin_pyg.GINEConvPyg.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">in_dim_edges</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span></code>

<a href="#goli.nn.pyg_layers.gin_pyg.GINEConvPyg.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>GINE: Graph Isomorphism Networks with Edges
Strategies for Pre-training Graph Neural Networks
Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, Jure Leskovec
<a href="https://arxiv.org/abs/1905.12265">https://arxiv.org/abs/1905.12265</a></p>
<p>[!] code uses the pytorch-geometric implementation of GINEConv</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>in_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Input feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>out_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Output feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="typing.Callable">Callable</span>, str]</code>
          </td>
          <td><p>activation function to use in the layer</p></td>
          <td>
                <code>&#39;relu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>dropout</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The ratio of units to dropout. Must be between 0 and 1</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>normalization</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Normalization to use. Choices:</p>
<ul>
<li>"none" or <code>None</code>: No normalization</li>
<li>"batch_norm": Batch normalization</li>
<li>"layer_norm": Layer normalization</li>
<li><code>Callable</code>: Any callable function</li>
</ul></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>init_eps</code></td>
          <td>
          </td>
          <td><p>Initial :math:<code>\epsilon</code> value, default: <code>0</code>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>learn_eps</code></td>
          <td>
          </td>
          <td><p>If True, :math:<code>\epsilon</code> will be a learnable parameter.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.gin_pyg.GINEConvPyg.layer_inputs_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_inputs_edges</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.pyg_layers.gin_pyg.GINEConvPyg.layer_inputs_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Return a boolean specifying if the layer type
uses edges as input or not.
It is different from <code>layer_supports_edges</code> since a layer that
supports edges can decide to not use them.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>bool</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Always <code>True</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.gin_pyg.GINEConvPyg.layer_outputs_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_outputs_edges</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.pyg_layers.gin_pyg.GINEConvPyg.layer_outputs_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Abstract method. Return a boolean specifying if the layer type
uses edges as input or not.
It is different from <code>layer_supports_edges</code> since a layer that
supports edges can decide to not use them.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>bool</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Always <code>False</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.gin_pyg.GINEConvPyg.layer_supports_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_supports_edges</span><span class="p">()</span></code>

<a href="#goli.nn.pyg_layers.gin_pyg.GINEConvPyg.layer_supports_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Return a boolean specifying if the layer type supports edges or not.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>supports_edges</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>bool
Always <code>True</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.gin_pyg.GINEConvPyg.out_dim_factor" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_dim_factor</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.pyg_layers.gin_pyg.GINEConvPyg.out_dim_factor" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Get the factor by which the output dimension is multiplied for
the next layer.</p>
<p>For standard layers, this will return <code>1</code>.</p>
<p>But for others, such as <code>GatLayer</code>, the output is the concatenation
of the outputs from each head, so the out_dim gets multiplied by
the number of heads, and this function should return the number
of heads.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>int</code></td>          <td>
                <code>int</code>
          </td>
          <td><p>Always <code>1</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>




  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="goli.nn.pyg_layers.gps_pyg" class="doc doc-heading">
          <code>goli.nn.pyg_layers.gps_pyg</code>


<a href="#goli.nn.pyg_layers.gps_pyg" class="headerlink" title="Permanent link">&para;</a></h3>

  <div class="doc doc-contents first">
  
      <p>adapated from <a href="https://github.com/rampasek/GraphGPS/blob/main/graphgps/layer/gps_layer.py">https://github.com/rampasek/GraphGPS/blob/main/graphgps/layer/gps_layer.py</a></p>

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h4 id="goli.nn.pyg_layers.gps_pyg.GPSLayerPyg" class="doc doc-heading">
        <code>GPSLayerPyg</code>


<a href="#goli.nn.pyg_layers.gps_pyg.GPSLayerPyg" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="goli.nn.base_graph_layer.BaseGraphModule" href="#goli.nn.base_graph_layer.BaseGraphModule">BaseGraphModule</a></code></p>




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.gps_pyg.GPSLayerPyg.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">in_dim_edges</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">mpnn_type</span><span class="o">=</span><span class="s1">&#39;pyg:gine&#39;</span><span class="p">,</span> <span class="n">mpnn_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attn_type</span><span class="o">=</span><span class="s1">&#39;full-attention&#39;</span><span class="p">,</span> <span class="n">attn_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#goli.nn.pyg_layers.gps_pyg.GPSLayerPyg.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>GINE: Graph Isomorphism Networks with Edges
Strategies for Pre-training Graph Neural Networks
Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, Jure Leskovec
<a href="https://arxiv.org/abs/1905.12265">https://arxiv.org/abs/1905.12265</a></p>
<p>[!] code uses the pytorch-geometric implementation of GINEConv</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>in_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Input feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>out_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Output feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="typing.Callable">Callable</span>, str]</code>
          </td>
          <td><p>activation function to use in the layer</p></td>
          <td>
                <code>&#39;relu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>dropout</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The ratio of units to dropout. Must be between 0 and 1</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>normalization</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Normalization to use. Choices:</p>
<ul>
<li>"none" or <code>None</code>: No normalization</li>
<li>"batch_norm": Batch normalization</li>
<li>"layer_norm": Layer normalization</li>
<li><code>Callable</code>: Any callable function</li>
</ul></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.gps_pyg.GPSLayerPyg.layer_inputs_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_inputs_edges</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.pyg_layers.gps_pyg.GPSLayerPyg.layer_inputs_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Return a boolean specifying if the layer type
uses edges as input or not.
It is different from <code>layer_supports_edges</code> since a layer that
supports edges can decide to not use them.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>bool</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Always <code>True</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.gps_pyg.GPSLayerPyg.layer_outputs_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_outputs_edges</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.pyg_layers.gps_pyg.GPSLayerPyg.layer_outputs_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Abstract method. Return a boolean specifying if the layer type
uses edges as input or not.
It is different from <code>layer_supports_edges</code> since a layer that
supports edges can decide to not use them.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>bool</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Always <code>False</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.gps_pyg.GPSLayerPyg.layer_supports_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_supports_edges</span><span class="p">()</span></code>

<a href="#goli.nn.pyg_layers.gps_pyg.GPSLayerPyg.layer_supports_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Return a boolean specifying if the layer type supports edges or not.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>supports_edges</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>bool
Always <code>True</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.gps_pyg.GPSLayerPyg.out_dim_factor" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_dim_factor</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.pyg_layers.gps_pyg.GPSLayerPyg.out_dim_factor" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Get the factor by which the output dimension is multiplied for
the next layer.</p>
<p>For standard layers, this will return <code>1</code>.</p>
<p>But for others, such as <code>GatLayer</code>, the output is the concatenation
of the outputs from each head, so the out_dim gets multiplied by
the number of heads, and this function should return the number
of heads.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>int</code></td>          <td>
                <code>int</code>
          </td>
          <td><p>Always <code>1</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>




  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="goli.nn.pyg_layers.pna_pyg" class="doc doc-heading">
          <code>goli.nn.pyg_layers.pna_pyg</code>


<a href="#goli.nn.pyg_layers.pna_pyg" class="headerlink" title="Permanent link">&para;</a></h3>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h4 id="goli.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg" class="doc doc-heading">
        <code>PNAMessagePassingPyg</code>


<a href="#goli.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><span title="torch_geometric.nn.conv.MessagePassing">MessagePassing</span></code>, <code><a class="autorefs autorefs-internal" title="goli.nn.base_graph_layer.BaseGraphStructure" href="#goli.nn.base_graph_layer.BaseGraphStructure">BaseGraphStructure</a></code></p>

  
      <p>Implementation of the message passing architecture of the PNA message passing layer,
previously known as <code>PNALayerComplex</code>. This layer applies an MLP as
pretransformation to the concatenation of <span class="arithmatex">\([h_u, h_v, e_{uv}]\)</span> to generate
the messages, with <span class="arithmatex">\(h_u\)</span> the node feature, <span class="arithmatex">\(h_v\)</span> the neighbour node features,
and <span class="arithmatex">\(e_{uv}\)</span> the edge feature between the nodes <span class="arithmatex">\(u\)</span> and <span class="arithmatex">\(v\)</span>.</p>
<p>After the pre-transformation, it aggregates the messages
multiple aggregators and scalers,
concatenates their results, then applies an MLP on the concatenated
features.</p>
<p>PNA: Principal Neighbourhood Aggregation
Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Lio, Petar Velickovic
<a href="https://arxiv.org/abs/2004.05718">https://arxiv.org/abs/2004.05718</a></p>
<p>[!] code adapted from pytorch-geometric implementation of PNAConv</p>



  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">aggregators</span><span class="p">,</span> <span class="n">scalers</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">avg_d</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;log&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;lin&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">},</span> <span class="n">last_activation</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">posttrans_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pretrans_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">in_dim_edges</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code>

<a href="#goli.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>in_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Input feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>out_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Output feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>aggregators</code></td>
          <td>
                <code><span title="typing.List">List</span>[str]</code>
          </td>
          <td><p>Set of aggregation function identifiers,
e.g. "mean", "max", "min", "std", "sum", "var", "moment3".
The results from all aggregators will be concatenated.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>scalers</code></td>
          <td>
                <code><span title="typing.List">List</span>[str]</code>
          </td>
          <td><p>Set of scaling functions identifiers
e.g. "identidy", "amplification", "attenuation"
The results from all scalers will be concatenated</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="typing.Callable">Callable</span>, str]</code>
          </td>
          <td><p>activation function to use in the layer</p></td>
          <td>
                <code>&#39;relu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>dropout</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The ratio of units to dropout. Must be between 0 and 1</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>normalization</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Normalization to use. Choices:</p>
<ul>
<li>"none" or <code>None</code>: No normalization</li>
<li>"batch_norm": Batch normalization</li>
<li>"layer_norm": Layer normalization</li>
<li><code>Callable</code>: Any callable function</li>
</ul></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>avg_d</code></td>
          <td>
                <code><span title="typing.Dict">Dict</span>[str, float]</code>
          </td>
          <td><p>Average degree of nodes in the training set, used by scalers to normalize</p></td>
          <td>
                <code>{&#39;log&#39;: 1.0, &#39;lin&#39;: 1.0}</code>
          </td>
        </tr>
        <tr>
          <td><code>last_activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="typing.Callable">Callable</span>, str]</code>
          </td>
          <td><p>activation function to use in the last layer of the internal MLP</p></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>posttrans_layers</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>number of layers in the MLP transformation after the aggregation</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>pretrans_layers</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>number of layers in the transformation before the aggregation</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>in_dim_edges</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>size of the edge features. If 0, edges are ignored</p></td>
          <td>
                <code>0</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.layer_inputs_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_inputs_edges</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.layer_inputs_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Return a boolean specifying if the layer type
uses edges as input or not.
It is different from <code>layer_supports_edges</code> since a layer that
supports edges can decide to not use them.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>bool</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Returns <code>self.edge_features</code></p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.layer_outputs_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_outputs_edges</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.layer_outputs_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Abstract method. Return a boolean specifying if the layer type
uses edges as input or not.
It is different from <code>layer_supports_edges</code> since a layer that
supports edges can decide to not use them.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>bool</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Always <code>False</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.layer_supports_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_supports_edges</span><span class="p">()</span></code>

<a href="#goli.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.layer_supports_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Return a boolean specifying if the layer type supports edges or not.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>bool</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Always <code>True</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.out_dim_factor" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_dim_factor</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.out_dim_factor" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Get the factor by which the output dimension is multiplied for
the next layer.</p>
<p>For standard layers, this will return <code>1</code>.</p>
<p>But for others, such as <code>GatLayer</code>, the output is the concatenation
of the outputs from each head, so the out_dim gets multiplied by
the number of heads, and this function should return the number
of heads.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>int</code></td>          <td>
                <code>int</code>
          </td>
          <td><p>Always <code>1</code> for the current class</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>




  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="goli.nn.pyg_layers.pooling_pyg" class="doc doc-heading">
          <code>goli.nn.pyg_layers.pooling_pyg</code>


<a href="#goli.nn.pyg_layers.pooling_pyg" class="headerlink" title="Permanent link">&para;</a></h3>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h4 id="goli.nn.pyg_layers.pooling_pyg.VirtualNodePyg" class="doc doc-heading">
        <code>VirtualNodePyg</code>


<a href="#goli.nn.pyg_layers.pooling_pyg.VirtualNodePyg" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.pooling_pyg.VirtualNodePyg.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">vn_type</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">residual</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#goli.nn.pyg_layers.pooling_pyg.VirtualNodePyg.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>The VirtualNode is a layer that pool the features of the graph,
applies a neural network layer on the pooled features,
then add the result back to the node features of every node.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Input and output feature dimensions of the virtual node layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>vn_type</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[type(None), str]</code>
          </td>
          <td><p>The type of the virtual node. Choices are:</p>
<ul>
<li>"none": No pooling</li>
<li>"sum": Sum all the nodes for each graph</li>
<li>"mean": Mean all the nodes for each graph</li>
<li>"logsum": Mean all the nodes then multiply by log(num_nodes) for each graph</li>
<li>"max": Max all the nodes for each graph</li>
<li>"min": Min all the nodes for each graph</li>
<li>"std": Standard deviation of all the nodes for each graph</li>
</ul></td>
          <td>
                <code>&#39;sum&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>activation function to use in the neural network layer.</p></td>
          <td>
                <code>&#39;relu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>dropout</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The ratio of units to dropout. Must be between 0 and 1</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>normalization</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Normalization to use. Choices:</p>
<ul>
<li>"none" or <code>None</code>: No normalization</li>
<li>"batch_norm": Batch normalization</li>
<li>"layer_norm": Layer normalization</li>
<li><code>Callable</code>: Any callable function</li>
</ul></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>bias</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to add a bias to the neural network</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
        <tr>
          <td><code>residual</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether all virtual nodes should be connected together
via a residual connection</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.pyg_layers.pooling_pyg.VirtualNodePyg.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">vn_h</span><span class="p">)</span></code>

<a href="#goli.nn.pyg_layers.pooling_pyg.VirtualNodePyg.forward" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Apply the virtual node layer.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>g</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="torch_geometric.data.Data">Data</span>, <span title="torch_geometric.data.Batch">Batch</span>]</code>
          </td>
          <td><p>PyG Graphs or Batched graphs.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>h</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span>[..., N, Din]</code>
          </td>
          <td><p>Node feature tensor, before convolution.
<code>N</code> is the number of nodes, <code>Din</code> is the input features</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>vn_h</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span>[..., M, Din]</code>
          </td>
          <td><p>Graph feature of the previous virtual node, or <code>None</code>
<code>M</code> is the number of graphs, <code>Din</code> is the input features.
It is added to the result after the MLP, as a residual connection</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>h = torch.Tensor[..., N, Dout]</code>:
Node feature tensor, after convolution and residual.
<code>N</code> is the number of nodes, <code>Dout</code> is the output features of the layer and residual</p></td>
        </tr>
        <tr>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>vn_h = torch.Tensor[..., M, Dout]</code>:
Graph feature tensor to be used at the next virtual node, or <code>None</code>
<code>M</code> is the number of graphs, <code>Dout</code> is the output features</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>


<div class="doc doc-object doc-function">



<h4 id="goli.nn.pyg_layers.pooling_pyg.parse_pooling_layer_pyg" class="doc doc-heading">
<code class="highlight language-python"><span class="n">parse_pooling_layer_pyg</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">pooling</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#goli.nn.pyg_layers.pooling_pyg.parse_pooling_layer_pyg" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Select the pooling layers from a list of strings, and put them
in a Module that concatenates their outputs.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>in_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The dimension at the input layer of the pooling</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>pooling</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.List">List</span>[str]]</code>
          </td>
          <td><p>The list of pooling layers to use. The accepted strings are:</p>
<ul>
<li>"none": No pooling</li>
<li>"sum": Sum all the nodes for each graph</li>
<li>"mean": Mean all the nodes for each graph</li>
<li>"logsum": Mean all the nodes then multiply by log(num_nodes) for each graph</li>
<li>"max": Max all the nodes for each graph</li>
<li>"min": Min all the nodes for each graph</li>
<li>"std": Standard deviation of all the nodes for each graph</li>
</ul></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="goli.nn.pyg_layers.pooling_pyg.scatter_logsum_pool" class="doc doc-heading">
<code class="highlight language-python"><span class="n">scatter_logsum_pool</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim_size</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#goli.nn.pyg_layers.pooling_pyg.scatter_logsum_pool" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Apply pooling over the nodes in the graph using a mean aggregation,
but scaled by the log of the number of nodes. This gives the same
expressive power as the sum, but helps deal with graphs that are
significantly larger than others by using a logarithmic scale.</p>
<div class="arithmatex">\[r^{(i)} = \frac{\log N_i}{N_i}\sum_{k=1}^{N_i} x^{(i)}_k\]</div>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>Node feature matrix
:math:<code>\mathbf{X} \in \mathbb{R}^{(N_1 + \ldots + N_B) \times F}</code>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>batch</code></td>
          <td>
                <code><span title="torch.LongTensor">LongTensor</span></code>
          </td>
          <td><p>Batch vector :math:<code>\mathbf{b} \in {\{ 0, \ldots,
B-1\}}^N</code>, which assigns each node to a specific example.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>size</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Batch-size :math:<code>B</code>.
Automatically calculated if not given. (default: :obj:<code>None</code>)</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>
      <p>:rtype: :class:<code>Tensor</code></p>

  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="goli.nn.pyg_layers.pooling_pyg.scatter_std_pool" class="doc doc-heading">
<code class="highlight language-python"><span class="n">scatter_std_pool</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim_size</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#goli.nn.pyg_layers.pooling_pyg.scatter_std_pool" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Returns batch-wise graph-level-outputs by taking the channel-wise
minimum across the node dimension, so that for a single graph
:math:<code>\mathcal{G}_i</code> its output is computed by</p>
<p>.. math::
    \mathbf{r}<em>i = \mathrm{max}</em>{n=1}^{N_i} \, \mathbf{x}_n</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>Node feature matrix
:math:<code>\mathbf{X} \in \mathbb{R}^{(N_1 + \ldots + N_B) \times F}</code>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>batch</code></td>
          <td>
                <code><span title="torch.LongTensor">LongTensor</span></code>
          </td>
          <td><p>Batch vector :math:<code>\mathbf{b} \in {\{ 0, \ldots,
B-1\}}^N</code>, which assigns each node to a specific example.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>size</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Batch-size :math:<code>B</code>.
Automatically calculated if not given. (default: :obj:<code>None</code>)</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>
      <p>:rtype: :class:<code>Tensor</code></p>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="goli.nn.base_graph_layer" class="doc doc-heading">
          <code>goli.nn.base_graph_layer</code>


<a href="#goli.nn.base_graph_layer" class="headerlink" title="Permanent link">&para;</a></h3>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h4 id="goli.nn.base_graph_layer.BaseGraphModule" class="doc doc-heading">
        <code>BaseGraphModule</code>


<a href="#goli.nn.base_graph_layer.BaseGraphModule" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="goli.nn.base_graph_layer.BaseGraphStructure" href="#goli.nn.base_graph_layer.BaseGraphStructure">BaseGraphStructure</a></code>, <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.base_graph_layer.BaseGraphModule.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span></code>

<a href="#goli.nn.base_graph_layer.BaseGraphModule.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Abstract class used to standardize the implementation of DGL layers
in the current library. It will allow a network to seemlesly swap between
different GNN layers by better understanding the expected inputs
and outputs.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>in_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Input feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>out_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Output feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>activation function to use in the layer</p></td>
          <td>
                <code>&#39;relu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>dropout</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The ratio of units to dropout. Must be between 0 and 1</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>normalization</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Normalization to use. Choices:</p>
<ul>
<li>"none" or <code>None</code>: No normalization</li>
<li>"batch_norm": Batch normalization</li>
<li>"layer_norm": Layer normalization</li>
<li><code>Callable</code>: Any callable function</li>
</ul></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="goli.nn.base_graph_layer.BaseGraphStructure" class="doc doc-heading">
        <code>BaseGraphStructure</code>


<a href="#goli.nn.base_graph_layer.BaseGraphStructure" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.base_graph_layer.BaseGraphStructure.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span></code>

<a href="#goli.nn.base_graph_layer.BaseGraphStructure.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Abstract class used to standardize the implementation of DGL layers
in the current library. It will allow a network to seemlesly swap between
different GNN layers by better understanding the expected inputs
and outputs.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>in_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Input feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>out_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Output feature dimensions of the layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>activation function to use in the layer</p></td>
          <td>
                <code>&#39;relu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>dropout</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The ratio of units to dropout. Must be between 0 and 1</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>normalization</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Normalization to use. Choices:</p>
<ul>
<li>"none" or <code>None</code>: No normalization</li>
<li>"batch_norm": Batch normalization</li>
<li>"layer_norm": Layer normalization</li>
<li><code>Callable</code>: Any callable function</li>
</ul></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.base_graph_layer.BaseGraphStructure.__repr__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__repr__</span><span class="p">()</span></code>

<a href="#goli.nn.base_graph_layer.BaseGraphStructure.__repr__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Controls how the class is printed</p>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.base_graph_layer.BaseGraphStructure.apply_norm_activation_dropout" class="doc doc-heading">
<code class="highlight language-python"><span class="n">apply_norm_activation_dropout</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#goli.nn.base_graph_layer.BaseGraphStructure.apply_norm_activation_dropout" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Apply the different normalization and the dropout to the
output layer.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>h</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>Feature tensor, to be normalized</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>normalization</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to apply the normalization</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
        <tr>
          <td><code>activation</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to apply the activation layer</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
        <tr>
          <td><code>dropout</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to apply the dropout layer</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>h</code></td>          <td>
          </td>
          <td><p>Normalized and dropped-out features</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.base_graph_layer.BaseGraphStructure.layer_inputs_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_inputs_edges</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.base_graph_layer.BaseGraphStructure.layer_inputs_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Abstract method. Return a boolean specifying if the layer type
uses edges as input or not.
It is different from <code>layer_supports_input_edges</code> since a layer that
supports edges can decide to not use them.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>bool</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Whether the layer uses input edges in the forward pass</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.base_graph_layer.BaseGraphStructure.layer_outputs_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_outputs_edges</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.base_graph_layer.BaseGraphStructure.layer_outputs_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Abstract method. Return a boolean specifying if the layer type
uses edges as input or not.
It is different from <code>layer_supports_output_edges</code> since a layer that
supports edges can decide to not use them.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>bool</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Whether the layer outputs edges in the forward pass</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.base_graph_layer.BaseGraphStructure.layer_supports_edges" class="doc doc-heading">
<code class="highlight language-python"><span class="n">layer_supports_edges</span><span class="p">()</span></code>

<a href="#goli.nn.base_graph_layer.BaseGraphStructure.layer_supports_edges" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Abstract method. Return a boolean specifying if the layer type
supports output edges edges or not.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>bool</code></td>          <td>
                <code>bool</code>
          </td>
          <td><p>Whether the layer supports the use of edges</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.base_graph_layer.BaseGraphStructure.out_dim_factor" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_dim_factor</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.base_graph_layer.BaseGraphStructure.out_dim_factor" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Abstract method.
Get the factor by which the output dimension is multiplied for
the next layer.</p>
<p>For standard layers, this will return <code>1</code>.</p>
<p>But for others, such as <code>GatLayer</code>, the output is the concatenation
of the outputs from each head, so the out_dim gets multiplied by
the number of heads, and this function should return the number
of heads.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>int</code></td>          <td>
                <code>int</code>
          </td>
          <td><p>The factor that multiplies the output dimensions</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>


<div class="doc doc-object doc-function">



<h4 id="goli.nn.base_graph_layer.check_intpus_allow_int" class="doc doc-heading">
<code class="highlight language-python"><span class="n">check_intpus_allow_int</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span></code>

<a href="#goli.nn.base_graph_layer.check_intpus_allow_int" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Overwrite the <strong>check_input</strong> to allow for int32 and int16
TODO: Remove when PyG and pytorch supports int32.</p>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="goli.nn.base_layers" class="doc doc-heading">
          <code>goli.nn.base_layers</code>


<a href="#goli.nn.base_layers" class="headerlink" title="Permanent link">&para;</a></h3>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h4 id="goli.nn.base_layers.FCLayer" class="doc doc-heading">
        <code>FCLayer</code>


<a href="#goli.nn.base_layers.FCLayer" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.base_layers.FCLayer.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">init_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#goli.nn.base_layers.FCLayer.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>A simple fully connected and customizable layer. This layer is centered around a torch.nn.Linear module.
The order in which transformations are applied is:</p>
<ul>
<li>Dense Layer</li>
<li>Activation</li>
<li>Dropout (if applicable)</li>
<li>Batch Normalization (if applicable)</li>
</ul>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>in_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Input dimension of the layer (the torch.nn.Linear)</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>out_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Output dimension of the layer.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>dropout</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The ratio of units to dropout. No dropout by default.</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Activation function to use.</p></td>
          <td>
                <code>&#39;relu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>normalization</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Normalization to use. Choices:</p>
<ul>
<li>"none" or <code>None</code>: No normalization</li>
<li>"batch_norm": Batch normalization</li>
<li>"layer_norm": Layer normalization</li>
<li><code>Callable</code>: Any callable function</li>
</ul></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>bias</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to enable bias in for the linear layer.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
        <tr>
          <td><code>init_fn</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Initialization function to use for the weight of the layer. Default is
<span class="arithmatex">\(<span class="arithmatex">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span>\)</span> with <span class="arithmatex">\(<span class="arithmatex">\(k=\frac{1}{ \text{in_dim}}\)</span>\)</span></p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Attributes:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>dropout</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The ratio of units to dropout.</p></td>
        </tr>
        <tr>
          <td><code>normalization</code></td>
          <td>
                <code>None or Callable</code>
          </td>
          <td><p>Normalization layer</p></td>
        </tr>
        <tr>
          <td><code>linear</code></td>
          <td>
                <code>torch.<span title="torch.nn">nn</span>.<span title="torch.nn.Linear">Linear</span></code>
          </td>
          <td><p>The linear layer</p></td>
        </tr>
        <tr>
          <td><code>activation</code></td>
          <td>
                <code>torch.<span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>
          </td>
          <td><p>The activation layer</p></td>
        </tr>
        <tr>
          <td><code>init_fn</code></td>
          <td>
                <code><span title="typing.Callable">Callable</span></code>
          </td>
          <td><p>Initialization function used for the weight of the layer</p></td>
        </tr>
        <tr>
          <td><code>in_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Input dimension of the linear layer</p></td>
        </tr>
        <tr>
          <td><code>out_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Output dimension of the linear layer</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.base_layers.FCLayer.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">h</span><span class="p">)</span></code>

<a href="#goli.nn.base_layers.FCLayer.forward" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Apply the FC layer on the input features.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>h</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>torch.Tensor[..., Din]</code>:
Input feature tensor, before the FC.
<code>Din</code> is the number of input features</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>torch.Tensor[..., Dout]</code>:
Output feature tensor, after the FC.
<code>Dout</code> is the number of output features</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.base_layers.FCLayer.in_channels" class="doc doc-heading">
<code class="highlight language-python"><span class="n">in_channels</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.base_layers.FCLayer.in_channels" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Get the input channel size. For compatibility with PyG.</p>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.base_layers.FCLayer.out_channels" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_channels</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#goli.nn.base_layers.FCLayer.out_channels" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Get the output channel size. For compatibility with PyG.</p>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="goli.nn.base_layers.GRU" class="doc doc-heading">
        <code>GRU</code>


<a href="#goli.nn.base_layers.GRU" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.base_layers.GRU.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span></code>

<a href="#goli.nn.base_layers.GRU.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Wrapper class for the GRU used by the GNN framework, nn.GRU is used for the Gated Recurrent Unit itself</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>in_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Input dimension of the GRU layer</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>hidden_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Hidden dimension of the GRU layer.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.base_layers.GRU.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></code>

<a href="#goli.nn.base_layers.GRU.forward" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x</code></td>
          <td>
          </td>
          <td><p><code>torch.Tensor[B, N, Din]</code>
where Din &lt;= in_dim (difference is padded)</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td>
          </td>
          <td><p><code>torch.Tensor[B, N, Dh]</code>
where Dh &lt;= hidden_dim (difference is padded)</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td><p>torch.Tensor: <code>torch.Tensor[B, N, Dh]</code></p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="goli.nn.base_layers.MLP" class="doc doc-heading">
        <code>MLP</code>


<a href="#goli.nn.base_layers.MLP" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.base_layers.MLP.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">last_activation</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">last_normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">first_normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">last_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span></code>

<a href="#goli.nn.base_layers.MLP.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Simple multi-layer perceptron, built of a series of FCLayers</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>in_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Input dimension of the MLP</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>hidden_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Hidden dimension of the MLP. All hidden dimensions will have
the same number of parameters</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>out_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Output dimension of the MLP.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>layers</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of hidden layers</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Activation function to use in all the layers except the last.
if <code>layers==1</code>, this parameter is ignored</p></td>
          <td>
                <code>&#39;relu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>last_activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Activation function to use in the last layer.</p></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>dropout</code></td>
          <td>
          </td>
          <td><p>The ratio of units to dropout. Must be between 0 and 1</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>normalization</code></td>
          <td>
          </td>
          <td><p>Normalization to use. Choices:</p>
<ul>
<li>"none" or <code>None</code>: No normalization</li>
<li>"batch_norm": Batch normalization</li>
<li>"layer_norm": Layer normalization in the hidden layers.</li>
<li><code>Callable</code>: Any callable function</li>
</ul>
<p>if <code>layers==1</code>, this parameter is ignored</p></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>last_normalization</code></td>
          <td>
          </td>
          <td><p>Norrmalization to use <strong>after the last layer</strong>. Same options as <code>normalization</code>.</p></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>first_normalization</code></td>
          <td>
          </td>
          <td><p>Norrmalization to use in <strong>before the first layer</strong>. Same options as <code>normalization</code>.</p></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>last_dropout</code></td>
          <td>
          </td>
          <td><p>The ratio of units to dropout at the last layer.</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.base_layers.MLP.__repr__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__repr__</span><span class="p">()</span></code>

<a href="#goli.nn.base_layers.MLP.__repr__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Controls how the class is printed</p>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.base_layers.MLP.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">h</span><span class="p">)</span></code>

<a href="#goli.nn.base_layers.MLP.forward" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Apply the MLP on the input features.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>h</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>torch.Tensor[..., Din]</code>:
Input feature tensor, before the MLP.
<code>Din</code> is the number of input features</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>torch.Tensor[..., Dout]</code>:
Output feature tensor, after the MLP.
<code>Dout</code> is the number of output features</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>


<div class="doc doc-object doc-function">



<h4 id="goli.nn.base_layers.get_activation" class="doc doc-heading">
<code class="highlight language-python"><span class="n">get_activation</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span></code>

<a href="#goli.nn.base_layers.get_activation" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>returns the activation function represented by the input string</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[type(None), str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Callable, <code>None</code>, or string with value:
"none", "ReLU", "Sigmoid", "Tanh", "ELU", "SELU", "GLU", "LeakyReLU", "Softplus"</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Callable or None: The activation function</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="goli.nn.base_layers.get_norm" class="doc doc-heading">
<code class="highlight language-python"><span class="n">get_norm</span><span class="p">(</span><span class="n">normalization</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#goli.nn.base_layers.get_norm" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>returns the normalization function represented by the input string</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>normalization</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="typing.Type">Type</span>[None], str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Callable, <code>None</code>, or string with value:
"none", "batch_norm", "layer_norm"</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>dim</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[int]</code>
          </td>
          <td><p>Dimension where to apply the norm. Mandatory for 'batch_norm' and 'layer_norm'</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td><p>Callable or None: The normalization function</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="goli.nn.residual_connections" class="doc doc-heading">
          <code>goli.nn.residual_connections</code>


<a href="#goli.nn.residual_connections" class="headerlink" title="Permanent link">&para;</a></h3>

  <div class="doc doc-contents first">
  
      <p>Different types of residual connections, including None, Simple (ResNet-like),
Concat and DenseNet</p>

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h4 id="goli.nn.residual_connections.ResidualConnectionBase" class="doc doc-heading">
        <code>ResidualConnectionBase</code>


<a href="#goli.nn.residual_connections.ResidualConnectionBase" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionBase.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">skip_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

<a href="#goli.nn.residual_connections.ResidualConnectionBase.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Abstract class for the residual connections. Using this class,
we implement different types of residual connections, such as
the ResNet, weighted-ResNet, skip-concat and DensNet.</p>
<p>The following methods must be implemented in a children class</p>
<ul>
<li><code>h_dim_increase_type()</code></li>
<li><code>has_weights()</code></li>
</ul>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>skip_steps</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>int
The number of steps to skip between the residual connections.
If <code>1</code>, all the layers are connected. If <code>2</code>, half of the
layers are connected.</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionBase.__repr__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__repr__</span><span class="p">()</span></code>

<a href="#goli.nn.residual_connections.ResidualConnectionBase.__repr__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Controls how the class is printed</p>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionBase.h_dim_increase_type" class="doc doc-heading">
<code class="highlight language-python"><span class="n">h_dim_increase_type</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

<a href="#goli.nn.residual_connections.ResidualConnectionBase.h_dim_increase_type" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>How does the dimension of the output features increases after each layer?</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>h_dim_increase_type</code></td>          <td>
          </td>
          <td><p>None or str
- <code>None</code>: The dimension of the output features do not change at each layer.
E.g. ResNet.</p>
<ul>
<li>
<p>"previous": The dimension of the output features is the concatenation of
the previous layer with the new layer.</p>
</li>
<li>
<p>"cumulative": The dimension of the output features is the concatenation
of all previous layers.</p>
</li>
</ul></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionBase.has_weights" class="doc doc-heading">
<code class="highlight language-python"><span class="n">has_weights</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

<a href="#goli.nn.residual_connections.ResidualConnectionBase.has_weights" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>has_weights</code></td>          <td>
          </td>
          <td><p>bool
Whether the residual connection uses weights</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="goli.nn.residual_connections.ResidualConnectionConcat" class="doc doc-heading">
        <code>ResidualConnectionConcat</code>


<a href="#goli.nn.residual_connections.ResidualConnectionConcat" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="goli.nn.residual_connections.ResidualConnectionBase" href="#goli.nn.residual_connections.ResidualConnectionBase">ResidualConnectionBase</a></code></p>




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionConcat.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">skip_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

<a href="#goli.nn.residual_connections.ResidualConnectionConcat.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Class for the simple residual connections proposed but where
the skip connection features are concatenated to the current
layer features.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>skip_steps</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>int
The number of steps to skip between the residual connections.
If <code>1</code>, all the layers are connected. If <code>2</code>, half of the
layers are connected.</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionConcat.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h_prev</span><span class="p">,</span> <span class="n">step_idx</span><span class="p">)</span></code>

<a href="#goli.nn.residual_connections.ResidualConnectionConcat.forward" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Concatenate <code>h</code> with the previous layers with skip connection <code>h_prev</code>.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>h</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>torch.Tensor(..., m)
The current layer features</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>h_prev</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>torch.Tensor(..., n), None
The features from the previous layer with a skip connection.
Usually, we have <code>n</code> equal to <code>m</code>.
At <code>step_idx==0</code>, <code>h_prev</code> can be set to <code>None</code>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>step_idx</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>int
Current layer index or step index in the forward loop of the architecture.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>h</code></td>          <td>
          </td>
          <td><p>torch.Tensor(..., m) or torch.Tensor(..., m + n)
Either return <code>h</code> unchanged, or the concatenation
with <code>h_prev</code>, depending on the <code>step_idx</code> and <code>self.skip_steps</code>.</p></td>
        </tr>
        <tr>
<td><code>h_prev</code></td>          <td>
          </td>
          <td><p>torch.Tensor(..., m) or torch.Tensor(..., m + n)
Either return <code>h_prev</code> unchanged, or the same value as <code>h</code>,
depending on the <code>step_idx</code> and <code>self.skip_steps</code>.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionConcat.h_dim_increase_type" class="doc doc-heading">
<code class="highlight language-python"><span class="n">h_dim_increase_type</span><span class="p">()</span></code>

<a href="#goli.nn.residual_connections.ResidualConnectionConcat.h_dim_increase_type" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td><p>"previous":
The dimension of the output layer is the concatenation with the previous layer.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionConcat.has_weights" class="doc doc-heading">
<code class="highlight language-python"><span class="n">has_weights</span><span class="p">()</span></code>

<a href="#goli.nn.residual_connections.ResidualConnectionConcat.has_weights" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td><p>False
The current class does not use weights</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="goli.nn.residual_connections.ResidualConnectionDenseNet" class="doc doc-heading">
        <code>ResidualConnectionDenseNet</code>


<a href="#goli.nn.residual_connections.ResidualConnectionDenseNet" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="goli.nn.residual_connections.ResidualConnectionBase" href="#goli.nn.residual_connections.ResidualConnectionBase">ResidualConnectionBase</a></code></p>




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionDenseNet.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">skip_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

<a href="#goli.nn.residual_connections.ResidualConnectionDenseNet.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Class for the residual connections proposed by DenseNet, where
all previous skip connection features are concatenated to the current
layer features.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>skip_steps</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>int
The number of steps to skip between the residual connections.
If <code>1</code>, all the layers are connected. If <code>2</code>, half of the
layers are connected.</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionDenseNet.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h_prev</span><span class="p">,</span> <span class="n">step_idx</span><span class="p">)</span></code>

<a href="#goli.nn.residual_connections.ResidualConnectionDenseNet.forward" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Concatenate <code>h</code> with all the previous layers with skip connection <code>h_prev</code>.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>h</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>torch.Tensor(..., m)
The current layer features</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>h_prev</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>torch.Tensor(..., n), None
The features from the previous layers.
n = ((step_idx // self.skip_steps) + 1) * m</p>
<p>At <code>step_idx==0</code>, <code>h_prev</code> can be set to <code>None</code>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>step_idx</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>int
Current layer index or step index in the forward loop of the architecture.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>h</code></td>          <td>
          </td>
          <td><p>torch.Tensor(..., m) or torch.Tensor(..., m + n)
Either return <code>h</code> unchanged, or the concatenation
with <code>h_prev</code>, depending on the <code>step_idx</code> and <code>self.skip_steps</code>.</p></td>
        </tr>
        <tr>
<td><code>h_prev</code></td>          <td>
          </td>
          <td><p>torch.Tensor(..., m) or torch.Tensor(..., m + n)
Either return <code>h_prev</code> unchanged, or the same value as <code>h</code>,
depending on the <code>step_idx</code> and <code>self.skip_steps</code>.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionDenseNet.h_dim_increase_type" class="doc doc-heading">
<code class="highlight language-python"><span class="n">h_dim_increase_type</span><span class="p">()</span></code>

<a href="#goli.nn.residual_connections.ResidualConnectionDenseNet.h_dim_increase_type" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td><p>"cumulative":
The dimension of the output layer is the concatenation of all the previous layer.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionDenseNet.has_weights" class="doc doc-heading">
<code class="highlight language-python"><span class="n">has_weights</span><span class="p">()</span></code>

<a href="#goli.nn.residual_connections.ResidualConnectionDenseNet.has_weights" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td><p>False
The current class does not use weights</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="goli.nn.residual_connections.ResidualConnectionNone" class="doc doc-heading">
        <code>ResidualConnectionNone</code>


<a href="#goli.nn.residual_connections.ResidualConnectionNone" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="goli.nn.residual_connections.ResidualConnectionBase" href="#goli.nn.residual_connections.ResidualConnectionBase">ResidualConnectionBase</a></code></p>

  
      <p>No residual connection.
This class is only used for simpler code compatibility</p>



  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionNone.__repr__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__repr__</span><span class="p">()</span></code>

<a href="#goli.nn.residual_connections.ResidualConnectionNone.__repr__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Controls how the class is printed</p>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionNone.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h_prev</span><span class="p">,</span> <span class="n">step_idx</span><span class="p">)</span></code>

<a href="#goli.nn.residual_connections.ResidualConnectionNone.forward" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Ignore the skip connection.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>h</code></td>          <td>
          </td>
          <td><p>torch.Tensor(..., m)
Return same as input.</p></td>
        </tr>
        <tr>
<td><code>h_prev</code></td>          <td>
          </td>
          <td><p>torch.Tensor(..., m)
Return same as input.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionNone.h_dim_increase_type" class="doc doc-heading">
<code class="highlight language-python"><span class="n">h_dim_increase_type</span><span class="p">()</span></code>

<a href="#goli.nn.residual_connections.ResidualConnectionNone.h_dim_increase_type" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>None</code></td>          <td>
          </td>
          <td><p>The dimension of the output features do not change at each layer.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionNone.has_weights" class="doc doc-heading">
<code class="highlight language-python"><span class="n">has_weights</span><span class="p">()</span></code>

<a href="#goli.nn.residual_connections.ResidualConnectionNone.has_weights" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td><p>False
The current class does not use weights</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="goli.nn.residual_connections.ResidualConnectionRandom" class="doc doc-heading">
        <code>ResidualConnectionRandom</code>


<a href="#goli.nn.residual_connections.ResidualConnectionRandom" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="goli.nn.residual_connections.ResidualConnectionBase" href="#goli.nn.residual_connections.ResidualConnectionBase">ResidualConnectionBase</a></code></p>




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionRandom.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">skip_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#goli.nn.residual_connections.ResidualConnectionRandom.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Class for the random residual connection, where each layer is connected
to each following layer with a random weight between 0 and 1.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>skip_steps</code></td>
          <td>
          </td>
          <td><p>Parameter only there for compatibility with other classes of the same parent.</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>out_dims</code></td>
          <td>
                <code><span title="typing.List">List</span>[int]</code>
          </td>
          <td><p>The list of output dimensions. Only required to get the number
of layers. Must be provided if <code>num_layers</code> is None.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>num_layers</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The number of layers. Must be provided if <code>out_dims</code> is None.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionRandom.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h_prev</span><span class="p">,</span> <span class="n">step_idx</span><span class="p">)</span></code>

<a href="#goli.nn.residual_connections.ResidualConnectionRandom.forward" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Add <code>h</code> with the previous layers with skip connection <code>h_prev</code>,
similar to ResNet.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>h</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>torch.Tensor(..., m)
The current layer features</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>h_prev</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>torch.Tensor(..., m), None
The features from the previous layer with a skip connection.
At <code>step_idx==0</code>, <code>h_prev</code> can be set to <code>None</code>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>step_idx</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>int
Current layer index or step index in the forward loop of the architecture.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>h</code></td>          <td>
          </td>
          <td><p>torch.Tensor(..., m)
Either return <code>h</code> unchanged, or the sum with
on <code>h_prev</code>, depending on the <code>step_idx</code> and <code>self.skip_steps</code>.</p></td>
        </tr>
        <tr>
<td><code>h_prev</code></td>          <td>
          </td>
          <td><p>torch.Tensor(..., m)
Either return <code>h_prev</code> unchanged, or the same value as <code>h</code>,
depending on the <code>step_idx</code> and <code>self.skip_steps</code>.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionRandom.h_dim_increase_type" class="doc doc-heading">
<code class="highlight language-python"><span class="n">h_dim_increase_type</span><span class="p">()</span></code>

<a href="#goli.nn.residual_connections.ResidualConnectionRandom.h_dim_increase_type" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>None</code></td>          <td>
          </td>
          <td><p>The dimension of the output features do not change at each layer.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionRandom.has_weights" class="doc doc-heading">
<code class="highlight language-python"><span class="n">has_weights</span><span class="p">()</span></code>

<a href="#goli.nn.residual_connections.ResidualConnectionRandom.has_weights" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td><p>False
The current class does not use weights</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="goli.nn.residual_connections.ResidualConnectionSimple" class="doc doc-heading">
        <code>ResidualConnectionSimple</code>


<a href="#goli.nn.residual_connections.ResidualConnectionSimple" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="goli.nn.residual_connections.ResidualConnectionBase" href="#goli.nn.residual_connections.ResidualConnectionBase">ResidualConnectionBase</a></code></p>




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionSimple.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">skip_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

<a href="#goli.nn.residual_connections.ResidualConnectionSimple.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Class for the simple residual connections proposed by ResNet,
where the current layer output is summed to a
previous layer output.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>skip_steps</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>int
The number of steps to skip between the residual connections.
If <code>1</code>, all the layers are connected. If <code>2</code>, half of the
layers are connected.</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionSimple.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h_prev</span><span class="p">,</span> <span class="n">step_idx</span><span class="p">)</span></code>

<a href="#goli.nn.residual_connections.ResidualConnectionSimple.forward" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Add <code>h</code> with the previous layers with skip connection <code>h_prev</code>,
similar to ResNet.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>h</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>torch.Tensor(..., m)
The current layer features</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>h_prev</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>torch.Tensor(..., m), None
The features from the previous layer with a skip connection.
At <code>step_idx==0</code>, <code>h_prev</code> can be set to <code>None</code>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>step_idx</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>int
Current layer index or step index in the forward loop of the architecture.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>h</code></td>          <td>
          </td>
          <td><p>torch.Tensor(..., m)
Either return <code>h</code> unchanged, or the sum with
on <code>h_prev</code>, depending on the <code>step_idx</code> and <code>self.skip_steps</code>.</p></td>
        </tr>
        <tr>
<td><code>h_prev</code></td>          <td>
          </td>
          <td><p>torch.Tensor(..., m)
Either return <code>h_prev</code> unchanged, or the same value as <code>h</code>,
depending on the <code>step_idx</code> and <code>self.skip_steps</code>.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionSimple.h_dim_increase_type" class="doc doc-heading">
<code class="highlight language-python"><span class="n">h_dim_increase_type</span><span class="p">()</span></code>

<a href="#goli.nn.residual_connections.ResidualConnectionSimple.h_dim_increase_type" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>None</code></td>          <td>
          </td>
          <td><p>The dimension of the output features do not change at each layer.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionSimple.has_weights" class="doc doc-heading">
<code class="highlight language-python"><span class="n">has_weights</span><span class="p">()</span></code>

<a href="#goli.nn.residual_connections.ResidualConnectionSimple.has_weights" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td><p>False
The current class does not use weights</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="goli.nn.residual_connections.ResidualConnectionWeighted" class="doc doc-heading">
        <code>ResidualConnectionWeighted</code>


<a href="#goli.nn.residual_connections.ResidualConnectionWeighted" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="goli.nn.residual_connections.ResidualConnectionBase" href="#goli.nn.residual_connections.ResidualConnectionBase">ResidualConnectionBase</a></code></p>




  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionWeighted.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">out_dims</span><span class="p">,</span> <span class="n">skip_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#goli.nn.residual_connections.ResidualConnectionWeighted.__init__" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Class for the simple residual connections proposed by ResNet,
with an added layer in the residual connection itself.
The layer output is summed to a a non-linear transformation
of a previous layer output.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>skip_steps</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>int
The number of steps to skip between the residual connections.
If <code>1</code>, all the layers are connected. If <code>2</code>, half of the
layers are connected.</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>out_dims</code></td>
          <td>
          </td>
          <td><p>list(int)
list of all output dimensions for the network
that will use this residual connection.
E.g. <code>out_dims = [4, 8, 8, 8, 2]</code>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>dropout</code></td>
          <td>
          </td>
          <td><p>float
value between 0 and 1.0 representing the percentage of dropout
to use in the weights</p></td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>activation</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>str, Callable
The activation function to use after the skip weights</p></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>normalization</code></td>
          <td>
          </td>
          <td><p>Normalization to use. Choices:</p>
<ul>
<li>"none" or <code>None</code>: No normalization</li>
<li>"batch_norm": Batch normalization</li>
<li>"layer_norm": Layer normalization in the hidden layers.</li>
<li><code>Callable</code>: Any callable function</li>
</ul></td>
          <td>
                <code>&#39;none&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>bias</code></td>
          <td>
          </td>
          <td><p>bool
Whether to apply add a bias after the weights</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionWeighted.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h_prev</span><span class="p">,</span> <span class="n">step_idx</span><span class="p">)</span></code>

<a href="#goli.nn.residual_connections.ResidualConnectionWeighted.forward" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  
      <p>Add <code>h</code> with the previous layers with skip connection <code>h_prev</code>, after
a feed-forward layer.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>h</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>torch.Tensor(..., m)
The current layer features</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>h_prev</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>torch.Tensor(..., m), None
The features from the previous layer with a skip connection.
At <code>step_idx==0</code>, <code>h_prev</code> can be set to <code>None</code>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>step_idx</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>int
Current layer index or step index in the forward loop of the architecture.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>h</code></td>          <td>
          </td>
          <td><p>torch.Tensor(..., m)
Either return <code>h</code> unchanged, or the sum with the output of a NN layer
on <code>h_prev</code>, depending on the <code>step_idx</code> and <code>self.skip_steps</code>.</p></td>
        </tr>
        <tr>
<td><code>h_prev</code></td>          <td>
          </td>
          <td><p>torch.Tensor(..., m)
Either return <code>h_prev</code> unchanged, or the same value as <code>h</code>,
depending on the <code>step_idx</code> and <code>self.skip_steps</code>.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionWeighted.h_dim_increase_type" class="doc doc-heading">
<code class="highlight language-python"><span class="n">h_dim_increase_type</span><span class="p">()</span></code>

<a href="#goli.nn.residual_connections.ResidualConnectionWeighted.h_dim_increase_type" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>None</code></td>          <td>
          </td>
          <td><p>The dimension of the output features do not change at each layer.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h5 id="goli.nn.residual_connections.ResidualConnectionWeighted.has_weights" class="doc doc-heading">
<code class="highlight language-python"><span class="n">has_weights</span><span class="p">()</span></code>

<a href="#goli.nn.residual_connections.ResidualConnectionWeighted.has_weights" class="headerlink" title="Permanent link">&para;</a></h5>


  <div class="doc doc-contents ">
  

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td><p>True
The current class uses weights</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>




  </div>

  </div>

</div>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="../license.html" class="md-footer__link md-footer__link--prev" aria-label="Previous: License" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              License
            </div>
          </div>
        </a>
      
      
        
        <a href="goli.features.html" class="md-footer__link md-footer__link--next" aria-label="Next: goli.features" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              goli.features
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright 2020 - 2023 Valence Discovery
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.expand"], "search": "../assets/javascripts/workers/search.ecf98df9.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.39f04ddb.min.js"></script>
      
        <script src="../javascripts/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>