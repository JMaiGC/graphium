# Testing the multitask pipeline with the micro ZINC dataset, by splitting it up into three tasks: SA, logp and score.

constants:
  seed: &seed 42
  raise_train_error: true   # Whether the code should raise an error if it crashes during training

datamodule:
  module_type: "MultitaskFromSmilesDataModule"
  args: # Matches that in the test_multitask_datamodule.py case.
    task_specific_args:   # To be replaced by a new class "DatasetParams"
      SA:
        df: null
        df_path: "goli/data/micro_ZINC/micro_ZINC.csv"
        smiles_col: "SMILES"
        label_cols: ["SA"]
        split_val: 0.2
        split_test: 0.2
        split_seed: *seed
        splits_path: null                 # This may not always be provided
        sample_size: null                 # This may not always be provided
        idx_col: null                     # This may not always be provided
        weights_col: null                 # This may not always be provided
        weights_type: null                # This may not always be provided
      logp:
        df: null
        df_path: "goli/data/micro_ZINC/micro_ZINC.csv"
        smiles_col: "SMILES"
        label_cols: ["logp"]
        split_val: 0.2
        split_test: 0.2
        split_seed: *seed
        splits_path: null                 # This may not always be provided
        sample_size: null                 # This may not always be provided
        idx_col: null                     # This may not always be provided
        weights_col: null                 # This may not always be provided
        weights_type: null                # This may not always be provided
      score:
        df: null
        df_path: "goli/data/micro_ZINC/micro_ZINC.csv"
        smiles_col: "SMILES"
        label_cols: ["score"]
        split_val: 0.2
        split_test: 0.2
        split_seed: *seed
        splits_path: null                 # This may not always be provided
        sample_size: null                 # This may not always be provided
        idx_col: null                     # This may not always be provided
        weights_col: null                 # This may not always be provided
        weights_type: null                # This may not always be provided

    # Featurization
    prepare_dict_or_graph: pyg:graph
    featurization_n_jobs: 0
    featurization_progress: True
    featurization:
      atom_property_list_onehot: [atomic-number, valence]
      atom_property_list_float: [mass, electronegativity, in-ring]
      edge_property_list: [bond-type-onehot, stereo, in-ring]
      add_self_loop: False
      explicit_H: False
      use_bonds_weights: False
      pos_encoding_as_features: &pos_enc
        pos_type: laplacian_eigvec
        num_pos: 3
        normalization: "none"
        disconnected_comp: True
      pos_encoding_as_directions: *pos_enc

    # Data handling-related
    batch_size_train_val: 16
    batch_size_test: 16
    # cache_data_path: null

architecture:
  model_type: FullGraphMultiTaskNetwork
  pre_nn:   # Set as null to avoid a pre-nn network
    out_dim: 32
    hidden_dims: 32
    depth: 1
    activation: relu
    last_activation: none
    dropout: &dropout 0.1
    normalization: &normalization "batch_norm"
    last_normalization: *normalization
    residual_type: none

  pre_nn_edges:   # Set as null to avoid a pre-nn network
    out_dim: 16
    hidden_dims: 16
    depth: 2
    activation: relu
    last_activation: none
    dropout: *dropout
    normalization: *normalization
    last_normalization: *normalization
    residual_type: none

  gnn:  # Set as null to avoid a post-nn network
    out_dim: 32
    hidden_dims: 64
    depth: 4
    activation: relu
    last_activation: none
    dropout: *dropout
    normalization: *normalization
    last_normalization: *normalization
    residual_type: random
    pooling: [sum, max]
    virtual_node: 'sum'
    layer_type: 'pyg:gine'
    layer_kwargs:
      # num_heads: 3
      # aggregators: [mean, max]
      # scalers: [identity, amplification, attenuation]

  post_nn: null
    #out_dim: 1
    #hidden_dims: 32
    #depth: 2
    #activation: relu
    #last_activation: none
    #dropout: *dropout
    #normalization: *normalization
    #last_normalization: "none"
    #residual_type: none

  task_heads:
    - task_name: "SA"
      out_dim: 1
      hidden_dims: 32
      depth: 2                          # Not needed if we have hidden_dims
      activation: relu
      last_activation: none
      dropout: *dropout
      normalization: *normalization
      last_normalization: "none"
      residual_type: none
    - task_name: "logp"
      out_dim: 1
      hidden_dims: 32
      depth: 2                          # Not needed if we have hidden_dims
      activation: relu
      last_activation: none
      dropout: *dropout
      normalization: *normalization
      last_normalization: "none"
      residual_type: none
    - task_name: "score"
      out_dim: 1
      hidden_dims: 32
      depth: 2                          # Not needed if we have hidden_dims
      activation: relu
      last_activation: none
      dropout: *dropout
      normalization: *normalization
      last_normalization: "none"
      residual_type: none

#Task-specific
predictor:
  metrics_on_progress_bar:
    SA: ["mae"]
    logp: ["mae"]
    score: ["mae", "pearsonr", "f1 gt 3"]
  loss_fun:
    SA: mse
    logp: mse
    score: mse
  random_seed: *seed
  optim_kwargs:
    lr: 1.e-2
    weight_decay: 1.e-7
  torch_scheduler_kwargs:
    #module_type: ReduceLROnPlateau
    #factor: 0.5
    #patience: 7
  scheduler_kwargs: null
  #  monitor: &monitor loss/val
  #  mode: min
  #  frequency: 1
  target_nan_mask: 0 # null: no mask, 0: 0 mask, ignore: ignore nan values from loss
  flag_kwargs:
    n_steps: 0 #1
    alpha: 0.0 #0.01

# Task-specific
metrics:
  SA:
    - name: mae
      metric: mae
      threshold_kwargs: null
    - name: pearsonr
      metric: pearsonr
      threshold_kwargs: null
    - name: f1 gt 0.5
      metric: f1
      num_classes: 2
      average: micro
      threshold_kwargs: &threshold_05
        operator: greater
        threshold: 0.5
        th_on_preds: True
        th_on_target: True
        target_to_int: True

    - name: precision gt 0.5
      metric: precision
      average: micro
      threshold_kwargs: *threshold_05

  logp:
    - name: mae
      metric: mae
      threshold_kwargs: null
    - name: pearsonr
      metric: pearsonr
      threshold_kwargs: null
    - name: f1 gt 3
      metric: f1
      num_classes: 2
      average: micro
      threshold_kwargs: &threshold_3
        operator: greater
        threshold: 3
        th_on_preds: True
        th_on_target: True
        target_to_int: True

    - name: f1 gt 0
      metric: f1
      num_classes: 2
      average: micro
      threshold_kwargs:
        operator: greater
        threshold: 0
        th_on_preds: True
        th_on_target: True
        target_to_int: True
    - name: precision gt 3
      metric: precision
      average: micro
      threshold_kwargs: *threshold_3

  score:
    - name: mae
      metric: mae
      threshold_kwargs: null
    - name: pearsonr
      metric: pearsonr
      threshold_kwargs: null
    - name: f1 gt 3
      metric: f1
      num_classes: 2
      average: micro
      threshold_kwargs: *threshold_3
    - name: f1 gt 0
      metric: f1
      num_classes: 2
      average: micro
      threshold_kwargs:
        operator: greater
        threshold: 0
        th_on_preds: True
        th_on_target: True
        target_to_int: True
    - name: precision gt 3
      metric: precision
      average: micro
      threshold_kwargs: *threshold_3

trainer:
  logger:
    save_dir: logs/micro_ZINC_mtl
  #early_stopping:
  #  monitor: *monitor
  #  min_delta: 0
  #  patience: 10
  #  mode: &mode min
  model_checkpoint:
    dirpath: models_checkpoints/micro_ZINC_mtl/
    filename: "model"
    #monitor: *monitor
    #mode: *mode
    save_top_k: 1
    every_n_epochs: 1
  trainer:
    precision: 32
    max_epochs: 25 #25
    min_epochs: 1 #5
    gpus: 1
