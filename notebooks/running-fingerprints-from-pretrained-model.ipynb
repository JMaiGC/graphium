{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('goli': conda)"
  },
  "interpreter": {
   "hash": "f4a99d018a205fcbcc0480c84566beaebcb91b08d0414b39a842df533e2a1d25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import yaml\n",
    "import numpy as np\n",
    "import torch\n",
    "import fsspec\n",
    "\n",
    "# Current project imports\n",
    "import goli\n",
    "from goli.config._loader import load_datamodule, load_trainer\n",
    "from goli.trainer.predictor import PredictorModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path containing the model and its configurations\n",
    "MODEL_PATH = \"gs://goli-private/pretrained-models/htsfp-pcba-24M\"\n",
    "MODEL_FILE = f\"{MODEL_PATH}/model.ckpt\"\n",
    "CONFIG_FILE = f\"{MODEL_PATH}/configs.yaml\"\n",
    "\n",
    "# Path containing the SMILES data to infer\n",
    "SMILES_DF_PATH = f\"gs://goli-public/datasets/goli-zinc-bench-gnn/smiles_score.csv.gz\"\n",
    "SMILES_COL = \"SMILES\"\n",
    "\n",
    "# Number of layers to drop when inferring the fingerprints\n",
    "NUM_LAYERS_TO_DROP = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration file of the trained model\n",
    "with fsspec.open(CONFIG_FILE, \"rb\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# Overwrite configurations of the datamodule\n",
    "cfg[\"datamodule\"][\"module_type\"] = \"DGLFromSmilesDataModule\"\n",
    "args = cfg[\"datamodule\"][\"args\"]\n",
    "cfg[\"datamodule\"][\"args\"] = {\n",
    "        \"df_path\": SMILES_DF_PATH,\n",
    "        \"smiles_col\": SMILES_COL,\n",
    "        \"label_cols\": [],\n",
    "        \"featurization\": args[\"featurization\"],\n",
    "    }\n",
    "\n",
    "# Load and initialize the dataset\n",
    "datamodule = load_datamodule(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-09 13:34:55.605 | WARNING  | goli.config._loader:load_trainer:126 - Number of GPUs selected is `1`, but will be ignored since no GPU are available on this device\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DGL_GNN\n",
       "---------\n",
       "    pre-NN(depth=3, ResidualConnectionSimple(skip_steps=1))\n",
       "        [FCLayer[69 -> 512 -> 512 -> 512]\n",
       "    \n",
       "    pre-NN-edges(depth=2, ResidualConnectionSimple(skip_steps=1))\n",
       "        [FCLayer[16 -> 32 -> 32]\n",
       "    \n",
       "    GNN(depth=8, ResidualConnectionSimple(skip_steps=1))\n",
       "        DGNMessagePassingLayer[512 -> 512 -> 512 -> 512 -> 512 -> 512 -> 512 -> 512 -> 1200]\n",
       "        -> Pooling(['sum', 'max']) -> FCLayer(2400 -> 1200, activation=None)\n",
       "    \n",
       "    post-NN(depth=3, ResidualConnectionSimple(skip_steps=1))\n",
       "        [FCLayer[1200 -> 1200 -> 1200 -> 689]\n",
       "\n",
       "  | Name               | Type           | Params\n",
       "------------------------------------------------------\n",
       "0 | model              | FullDGLNetwork | 20.0 M\n",
       "1 | model.pre_nn       | FeedForwardNN  | 564 K \n",
       "2 | model.pre_nn_edges | FeedForwardNN  | 1.7 K \n",
       "3 | model.gnn          | FeedForwardDGL | 19.5 M\n",
       "4 | model.post_nn      | FeedForwardNN  | 0     \n",
       "5 | loss_fun           | BCELoss        | 0     \n",
       "------------------------------------------------------\n",
       "20.0 M    Trainable params\n",
       "0         Non-trainable params\n",
       "20.0 M    Total params\n",
       "80.194    Total estimated model params size (MB)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Load the model, drop the layers, and load the trainer\n",
    "predictor = PredictorModule.load_from_checkpoint(MODEL_FILE)\n",
    "predictor.model.drop_post_nn_layers(num_layers_to_drop=NUM_LAYERS_TO_DROP)\n",
    "trainer = load_trainer(cfg)\n",
    "\n",
    "predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-09 13:35:05.180 | INFO     | goli.data.datamodule:prepare_data:355 - Prepare dataset with 12000 data points.\n",
      "Missing logger folder: logs/htsfp-pcba\\default\n",
      "Predicting: 0it [00:00, ?it/s]C:\\Users\\Domin\\miniconda3\\envs\\goli\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:69: UserWarning: The dataloader, predict dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Predicting: 100%|██████████| 750/750 [04:49<00:00,  2.59it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-0.5803324 ,  0.9320879 ,  0.36963642, ...,  0.0749743 ,\n",
       "        -0.51820433,  0.9317135 ],\n",
       "       [-1.373152  , -2.6965613 ,  1.6978087 , ...,  0.5530317 ,\n",
       "        -0.44438946,  0.3196116 ],\n",
       "       [-1.4481115 , -0.613314  ,  0.33443648, ..., -0.7772472 ,\n",
       "         0.6488527 ,  0.94782364],\n",
       "       ...,\n",
       "       [ 0.48269045, -2.212933  , -0.76536405, ...,  0.25664926,\n",
       "        -0.23683667, -1.0514059 ],\n",
       "       [-0.26671052, -2.3024874 , -1.1361103 , ...,  0.42454743,\n",
       "         0.57352805,  0.4816807 ],\n",
       "       [-0.6714377 , -0.7725297 , -1.1829937 , ...,  0.8448169 ,\n",
       "        -0.6167966 ,  0.9792832 ]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Run the model prediction, and concatenate the batched results\n",
    "preds = trainer.predict(model=predictor, datamodule=datamodule)\n",
    "if isinstance(preds[0], torch.Tensor):\n",
    "    preds = [p.detach().cpu().numpy() for p in preds]\n",
    "preds = np.concatenate(preds, axis=0)\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(12000, 1200)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "preds.shape"
   ]
  }
 ]
}